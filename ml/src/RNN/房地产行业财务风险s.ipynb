{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************************\n",
      "\n",
      "**************************当前是第4列数据,共16列**************************\n",
      "\n",
      "**************************************************************************\n",
      "\n",
      "[[0.81479731]\n",
      " [0.72984199]\n",
      " [0.26095935]\n",
      " [0.27438644]\n",
      " [0.59597564]\n",
      " [1.38416717]\n",
      " [1.59575411]\n",
      " [1.84618745]\n",
      " [1.81554145]\n",
      " [2.37936166]\n",
      " [3.29088238]\n",
      " [3.74076685]\n",
      " [4.50338997]\n",
      " [5.28809505]\n",
      " [5.51065526]\n",
      " [5.72909665]\n",
      " [5.92401827]\n",
      " [5.77068446]\n",
      " [6.84187   ]\n",
      " [7.02052901]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4lOWh/vHvk42QsASSAAECIezITsQNcUNFVFRcaWu1lmJb61KX6tHWan+/Ho9162atVD21iAguUEsRRUVwA0kgBMISSCAQIDBhC9kzk+f8kcEiEjKQzLyz3J/rysVk5p2ZO28mN8+8yzPGWouIiISOKKcDiIjIyVFxi4iEGBW3iEiIUXGLiIQYFbeISIhRcYuIhBgVt4hIiFFxi4iEGBW3iEiIifHHg6akpNiMjAx/PLSISFjKyckps9am+rKsX4o7IyOD7Oxsfzy0iEhYMsYU+7qsNpWIiIQYFbeISIhRcYuIhBgVt4hIiFFxi4iEmGaL2xgz0BiTe9RXuTHmnkCEExGRb2v2cEBr7SZgJIAxJhrYCczzcy4REWnCyW4quQgotNb6fLyhiEi4q6x1syBvFy98UhiQ5zvZE3BuAmb7I4iISCgpr6nn4w17Wbh2N0sLXNS6G+jeMZ5p5/YhNtq/uw99Lm5jTBwwGfivJm6fDkwH6NWrV6uEExEJJger6vhg/R4WrSvls81l1Hka6NYhnqlje3HZ0G5kZXQmOsr4PcfJjLgvA1ZZa/cc70Zr7QxgBkBWVpY+Ol5EwkJZRS3v55eyaF0pXxbuw91g6ZHUllvO7s3EoWmMSk8iKgBlfbSTKe6paDOJiESAPeU1LFpXysK1u1m5bT8NFjKSE/jR+EwuG9qNYT06Ykxgy/poPhW3MSYRuBi43b9xRESccai6njezd/DeulJyig8A0L9LO352QT8uG5bGoG7tHS3ro/lU3NbaSiDZz1lERBxz5+zVLCtwMTitA/ddPIDLhnWjX5f2Tsc6Lr9M6yoiEkq+2rqfZQUuHpw4iJ+c39fpOM3SKe8iEtGstTzzwSZS27fh1rMznI7jExW3iES0Lwr3sWLrfn56fl/axkU7HccnKm4RiVhHRttpHRuPxQ4VKm4RiVhLC1ys2n6QOy7oR3xsaIy2QcUtIhHKWsuziwvo2aktN2SlOx3npKi4RSQifbhhL3klh7jrwv7ExYRWFYZWWhGRVtDQ0DjazkhOYMroHk7HOWkqbhGJOIvyS9mwu5y7J/Qnxs8z+flD6CUWEWkBT4PlucUF9E1NZPKI0Bttg4pbRCLMgrxdbN5bwT0TBgRkClZ/UHGLSMRwexr4/YebGdStPZcPS3M6zilTcYtIxJi3eidbyyq5Z8KAgM+h3ZpU3CISEeo9Dfzx480M7dGBS0/r6nScFlFxi0hEeCunhB37q7n34gFBM6/2qVJxi0jYq3V7+NNHmxmZnsQFA7s4HafFVNwiEvbmrNzBrkM13HdJ6I+2QcUtImGupt7Dnz/ewtiMzozrl+J0nFah4haRsPba8mL2Hq7l3jAZbYOKW0TCWFWdm78uLeScfsmcmRk+H5vrU3EbY5KMMW8ZYzYaYzYYY87ydzARkZZ69YtiyirquPfiAU5HaVW+fljwH4BF1trrjDFxQIIfM4mItNjhmnpeXFbIeQNSGdO7s9NxWlWzxW2M6QiMB24FsNbWAXX+jSUi0jL/+/k2DlbVh91oG3zbVNIHcAH/a4xZbYx5yRiT6OdcIiKn7FB1PX/7tIgJg7syIj3J6TitzpfijgFGAy9Ya0cBlcBDxy5kjJlujMk2xmS7XK5Wjiki4ruXPy3icI07LEfb4FtxlwAl1toV3u/forHIv8FaO8Nam2WtzUpNTW3NjCIS5tyeBpYVuPhn7k7KKmpb9FgHKut45fNtTBrWjSHdO7RSwuDS7DZua22pMWaHMWagtXYTcBGw3v/RRCScWWtZU3KI+at3siBvF2UV/9l1NrRHB84bkMp5A7owqlcSsSfxKTUvLiuiss7NPRPCc7QNvh9Vcicwy3tESRHwA/9FEpFwtrWskvmrd/Luml1sLaskLjqKCwd14epR3Unr2JZPN7tYWuDir0uLeH5JIe3bxHB2v2TGD0jlvAGp9OzU9EFtrsO1vPrFNiaP6M6Aru0D+FMFlk/Fba3NBbL8nEVEwpTrcC0L8nYxP3cXa3YcxBg4s08yPz4vk4lD0+jYNvbrZUekJ/GzC/tzqLqeLwvLWFrgYukmF+/n7wGgb2oi5w3owvgBKZyZmUx8bPTX9/3r0kJq3R7uvqh/wH/GQPJ1xC0iclIqa928n1/K/NxdfL6lDE+DZUhaBx6eNIgrRzSOrk+kY9tYJg5NY+LQNKy1FLoq+GSTi2Wby5i1ophXPt9Km5gozshMZnz/FIb16Mhry4uZMronmantAvRTOsNYa1v9QbOysmx2dnarP66IBLd6707G+bm7WLy+lJr6Bnp2astVI7tz9cge9G+lzRc19R5WbN3P0k0ulhbspdBVCUBMlGHJ/eeT3jn0zhE0xuRYa33asqERt4i02KbSw8z+ajvvrtnF/so6OiXEct2Ynlw9sgdjendq9cmd4mOjvTsvU4EhlByoYllBGUkJsSFZ2idLxS0ip6S6zsO/1+5m9lfbySk+QFx0FBef1pUpo3pwbv9U4mICN4ddz04JfOeMXgF7PqepuEXkpGwsLWf2iu28s3onh2vcZKYm8svLBzNldE86J8Y5HS8iqLhFpFlVdW4W5DWOrldvP0hcTBSThnZj6thejO3TOWzmuQ4VKm4RadL6XeW8sXI781bt5HCtm35d2vGrK4YwZVQPOml07RgVt4h8Q1WdmwVrdvP6V9vJ3dE4ur5iWBpTz+hFlh92NMrJU3GLCAD5uw4x+6vtzF+9i4paN/27tOPRK4YwZXQPkhI0ug4mKm6RCFZd52FB3i5mrWgcXbeJieLy4Wl8Z2wvvxzGJ61DxS0SgbbsPcysFdt5O6eE8ho3fVMTefSKIVw7uicdE2KbfwBxlIpbJELUuj28n7+HWcuLWbF1P7HRholD0/jeGToyJNSouEXC3PZ9Vbz+1XbezN7Bvso60ju35cGJg7g+qycp7do4HU9OgYpbJAy5PQ18tHEvs1ZsZ1mBi+gow0WDuvDdM3tzbr8UoqI0ug5lKm6RMLL7UDVvfLWDOSt3UFpeQ7cO8dwzoT83np7e7Gx8EjpU3CJh4qn3N/LCJ4VYYHz/VH5z1WlcOKgLMSfx6TESGlTcImHgow17eH5JIZNHdOf+SwbSKzn8Z8iLZCpukRC3r6KWB9/OY1C39jx1/XDaxEQ3fycJaSpukRBmreWhd9ZSXu3mtWlnqLQjhDZ+iYSwudk7WLx+Dw9cOpBB3To4HUcCRMUtEqKK91Xy+L/Wc1ZmMj8c18fpOBJAPm0qMcZsAw4DHsDt6+eiiYh/uD0N/HxOLtFRhmduGKHjsiPMyWzjvsBaW+a3JCLis78uLWTV9oP84aaRdE/S8dmRRptKREJMXslBfv/hZq4YnsbkEd2djiMO8LW4LfChMSbHGDP9eAsYY6YbY7KNMdkul6v1EorI16rrPPx8Ti4p7drw26uHaWKoCOVrcY+z1o4ELgPuMMaMP3YBa+0Ma22WtTYrNTW1VUOKSKP/eW8Dha5Knr5+hKZfjWA+Fbe1dqf3373APGCsP0OJyLctLXDx6pfF/OCcDMb1T3E6jjio2eI2xiQaY9ofuQxcAqzzdzAR+Y8DlXU88OYa+ndpx4MTBzkdRxzmy1ElXYF53m1pMcDr1tpFfk0lIl+z1vLwvLUcqKrjf39wOvGxOjsy0jVb3NbaImBEALKIyHG8s2on760r5cGJgzite0en40gQ0OGAIkFsx/4qfv1uPmMzOjN9fKbTcSRIqLhFgpSnwXLf3DUAPHPDCKJ1dqR4aXZAkSD1t0+L+Grbfp6+fgTpnTW/tvyHRtwiQSh/1yGe+WATE0/rxrWjezgdR4KMilskyNTUN54dmZQQx39P0dmR8m3aVCISZJ56fxMFeyr4+w9Op3NinNNxJAhpxC0SRD7fUsbLn23l5jN7c/7ALk7HkSCl4hYJEoeq6rlv7hoyUxN5eNJgp+NIENOmEpEgYK3lkflrKauo5Z3vn03bOJ0dKU3TiFskCLy9aicL8nZzz4T+DO+Z5HQcCXIqbhGHbSur5Nf/XMfYPp35yfn9nI4jIUDFLeKgek8Dd7+xmugow+9vHKmzI8Un2sYt4qDnFhewpuQQf/nuaH12pPhMI24Rh3xRWMYLSwu5MSudScPSnI4jIUTFLeKAA5V13DtnDX2SE3n0yiFOx5EQo+IWCTBrLQ+9k8e+ylr+cNMoEttoi6WcHBW3SIC9sXIH7+fv4YFLBzKspz4YQU6eilskgLbsreDxf+Uzrl8K08bpgxHk1Ki4RQKk1u3hrtmrSYiL4dkbRhClQ//kFGnjmkiAPLVoE+t3l/PS97Po0iHe6TgSwnwecRtjoo0xq40xC/wZSCQcLS1w8dJnW/n+Wb2ZMKSr03EkxJ3MppK7gQ3+CiISrsoqarlv7hoGdG2nWf+kVfhU3MaYnsDlwEv+jSMSXqy1PPDmGspr6vnDTaOIj9Wsf9Jyvo64fw/8AmjwYxaRsPOPL4tZssnFw5cNYnBaB6fjSJhotriNMVcAe621Oc0sN90Yk22MyXa5XK0WUCRUbSwt57cLN3DBwFRuOTvD6TgSRnwZcZ8DTDbGbAPeAC40xrx27ELW2hnW2ixrbVZqamorxxQJLTX1jYf+dYiP5anrR+gDf6VVNVvc1tr/stb2tNZmADcBH1trv+f3ZCIh7L8XbqBgTwXP3DCClHZtnI4jYUYn4Ii0sg/X7+EfXxYzbVwfzhugd5/S+k7qBBxr7SfAJ35JIhIG9pTX8MBbaxiS1oEHJg50Oo6EKY24RVpJQ4PlvrlrqK738Mepo2gTo0P/xD9U3CKt5PklW/hsSxmPXnEa/bq0czqOhDHNVSLSQvWeBh7/Vz6vLd/OlSO6M3VsutORJMypuEVaYH9lHT+dlcPyov3cfl4mv7h0kA79E79TcYucok2lh5n2j5XsKa/luRtHcM2onk5Hkgih4hY5BR/kl/LzObkktolhzvQzGdWrk9ORJIKouEVOgrWW55ds4ekPChjesyMzbs6iW0fNrS2BpeIW8VF1nYcH3lrDgrzdXDWyO09eO1yz/YkjVNwiPth1sJrpM7PJ31XOgxMH8ePzMrUTUhyj4hZpRk7xAW6fmUNNvYeXvp/FRYP1CTbiLBW3yAm8mb2DR+atIy0pntk/OoP+Xds7HUlExS1yPG5PA0+8t5GXP9vK2X2Tef47o+mUGOd0LBFAxS3yLYeq67lz9mqWFbi49ewMHrl8MLHRmh1CgoeKW+Qoha4KfvRqNjsOVPHElGFMHdvL6Ugi36LiFvH6bHMZP5mVQ2x0FLOmncnYPp2djiRyXCpuEWBfRS13vL6KtI7xvHzL6aR3TnA6kkiTtOFOBPjdok1U1rr583dGq7Ql6Km4JeKt2n6AOdk7uG1cHwbocD8JASpuiWieBsuj/1xH1w5tuOui/k7HEfGJilsi2utfbWfdznIeuXwI7dpol4+EhmaL2xgTb4z5yhizxhiTb4x5PBDBRPxtX0UtTy3ayFmZyVw5PM3pOCI+82WIUQtcaK2tMMbEAp8ZY96z1i73czYRv3py0Uaq6jz85qrTNGGUhJRmR9y2UYX321jvl/VrKhE/yyk+wNzsEn44ro/mH5GQ49M2bmNMtDEmF9gLLLbWrvBvLBH/ObJDsluHeO7UDkkJQT4Vt7XWY60dCfQExhpjhh67jDFmujEm2xiT7XK5WjunSKt5fUUx+bvK+eUVg7VDUkLSSR1VYq09CCwBJh7nthnW2ixrbVZqampr5RNpVWUVtTz1/ibO6ZfM5cO0Q1JCky9HlaQaY5K8l9sCFwMb/R1MxB+efK9xh+Tjk7VDUkKXL+8T04BXjTHRNBb9XGvtAv/GEml9OcX7eTOnhNvPy6RfF+2QlNDVbHFba/OAUQHIIuI3ngbLr+bn061DPHddqB2SEtp05qREhFkrilm/u5xfXTGERO2QlBCn4pawd2SH5Lh+KUwa1s3pOCItpuKWsPc/722kpt7DY9ohKWFCxS1hLXvbft7KKWHauZn069LO6TgirULFLWHL7WngV//MJ61jPHde2M/pOCKtRsUtYeu15cVs8O6QTIjTDkkJHypuCUuuw7U8s7iAc/uncNlQ7ZCU8KLilrCkHZISzlTcEnZWbtvP26tK+NG5mfRN1Q5JCT8qbgkrbk8Dv5q/ju4d4/mZdkhKmFJxS1iZubyYjaWHefRK7ZCU8KXilrCx93ANz35QwPgBqVx6mnZISvhScUtYaGiwPPZuPjVuD49dOUQ7JCWsqbgl5FlreWT+OhauLeW+SwaSqR2SEuZU3BLSrLU8/q/1zP5qO3dc0Jfbx2c6HUnE71TcErKstTzx3kb+/sU2po3rw/2XDNQmEokIKm4JWc98UMCMZUXcclZvHrl8sEpbIoaKW0LSnz7azJ+XbGHq2HR+faXOjpTIouKWkPPi0kKeWVzAtaN78turhxEVpdKWyKLilpDyymdbeeK9jVw5oju/u264SlsiUrPFbYxJN8YsMcasN8bkG2PuDkQwkWO9tryY3yxYz8TTuvHsDSOIVmlLhPLlnGA3cJ+1dpUxpj2QY4xZbK1d7+dsIl+bu3IHv5y/jgmDu/DHqaOIjdabRYlczb76rbW7rbWrvJcPAxuAHv4OJnLE/NU7efCdPMYPSOX5744mLkalLZHtpP4CjDEZwChghT/CiBzr33m7uXduLmf2SWbGzWNoExPtdCQRx/lc3MaYdsDbwD3W2vLj3D7dGJNtjMl2uVytmVEi1Af5pdz9xmrG9O7Ey7dmER+r0hYBH4vbGBNLY2nPsta+c7xlrLUzrLVZ1tqs1NTU1swoEWjJpr3c8foqhvboyCu3nq4pWkWO4stRJQZ4GdhgrX3W/5Ek0n22uYzbZ+YwsFt7Xr1tLO3jY52OJBJUfBlxnwPcDFxojMn1fk3ycy4JQfsqatm85zC7D1VTUeumocGe9GMsL9rHtH+sJDMlkZm3nUHHtiptkWM1+/7TWvsZoANmpUmb9xxmxrIi5ufupN7zn7I2BtrFxdA+PoZ28TG0axND+/hY2sXH0L6N9/o23u/jY2hosPxmwXrSOyXw2rQz6JQY5+BPJRK8tOFQTom1lhVb9zNjWREfb9xLfGwUU8f2IiujM5W1bg7X1FNR4+ZwrZvDNW4qatxU1Lo5WFXHjgNVjbfVuKmu93zjcfukJDJr2hmktGvj0E8mEvxU3HJSPA2WRetKmbGskDUlh0hOjOPnEwZw81m96XwKI2S3p4HKWg/lNfVU1Lrpk5Koo0dEmqHiFp9U13l4M2cHL326le37q8hITuD/Xz2U68b0bFHRxkRH0TEhio4J2pYt4isVt5zQvopaXv2ymJlfbuNAVT2jeiXx8KRBXDykm+YKEXGIiluOa1tZJX/7tIi3ckqodTcwYXBXbj8vk6zenTT3tYjDVNzyDau3H+DFpUW8v76U2KgopozuwbRzM+nXRR/AKxIsVNxCZa2bf6/dzZyVO8gpPkCH+Bh+en5fbjk7gy7t452OJyLHUHFHKGstq7YfZO7KHSzI20VlnYfM1ER+dcUQbjw9nXZt9NIQCVb664wwZRW1zFu1kznZO9iyt4KEuGguH5bGjaenM0bbr0VCgoo7Arg9DSzb7GLOyh18tGEv7gbL6F5J/M+UYVwxortG1yIhRn+xYax4XyVzs3fwVk4Je8prSU6M4wfnZHBDVjr9u7Z3Op6InCIVd5iprvOwKL9xR+Pyov1EGThvQCqPT07nwkFd9ekxImFAxR1G/vTRZmZ8WsThGje9Oidw/yUDuHZMT9I6tnU6moi0IhV3mFi4djfPLC5gwuAu3DauD2f2SSZKZzaKhCUVdxjYW17Dw/PWMrxnR1743hh9ArpImNNfeIiz1vLAW3nU1Ht47saRKm2RCKC/8hD32ortLC1w8fCkwfRN1WnpIpFAxR3CilwV/Pbf6xk/IJWbz+ztdBwRCRAVd4iq9zTw8zm5tImJ5qnrhuuMR5EIop2TIer5JVtYU3KI578zmq4dNBGUSCTRiDsE5e44yJ8+3sI1o3pw+fA0p+OISIA1W9zGmFeMMXuNMesCEUhOrKrOzc/n5NK1fRsem3ya03FExAG+jLj/Dkz0cw7x0RMLN7K1rJKnbxhBx7b6nEaRSNRscVtrlwH7A5BFmrFk015mLi9m2rg+nN03xek4IuKQVtvGbYyZbozJNsZku1yu1npY8TpQWccv3spjQNd23H/pQKfjiIiDWq24rbUzrLVZ1tqs1NTU1npYofHsyEfmr+VgVR3P3TiS+NhopyOJiIN0VEkImJ+7k4VrS7n34oGc1r2j03FExGEq7iC382A1j87P5/SMTkwfn+l0HBEJAr4cDjgb+BIYaIwpMcb80F9hDlXXY63118OHnIYGy31zc2mwlmdvGEm0pmkVEXw4c9JaOzUQQay1nPvkxwD07dKOvqmNX5mpifRNbUfv5ISIm/nulc+3srxoP7+7djjpnROcjiMiQSJoTnn3NFjumTCAQlcFRa5KlhW4eCun5OvbY6IMvZITyExpR98uiV8Xe9/URJIS4hxM7h+bSg/zu0WbuHhIV67P6ul0HBEJIkFT3DHRUdw2rs83riuvqafIVUmRq4JCVwWFeyspKqtgWYGLOk/D18slJ8Y1lniXRG48vRcj05MCHb9V1bo93DMnlw5tY3hiyjBNICUi3xA0xX08HeJjGZme9K0idnsaKDlQTVFZY5kfGaUvyNvN26t28vT1I5g8ortDqVvuucWb2bC7nJe+n0VKuzZOxxGRIBPUxd2UmOgoMlISyUhJ5MJB/7n+QGUdt8/M4a7Zq9m+r5I7LugXcqPVr7bu58VlhUwdm86EIV2djiMiQSis9vZ1Soxj5rSxXDOqB09/UMD9b+ZR525o/o5B4nBNPffOzSW9UwK/vHyI03FEJEiF5Ij7RNrERPPsDSPISE7kuQ8LKDlQxYs3jwn6HZgVtW4efDuPXQerefPHZ5HYJux+NSLSSsJqxH2EMYa7J/TnDzeNZPX2g0z5yxdsK6t0OtZxNTRY3llVwgVPf8LCtaXcf+lAxvTu7HQsEQliYVncR1w1sgezfnQGB6rquOYvn7NyW3BNcri25BDX/fUL7p27hu5JbZl/xzn89Px+TscSkSAX1sUNcHpGZ+b99Bw6JcTx3b+t4J+5O52OxL6KWv7rnTwmP/8Z2/dX8bvrhjPvJ2eH/GGMIhIYEbEhNSMlkXd+eja3z8zh7jdy2VZWxV0XBf6IE7engZnLi3l2cQHVdR5+eE4f7prQnw7x+kAEEfFdRBQ3QFJCHDN/eAYPvZPHcx8WULyvkieuHUabmMBMkfrFljIe+1c+BXsqOLd/Cr++cgj9urQPyHOLSHiJmOIGiIuJ4pnrR9AnOZFnFhdQcrCaF783hk6J/jvipORAFf+9cAML15bSs1NbXrx5DJcM6Rpyx5eLSPCIqOKGxiNO7ryoP72SE3jgrTymvPAFr9x6On1SElv1eWrqPby4tIgXlm4B4N6LBzB9fKY+BEFEWiziivuIq0b2oEdSW6bPzOGav3zOjJuzGNun5YfhWWt5P7+U/7dgAzsPVnP58DQenjSYHkltWyG1iAgYf8x/nZWVZbOzs1v9cf2heF8lP/j7Skr2V/PkdcO4ZtTxZ+Kz1lLvsdR5Gqh3N1DvaaDW+2+9x1LvaaC8up7nP9nC51v2MbBre349eYg+1FdEfGKMybHWZvm0bKQXN8Chqnp+/FoOXxbtIyM54T8F7S3pOm85+6JDfAz3XTKQ757Ri5gImz9cRE7dyRR3xG4qOVrHhFhevW0sf/54M1v3VREXHUVcjCE2Ooq46ChiY6KIjY6iTUwUsdGN18dGRxEX473dezk22jCiZ5Jfd3aKiKi4veJiorj3koFOxxARaZbey4uIhBgVt4hIiPGpuI0xE40xm4wxW4wxD/k7lIiINK3Z4jbGRAPPA5cBQ4CpxhjN8i8i4hBfRtxjgS3W2iJrbR3wBnCVf2OJiEhTfCnuHsCOo74v8V4nIiIOaLWdk8aY6caYbGNMtsvlaq2HFRGRY/hS3DuB9KO+7+m97hustTOstVnW2qzU1NTWyiciIsdo9pR3Y0wMUABcRGNhrwS+Y63NP8F9XEDxKWZKAcpO8b6BoHwto3wto3wtE8z5eltrfRr1NnvmpLXWbYz5GfA+EA28cqLS9t7nlIfcxphsX8/Xd4LytYzytYzytUyw5/OVT6e8W2sXAgv9nEVERHygMydFREJMMBb3DKcDNEP5Wkb5Wkb5WibY8/nEL/Nxi4iI/wTjiFtERE7AkeJubtIq0+iP3tvzjDGjA5wv3RizxBiz3hiTb4y5+zjLnG+MOWSMyfV+PRrgjNuMMWu9z/2tjxtych0aYwYetV5yjTHlxph7jlkmoOvPGPOKMWavMWbdUdd1NsYsNsZs9v7bqYn7+n2StSbyPWWM2ej9/c0zxiQ1cd8Tvhb8mO8xY8zOo36Hk5q4r1Prb85R2bYZY3KbuK/f11+rs9YG9IvGQwoLgUwgDlgDDDlmmUnAe4ABzgRWBDhjGjDae7k9jcexH5vxfGBBoNffUc+/DUg5we2OrsNjft+lNB6j6tj6A8YDo4F1R133O+Ah7+WHgCebyH/C16sf810CxHgvP3m8fL68FvyY7zHgfh9+/46sv2NufwZ41Kn119pfToy4fZm06irgH7bRciDJGJMWqIDW2t3W2lXey4eBDYTe/CyOrsOjXAQUWmtP9YSsVmGtXQbsP+bqq4BXvZdfBa4+zl0DMsna8fJZaz+w1rq93y6n8axlRzSx/nzh2Po7whhjgBuA2a39vE5xorh9mbQqaCa2MsZkAKOAFce5+Wzv29j3jDGnBTQYWOBDY0yOMWb6cW4PlnV4E03/wTi5/gC6Wmt3ey+XAl2Ps0ywrMfbaHwHdTzNvRb86U7v7/CVJjY1BcP6OxfYY63d3MTtTq6/U6KdkydgjGm3xpf4AAACOElEQVQHvA3cY60tP+bmVUAva+1w4E/A/ADHG2etHUnjPOl3GGPGB/j5m2WMiQMmA28e52an19832Mb3zEF5iJUx5hHADcxqYhGnXgsv0LgJZCSwm8bNEcFoKicebQf939KxnChuXyat8mliK38yxsTSWNqzrLXvHHu7tbbcWlvhvbwQiDXGpAQqn7V2p/ffvcA8Gt+SHs3xdUjjH8Iqa+2eY29wev157Tmy+cj7797jLOPoejTG3ApcAXzX+5/Lt/jwWvALa+0ea63HWtsA/K2J53V6/cUAU4A5TS3j1PprCSeKeyXQ3xjTxzsiuwl495hl3gW+7z0y4kzg0FFvaf3Ou03sZWCDtfbZJpbp5l0OY8xYGtflvgDlSzTGtD9ymcadWOuOWczRdejV5EjHyfV3lHeBW7yXbwH+eZxlfHm9+oUxZiLwC2CytbaqiWV8eS34K9/R+0yuaeJ5HVt/XhOAjdbakuPd6OT6axEn9ojSeMRDAY17mx/xXvdj4Mfey4bGj0srBNYCWQHON47Gt815QK73a9IxGX8G5NO4l3w5cHYA82V6n3eNN0MwrsNEGou441HXObb+aPwPZDdQT+N21h8CycBHwGbgQ6Czd9nuwMITvV4DlG8LjduHj7wG/3psvqZeCwHKN9P72sqjsYzTgmn9ea//+5HX3FHLBnz9tfaXzpwUEQkx2jkpIhJiVNwiIiFGxS0iEmJU3CIiIUbFLSISYlTcIiIhRsUtIhJiVNwiIiHm/wCZzOvFi7flzAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffa0c6eca58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn_2/Reshape_1:0\", shape=(?, 5, 10), dtype=float32)\n",
      "Epoch: 0/1000 Iteration: 1 Train loss: 0.010574\n",
      "Epoch: 1/1000 Iteration: 3 Train loss: 0.010457\n",
      "Epoch: 2/1000 Iteration: 5 Train loss: 0.010531\n",
      "Epoch: 3/1000 Iteration: 7 Train loss: 0.010622\n",
      "Epoch: 4/1000 Iteration: 9 Train loss: 0.010721\n",
      "Epoch: 5/1000 Iteration: 11 Train loss: 0.010825\n",
      "Epoch: 6/1000 Iteration: 13 Train loss: 0.010933\n",
      "Epoch: 7/1000 Iteration: 15 Train loss: 0.011043\n",
      "Epoch: 8/1000 Iteration: 17 Train loss: 0.011156\n",
      "Epoch: 9/1000 Iteration: 19 Train loss: 0.011270\n",
      "Epoch: 10/1000 Iteration: 21 Train loss: 0.011385\n",
      "Epoch: 11/1000 Iteration: 23 Train loss: 0.011501\n",
      "Epoch: 12/1000 Iteration: 25 Train loss: 0.011617\n",
      "Epoch: 13/1000 Iteration: 27 Train loss: 0.011732\n",
      "Epoch: 14/1000 Iteration: 29 Train loss: 0.011847\n",
      "Epoch: 15/1000 Iteration: 31 Train loss: 0.011960\n",
      "Epoch: 16/1000 Iteration: 33 Train loss: 0.012072\n",
      "Epoch: 17/1000 Iteration: 35 Train loss: 0.012182\n",
      "Epoch: 18/1000 Iteration: 37 Train loss: 0.012290\n",
      "Epoch: 19/1000 Iteration: 39 Train loss: 0.012396\n",
      "Epoch: 20/1000 Iteration: 41 Train loss: 0.012499\n",
      "Epoch: 21/1000 Iteration: 43 Train loss: 0.012600\n",
      "Epoch: 22/1000 Iteration: 45 Train loss: 0.012697\n",
      "Epoch: 23/1000 Iteration: 47 Train loss: 0.012791\n",
      "Epoch: 24/1000 Iteration: 49 Train loss: 0.012882\n",
      "Epoch: 25/1000 Iteration: 51 Train loss: 0.012969\n",
      "Epoch: 26/1000 Iteration: 53 Train loss: 0.013052\n",
      "Epoch: 27/1000 Iteration: 55 Train loss: 0.013131\n",
      "Epoch: 28/1000 Iteration: 57 Train loss: 0.013207\n",
      "Epoch: 29/1000 Iteration: 59 Train loss: 0.013279\n",
      "Epoch: 30/1000 Iteration: 61 Train loss: 0.013346\n",
      "Epoch: 31/1000 Iteration: 63 Train loss: 0.013410\n",
      "Epoch: 32/1000 Iteration: 65 Train loss: 0.013469\n",
      "Epoch: 33/1000 Iteration: 67 Train loss: 0.013525\n",
      "Epoch: 34/1000 Iteration: 69 Train loss: 0.013576\n",
      "Epoch: 35/1000 Iteration: 71 Train loss: 0.013623\n",
      "Epoch: 36/1000 Iteration: 73 Train loss: 0.013665\n",
      "Epoch: 37/1000 Iteration: 75 Train loss: 0.013704\n",
      "Epoch: 38/1000 Iteration: 77 Train loss: 0.013739\n",
      "Epoch: 39/1000 Iteration: 79 Train loss: 0.013769\n",
      "Epoch: 40/1000 Iteration: 81 Train loss: 0.013796\n",
      "Epoch: 41/1000 Iteration: 83 Train loss: 0.013818\n",
      "Epoch: 42/1000 Iteration: 85 Train loss: 0.013837\n",
      "Epoch: 43/1000 Iteration: 87 Train loss: 0.013851\n",
      "Epoch: 44/1000 Iteration: 89 Train loss: 0.013862\n",
      "Epoch: 45/1000 Iteration: 91 Train loss: 0.013869\n",
      "Epoch: 46/1000 Iteration: 93 Train loss: 0.013873\n",
      "Epoch: 47/1000 Iteration: 95 Train loss: 0.013873\n",
      "Epoch: 48/1000 Iteration: 97 Train loss: 0.013869\n",
      "Epoch: 49/1000 Iteration: 99 Train loss: 0.013862\n",
      "Epoch: 50/1000 Iteration: 101 Train loss: 0.013852\n",
      "Epoch: 51/1000 Iteration: 103 Train loss: 0.013839\n",
      "Epoch: 52/1000 Iteration: 105 Train loss: 0.013822\n",
      "Epoch: 53/1000 Iteration: 107 Train loss: 0.013802\n",
      "Epoch: 54/1000 Iteration: 109 Train loss: 0.013780\n",
      "Epoch: 55/1000 Iteration: 111 Train loss: 0.013754\n",
      "Epoch: 56/1000 Iteration: 113 Train loss: 0.013726\n",
      "Epoch: 57/1000 Iteration: 115 Train loss: 0.013695\n",
      "Epoch: 58/1000 Iteration: 117 Train loss: 0.013662\n",
      "Epoch: 59/1000 Iteration: 119 Train loss: 0.013626\n",
      "Epoch: 60/1000 Iteration: 121 Train loss: 0.013588\n",
      "Epoch: 61/1000 Iteration: 123 Train loss: 0.013547\n",
      "Epoch: 62/1000 Iteration: 125 Train loss: 0.013505\n",
      "Epoch: 63/1000 Iteration: 127 Train loss: 0.013460\n",
      "Epoch: 64/1000 Iteration: 129 Train loss: 0.013413\n",
      "Epoch: 65/1000 Iteration: 131 Train loss: 0.013365\n",
      "Epoch: 66/1000 Iteration: 133 Train loss: 0.013314\n",
      "Epoch: 67/1000 Iteration: 135 Train loss: 0.013262\n",
      "Epoch: 68/1000 Iteration: 137 Train loss: 0.013208\n",
      "Epoch: 69/1000 Iteration: 139 Train loss: 0.013153\n",
      "Epoch: 70/1000 Iteration: 141 Train loss: 0.013096\n",
      "Epoch: 71/1000 Iteration: 143 Train loss: 0.013038\n",
      "Epoch: 72/1000 Iteration: 145 Train loss: 0.012978\n",
      "Epoch: 73/1000 Iteration: 147 Train loss: 0.012917\n",
      "Epoch: 74/1000 Iteration: 149 Train loss: 0.012855\n",
      "Epoch: 75/1000 Iteration: 151 Train loss: 0.012792\n",
      "Epoch: 76/1000 Iteration: 153 Train loss: 0.012728\n",
      "Epoch: 77/1000 Iteration: 155 Train loss: 0.012663\n",
      "Epoch: 78/1000 Iteration: 157 Train loss: 0.012598\n",
      "Epoch: 79/1000 Iteration: 159 Train loss: 0.012531\n",
      "Epoch: 80/1000 Iteration: 161 Train loss: 0.012464\n",
      "Epoch: 81/1000 Iteration: 163 Train loss: 0.012396\n",
      "Epoch: 82/1000 Iteration: 165 Train loss: 0.012327\n",
      "Epoch: 83/1000 Iteration: 167 Train loss: 0.012258\n",
      "Epoch: 84/1000 Iteration: 169 Train loss: 0.012189\n",
      "Epoch: 85/1000 Iteration: 171 Train loss: 0.012119\n",
      "Epoch: 86/1000 Iteration: 173 Train loss: 0.012048\n",
      "Epoch: 87/1000 Iteration: 175 Train loss: 0.011977\n",
      "Epoch: 88/1000 Iteration: 177 Train loss: 0.011906\n",
      "Epoch: 89/1000 Iteration: 179 Train loss: 0.011835\n",
      "Epoch: 90/1000 Iteration: 181 Train loss: 0.011764\n",
      "Epoch: 91/1000 Iteration: 183 Train loss: 0.011692\n",
      "Epoch: 92/1000 Iteration: 185 Train loss: 0.011621\n",
      "Epoch: 93/1000 Iteration: 187 Train loss: 0.011549\n",
      "Epoch: 94/1000 Iteration: 189 Train loss: 0.011478\n",
      "Epoch: 95/1000 Iteration: 191 Train loss: 0.011406\n",
      "Epoch: 96/1000 Iteration: 193 Train loss: 0.011335\n",
      "Epoch: 97/1000 Iteration: 195 Train loss: 0.011263\n",
      "Epoch: 98/1000 Iteration: 197 Train loss: 0.011192\n",
      "Epoch: 99/1000 Iteration: 199 Train loss: 0.011121\n",
      "Epoch: 100/1000 Iteration: 201 Train loss: 0.011050\n",
      "Epoch: 101/1000 Iteration: 203 Train loss: 0.010979\n",
      "Epoch: 102/1000 Iteration: 205 Train loss: 0.010909\n",
      "Epoch: 103/1000 Iteration: 207 Train loss: 0.010839\n",
      "Epoch: 104/1000 Iteration: 209 Train loss: 0.010769\n",
      "Epoch: 105/1000 Iteration: 211 Train loss: 0.010699\n",
      "Epoch: 106/1000 Iteration: 213 Train loss: 0.010630\n",
      "Epoch: 107/1000 Iteration: 215 Train loss: 0.010562\n",
      "Epoch: 108/1000 Iteration: 217 Train loss: 0.010493\n",
      "Epoch: 109/1000 Iteration: 219 Train loss: 0.010425\n",
      "Epoch: 110/1000 Iteration: 221 Train loss: 0.010358\n",
      "Epoch: 111/1000 Iteration: 223 Train loss: 0.010291\n",
      "Epoch: 112/1000 Iteration: 225 Train loss: 0.010225\n",
      "Epoch: 113/1000 Iteration: 227 Train loss: 0.010159\n",
      "Epoch: 114/1000 Iteration: 229 Train loss: 0.010093\n",
      "Epoch: 115/1000 Iteration: 231 Train loss: 0.010028\n",
      "Epoch: 116/1000 Iteration: 233 Train loss: 0.009964\n",
      "Epoch: 117/1000 Iteration: 235 Train loss: 0.009900\n",
      "Epoch: 118/1000 Iteration: 237 Train loss: 0.009836\n",
      "Epoch: 119/1000 Iteration: 239 Train loss: 0.009773\n",
      "Epoch: 120/1000 Iteration: 241 Train loss: 0.009711\n",
      "Epoch: 121/1000 Iteration: 243 Train loss: 0.009650\n",
      "Epoch: 122/1000 Iteration: 245 Train loss: 0.009589\n",
      "Epoch: 123/1000 Iteration: 247 Train loss: 0.009528\n",
      "Epoch: 124/1000 Iteration: 249 Train loss: 0.009468\n",
      "Epoch: 125/1000 Iteration: 251 Train loss: 0.009409\n",
      "Epoch: 126/1000 Iteration: 253 Train loss: 0.009350\n",
      "Epoch: 127/1000 Iteration: 255 Train loss: 0.009292\n",
      "Epoch: 128/1000 Iteration: 257 Train loss: 0.009235\n",
      "Epoch: 129/1000 Iteration: 259 Train loss: 0.009178\n",
      "Epoch: 130/1000 Iteration: 261 Train loss: 0.009122\n",
      "Epoch: 131/1000 Iteration: 263 Train loss: 0.009067\n",
      "Epoch: 132/1000 Iteration: 265 Train loss: 0.009012\n",
      "Epoch: 133/1000 Iteration: 267 Train loss: 0.008958\n",
      "Epoch: 134/1000 Iteration: 269 Train loss: 0.008904\n",
      "Epoch: 135/1000 Iteration: 271 Train loss: 0.008851\n",
      "Epoch: 136/1000 Iteration: 273 Train loss: 0.008799\n",
      "Epoch: 137/1000 Iteration: 275 Train loss: 0.008747\n",
      "Epoch: 138/1000 Iteration: 277 Train loss: 0.008696\n",
      "Epoch: 139/1000 Iteration: 279 Train loss: 0.008646\n",
      "Epoch: 140/1000 Iteration: 281 Train loss: 0.008596\n",
      "Epoch: 141/1000 Iteration: 283 Train loss: 0.008547\n",
      "Epoch: 142/1000 Iteration: 285 Train loss: 0.008499\n",
      "Epoch: 143/1000 Iteration: 287 Train loss: 0.008451\n",
      "Epoch: 144/1000 Iteration: 289 Train loss: 0.008404\n",
      "Epoch: 145/1000 Iteration: 291 Train loss: 0.008358\n",
      "Epoch: 146/1000 Iteration: 293 Train loss: 0.008312\n",
      "Epoch: 147/1000 Iteration: 295 Train loss: 0.008266\n",
      "Epoch: 148/1000 Iteration: 297 Train loss: 0.008222\n",
      "Epoch: 149/1000 Iteration: 299 Train loss: 0.008178\n",
      "Epoch: 150/1000 Iteration: 301 Train loss: 0.008135\n",
      "Epoch: 151/1000 Iteration: 303 Train loss: 0.008092\n",
      "Epoch: 152/1000 Iteration: 305 Train loss: 0.008050\n",
      "Epoch: 153/1000 Iteration: 307 Train loss: 0.008008\n",
      "Epoch: 154/1000 Iteration: 309 Train loss: 0.007967\n",
      "Epoch: 155/1000 Iteration: 311 Train loss: 0.007927\n",
      "Epoch: 156/1000 Iteration: 313 Train loss: 0.007887\n",
      "Epoch: 157/1000 Iteration: 315 Train loss: 0.007848\n",
      "Epoch: 158/1000 Iteration: 317 Train loss: 0.007810\n",
      "Epoch: 159/1000 Iteration: 319 Train loss: 0.007772\n",
      "Epoch: 160/1000 Iteration: 321 Train loss: 0.007734\n",
      "Epoch: 161/1000 Iteration: 323 Train loss: 0.007697\n",
      "Epoch: 162/1000 Iteration: 325 Train loss: 0.007661\n",
      "Epoch: 163/1000 Iteration: 327 Train loss: 0.007626\n",
      "Epoch: 164/1000 Iteration: 329 Train loss: 0.007590\n",
      "Epoch: 165/1000 Iteration: 331 Train loss: 0.007556\n",
      "Epoch: 166/1000 Iteration: 333 Train loss: 0.007522\n",
      "Epoch: 167/1000 Iteration: 335 Train loss: 0.007488\n",
      "Epoch: 168/1000 Iteration: 337 Train loss: 0.007456\n",
      "Epoch: 169/1000 Iteration: 339 Train loss: 0.007423\n",
      "Epoch: 170/1000 Iteration: 341 Train loss: 0.007391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 171/1000 Iteration: 343 Train loss: 0.007360\n",
      "Epoch: 172/1000 Iteration: 345 Train loss: 0.007329\n",
      "Epoch: 173/1000 Iteration: 347 Train loss: 0.007299\n",
      "Epoch: 174/1000 Iteration: 349 Train loss: 0.007269\n",
      "Epoch: 175/1000 Iteration: 351 Train loss: 0.007240\n",
      "Epoch: 176/1000 Iteration: 353 Train loss: 0.007211\n",
      "Epoch: 177/1000 Iteration: 355 Train loss: 0.007183\n",
      "Epoch: 178/1000 Iteration: 357 Train loss: 0.007155\n",
      "Epoch: 179/1000 Iteration: 359 Train loss: 0.007127\n",
      "Epoch: 180/1000 Iteration: 361 Train loss: 0.007101\n",
      "Epoch: 181/1000 Iteration: 363 Train loss: 0.007074\n",
      "Epoch: 182/1000 Iteration: 365 Train loss: 0.007048\n",
      "Epoch: 183/1000 Iteration: 367 Train loss: 0.007023\n",
      "Epoch: 184/1000 Iteration: 369 Train loss: 0.006998\n",
      "Epoch: 185/1000 Iteration: 371 Train loss: 0.006973\n",
      "Epoch: 186/1000 Iteration: 373 Train loss: 0.006949\n",
      "Epoch: 187/1000 Iteration: 375 Train loss: 0.006925\n",
      "Epoch: 188/1000 Iteration: 377 Train loss: 0.006902\n",
      "Epoch: 189/1000 Iteration: 379 Train loss: 0.006879\n",
      "Epoch: 190/1000 Iteration: 381 Train loss: 0.006856\n",
      "Epoch: 191/1000 Iteration: 383 Train loss: 0.006834\n",
      "Epoch: 192/1000 Iteration: 385 Train loss: 0.006813\n",
      "Epoch: 193/1000 Iteration: 387 Train loss: 0.006791\n",
      "Epoch: 194/1000 Iteration: 389 Train loss: 0.006770\n",
      "Epoch: 195/1000 Iteration: 391 Train loss: 0.006750\n",
      "Epoch: 196/1000 Iteration: 393 Train loss: 0.006730\n",
      "Epoch: 197/1000 Iteration: 395 Train loss: 0.006710\n",
      "Epoch: 198/1000 Iteration: 397 Train loss: 0.006691\n",
      "Epoch: 199/1000 Iteration: 399 Train loss: 0.006672\n",
      "Epoch: 200/1000 Iteration: 401 Train loss: 0.006653\n",
      "Epoch: 201/1000 Iteration: 403 Train loss: 0.006635\n",
      "Epoch: 202/1000 Iteration: 405 Train loss: 0.006617\n",
      "Epoch: 203/1000 Iteration: 407 Train loss: 0.006599\n",
      "Epoch: 204/1000 Iteration: 409 Train loss: 0.006582\n",
      "Epoch: 205/1000 Iteration: 411 Train loss: 0.006565\n",
      "Epoch: 206/1000 Iteration: 413 Train loss: 0.006548\n",
      "Epoch: 207/1000 Iteration: 415 Train loss: 0.006532\n",
      "Epoch: 208/1000 Iteration: 417 Train loss: 0.006516\n",
      "Epoch: 209/1000 Iteration: 419 Train loss: 0.006500\n",
      "Epoch: 210/1000 Iteration: 421 Train loss: 0.006485\n",
      "Epoch: 211/1000 Iteration: 423 Train loss: 0.006469\n",
      "Epoch: 212/1000 Iteration: 425 Train loss: 0.006455\n",
      "Epoch: 213/1000 Iteration: 427 Train loss: 0.006440\n",
      "Epoch: 214/1000 Iteration: 429 Train loss: 0.006426\n",
      "Epoch: 215/1000 Iteration: 431 Train loss: 0.006412\n",
      "Epoch: 216/1000 Iteration: 433 Train loss: 0.006398\n",
      "Epoch: 217/1000 Iteration: 435 Train loss: 0.006385\n",
      "Epoch: 218/1000 Iteration: 437 Train loss: 0.006372\n",
      "Epoch: 219/1000 Iteration: 439 Train loss: 0.006359\n",
      "Epoch: 220/1000 Iteration: 441 Train loss: 0.006346\n",
      "Epoch: 221/1000 Iteration: 443 Train loss: 0.006334\n",
      "Epoch: 222/1000 Iteration: 445 Train loss: 0.006322\n",
      "Epoch: 223/1000 Iteration: 447 Train loss: 0.006310\n",
      "Epoch: 224/1000 Iteration: 449 Train loss: 0.006298\n",
      "Epoch: 225/1000 Iteration: 451 Train loss: 0.006287\n",
      "Epoch: 226/1000 Iteration: 453 Train loss: 0.006275\n",
      "Epoch: 227/1000 Iteration: 455 Train loss: 0.006264\n",
      "Epoch: 228/1000 Iteration: 457 Train loss: 0.006254\n",
      "Epoch: 229/1000 Iteration: 459 Train loss: 0.006243\n",
      "Epoch: 230/1000 Iteration: 461 Train loss: 0.006233\n",
      "Epoch: 231/1000 Iteration: 463 Train loss: 0.006222\n",
      "Epoch: 232/1000 Iteration: 465 Train loss: 0.006213\n",
      "Epoch: 233/1000 Iteration: 467 Train loss: 0.006203\n",
      "Epoch: 234/1000 Iteration: 469 Train loss: 0.006193\n",
      "Epoch: 235/1000 Iteration: 471 Train loss: 0.006184\n",
      "Epoch: 236/1000 Iteration: 473 Train loss: 0.006175\n",
      "Epoch: 237/1000 Iteration: 475 Train loss: 0.006166\n",
      "Epoch: 238/1000 Iteration: 477 Train loss: 0.006157\n",
      "Epoch: 239/1000 Iteration: 479 Train loss: 0.006148\n",
      "Epoch: 240/1000 Iteration: 481 Train loss: 0.006140\n",
      "Epoch: 241/1000 Iteration: 483 Train loss: 0.006131\n",
      "Epoch: 242/1000 Iteration: 485 Train loss: 0.006123\n",
      "Epoch: 243/1000 Iteration: 487 Train loss: 0.006115\n",
      "Epoch: 244/1000 Iteration: 489 Train loss: 0.006107\n",
      "Epoch: 245/1000 Iteration: 491 Train loss: 0.006100\n",
      "Epoch: 246/1000 Iteration: 493 Train loss: 0.006092\n",
      "Epoch: 247/1000 Iteration: 495 Train loss: 0.006085\n",
      "Epoch: 248/1000 Iteration: 497 Train loss: 0.006078\n",
      "Epoch: 249/1000 Iteration: 499 Train loss: 0.006071\n",
      "Epoch: 250/1000 Iteration: 501 Train loss: 0.006064\n",
      "Epoch: 251/1000 Iteration: 503 Train loss: 0.006057\n",
      "Epoch: 252/1000 Iteration: 505 Train loss: 0.006050\n",
      "Epoch: 253/1000 Iteration: 507 Train loss: 0.006044\n",
      "Epoch: 254/1000 Iteration: 509 Train loss: 0.006037\n",
      "Epoch: 255/1000 Iteration: 511 Train loss: 0.006031\n",
      "Epoch: 256/1000 Iteration: 513 Train loss: 0.006025\n",
      "Epoch: 257/1000 Iteration: 515 Train loss: 0.006018\n",
      "Epoch: 258/1000 Iteration: 517 Train loss: 0.006012\n",
      "Epoch: 259/1000 Iteration: 519 Train loss: 0.006007\n",
      "Epoch: 260/1000 Iteration: 521 Train loss: 0.006001\n",
      "Epoch: 261/1000 Iteration: 523 Train loss: 0.005995\n",
      "Epoch: 262/1000 Iteration: 525 Train loss: 0.005990\n",
      "Epoch: 263/1000 Iteration: 527 Train loss: 0.005984\n",
      "Epoch: 264/1000 Iteration: 529 Train loss: 0.005979\n",
      "Epoch: 265/1000 Iteration: 531 Train loss: 0.005974\n",
      "Epoch: 266/1000 Iteration: 533 Train loss: 0.005968\n",
      "Epoch: 267/1000 Iteration: 535 Train loss: 0.005963\n",
      "Epoch: 268/1000 Iteration: 537 Train loss: 0.005958\n",
      "Epoch: 269/1000 Iteration: 539 Train loss: 0.005953\n",
      "Epoch: 270/1000 Iteration: 541 Train loss: 0.005949\n",
      "Epoch: 271/1000 Iteration: 543 Train loss: 0.005944\n",
      "Epoch: 272/1000 Iteration: 545 Train loss: 0.005939\n",
      "Epoch: 273/1000 Iteration: 547 Train loss: 0.005935\n",
      "Epoch: 274/1000 Iteration: 549 Train loss: 0.005930\n",
      "Epoch: 275/1000 Iteration: 551 Train loss: 0.005926\n",
      "Epoch: 276/1000 Iteration: 553 Train loss: 0.005921\n",
      "Epoch: 277/1000 Iteration: 555 Train loss: 0.005917\n",
      "Epoch: 278/1000 Iteration: 557 Train loss: 0.005913\n",
      "Epoch: 279/1000 Iteration: 559 Train loss: 0.005908\n",
      "Epoch: 280/1000 Iteration: 561 Train loss: 0.005904\n",
      "Epoch: 281/1000 Iteration: 563 Train loss: 0.005900\n",
      "Epoch: 282/1000 Iteration: 565 Train loss: 0.005896\n",
      "Epoch: 283/1000 Iteration: 567 Train loss: 0.005892\n",
      "Epoch: 284/1000 Iteration: 569 Train loss: 0.005888\n",
      "Epoch: 285/1000 Iteration: 571 Train loss: 0.005885\n",
      "Epoch: 286/1000 Iteration: 573 Train loss: 0.005881\n",
      "Epoch: 287/1000 Iteration: 575 Train loss: 0.005877\n",
      "Epoch: 288/1000 Iteration: 577 Train loss: 0.005874\n",
      "Epoch: 289/1000 Iteration: 579 Train loss: 0.005870\n",
      "Epoch: 290/1000 Iteration: 581 Train loss: 0.005866\n",
      "Epoch: 291/1000 Iteration: 583 Train loss: 0.005863\n",
      "Epoch: 292/1000 Iteration: 585 Train loss: 0.005859\n",
      "Epoch: 293/1000 Iteration: 587 Train loss: 0.005856\n",
      "Epoch: 294/1000 Iteration: 589 Train loss: 0.005852\n",
      "Epoch: 295/1000 Iteration: 591 Train loss: 0.005849\n",
      "Epoch: 296/1000 Iteration: 593 Train loss: 0.005846\n",
      "Epoch: 297/1000 Iteration: 595 Train loss: 0.005842\n",
      "Epoch: 298/1000 Iteration: 597 Train loss: 0.005839\n",
      "Epoch: 299/1000 Iteration: 599 Train loss: 0.005836\n",
      "Epoch: 300/1000 Iteration: 601 Train loss: 0.005833\n",
      "Epoch: 301/1000 Iteration: 603 Train loss: 0.005830\n",
      "Epoch: 302/1000 Iteration: 605 Train loss: 0.005827\n",
      "Epoch: 303/1000 Iteration: 607 Train loss: 0.005823\n",
      "Epoch: 304/1000 Iteration: 609 Train loss: 0.005820\n",
      "Epoch: 305/1000 Iteration: 611 Train loss: 0.005817\n",
      "Epoch: 306/1000 Iteration: 613 Train loss: 0.005814\n",
      "Epoch: 307/1000 Iteration: 615 Train loss: 0.005811\n",
      "Epoch: 308/1000 Iteration: 617 Train loss: 0.005808\n",
      "Epoch: 309/1000 Iteration: 619 Train loss: 0.005806\n",
      "Epoch: 310/1000 Iteration: 621 Train loss: 0.005803\n",
      "Epoch: 311/1000 Iteration: 623 Train loss: 0.005800\n",
      "Epoch: 312/1000 Iteration: 625 Train loss: 0.005797\n",
      "Epoch: 313/1000 Iteration: 627 Train loss: 0.005794\n",
      "Epoch: 314/1000 Iteration: 629 Train loss: 0.005791\n",
      "Epoch: 315/1000 Iteration: 631 Train loss: 0.005788\n",
      "Epoch: 316/1000 Iteration: 633 Train loss: 0.005786\n",
      "Epoch: 317/1000 Iteration: 635 Train loss: 0.005783\n",
      "Epoch: 318/1000 Iteration: 637 Train loss: 0.005780\n",
      "Epoch: 319/1000 Iteration: 639 Train loss: 0.005777\n",
      "Epoch: 320/1000 Iteration: 641 Train loss: 0.005775\n",
      "Epoch: 321/1000 Iteration: 643 Train loss: 0.005772\n",
      "Epoch: 322/1000 Iteration: 645 Train loss: 0.005769\n",
      "Epoch: 323/1000 Iteration: 647 Train loss: 0.005767\n",
      "Epoch: 324/1000 Iteration: 649 Train loss: 0.005764\n",
      "Epoch: 325/1000 Iteration: 651 Train loss: 0.005761\n",
      "Epoch: 326/1000 Iteration: 653 Train loss: 0.005759\n",
      "Epoch: 327/1000 Iteration: 655 Train loss: 0.005756\n",
      "Epoch: 328/1000 Iteration: 657 Train loss: 0.005754\n",
      "Epoch: 329/1000 Iteration: 659 Train loss: 0.005751\n",
      "Epoch: 330/1000 Iteration: 661 Train loss: 0.005748\n",
      "Epoch: 331/1000 Iteration: 663 Train loss: 0.005746\n",
      "Epoch: 332/1000 Iteration: 665 Train loss: 0.005743\n",
      "Epoch: 333/1000 Iteration: 667 Train loss: 0.005741\n",
      "Epoch: 334/1000 Iteration: 669 Train loss: 0.005738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 335/1000 Iteration: 671 Train loss: 0.005736\n",
      "Epoch: 336/1000 Iteration: 673 Train loss: 0.005733\n",
      "Epoch: 337/1000 Iteration: 675 Train loss: 0.005731\n",
      "Epoch: 338/1000 Iteration: 677 Train loss: 0.005728\n",
      "Epoch: 339/1000 Iteration: 679 Train loss: 0.005726\n",
      "Epoch: 340/1000 Iteration: 681 Train loss: 0.005723\n",
      "Epoch: 341/1000 Iteration: 683 Train loss: 0.005721\n",
      "Epoch: 342/1000 Iteration: 685 Train loss: 0.005718\n",
      "Epoch: 343/1000 Iteration: 687 Train loss: 0.005716\n",
      "Epoch: 344/1000 Iteration: 689 Train loss: 0.005713\n",
      "Epoch: 345/1000 Iteration: 691 Train loss: 0.005711\n",
      "Epoch: 346/1000 Iteration: 693 Train loss: 0.005708\n",
      "Epoch: 347/1000 Iteration: 695 Train loss: 0.005706\n",
      "Epoch: 348/1000 Iteration: 697 Train loss: 0.005703\n",
      "Epoch: 349/1000 Iteration: 699 Train loss: 0.005701\n",
      "Epoch: 350/1000 Iteration: 701 Train loss: 0.005699\n",
      "Epoch: 351/1000 Iteration: 703 Train loss: 0.005696\n",
      "Epoch: 352/1000 Iteration: 705 Train loss: 0.005694\n",
      "Epoch: 353/1000 Iteration: 707 Train loss: 0.005691\n",
      "Epoch: 354/1000 Iteration: 709 Train loss: 0.005689\n",
      "Epoch: 355/1000 Iteration: 711 Train loss: 0.005686\n",
      "Epoch: 356/1000 Iteration: 713 Train loss: 0.005684\n",
      "Epoch: 357/1000 Iteration: 715 Train loss: 0.005682\n",
      "Epoch: 358/1000 Iteration: 717 Train loss: 0.005679\n",
      "Epoch: 359/1000 Iteration: 719 Train loss: 0.005677\n",
      "Epoch: 360/1000 Iteration: 721 Train loss: 0.005674\n",
      "Epoch: 361/1000 Iteration: 723 Train loss: 0.005672\n",
      "Epoch: 362/1000 Iteration: 725 Train loss: 0.005669\n",
      "Epoch: 363/1000 Iteration: 727 Train loss: 0.005667\n",
      "Epoch: 364/1000 Iteration: 729 Train loss: 0.005665\n",
      "Epoch: 365/1000 Iteration: 731 Train loss: 0.005662\n",
      "Epoch: 366/1000 Iteration: 733 Train loss: 0.005660\n",
      "Epoch: 367/1000 Iteration: 735 Train loss: 0.005657\n",
      "Epoch: 368/1000 Iteration: 737 Train loss: 0.005655\n",
      "Epoch: 369/1000 Iteration: 739 Train loss: 0.005653\n",
      "Epoch: 370/1000 Iteration: 741 Train loss: 0.005650\n",
      "Epoch: 371/1000 Iteration: 743 Train loss: 0.005648\n",
      "Epoch: 372/1000 Iteration: 745 Train loss: 0.005645\n",
      "Epoch: 373/1000 Iteration: 747 Train loss: 0.005643\n",
      "Epoch: 374/1000 Iteration: 749 Train loss: 0.005641\n",
      "Epoch: 375/1000 Iteration: 751 Train loss: 0.005638\n",
      "Epoch: 376/1000 Iteration: 753 Train loss: 0.005636\n",
      "Epoch: 377/1000 Iteration: 755 Train loss: 0.005633\n",
      "Epoch: 378/1000 Iteration: 757 Train loss: 0.005631\n",
      "Epoch: 379/1000 Iteration: 759 Train loss: 0.005628\n",
      "Epoch: 380/1000 Iteration: 761 Train loss: 0.005626\n",
      "Epoch: 381/1000 Iteration: 763 Train loss: 0.005624\n",
      "Epoch: 382/1000 Iteration: 765 Train loss: 0.005621\n",
      "Epoch: 383/1000 Iteration: 767 Train loss: 0.005619\n",
      "Epoch: 384/1000 Iteration: 769 Train loss: 0.005616\n",
      "Epoch: 385/1000 Iteration: 771 Train loss: 0.005614\n",
      "Epoch: 386/1000 Iteration: 773 Train loss: 0.005612\n",
      "Epoch: 387/1000 Iteration: 775 Train loss: 0.005609\n",
      "Epoch: 388/1000 Iteration: 777 Train loss: 0.005607\n",
      "Epoch: 389/1000 Iteration: 779 Train loss: 0.005604\n",
      "Epoch: 390/1000 Iteration: 781 Train loss: 0.005602\n",
      "Epoch: 391/1000 Iteration: 783 Train loss: 0.005600\n",
      "Epoch: 392/1000 Iteration: 785 Train loss: 0.005597\n",
      "Epoch: 393/1000 Iteration: 787 Train loss: 0.005595\n",
      "Epoch: 394/1000 Iteration: 789 Train loss: 0.005592\n",
      "Epoch: 395/1000 Iteration: 791 Train loss: 0.005590\n",
      "Epoch: 396/1000 Iteration: 793 Train loss: 0.005587\n",
      "Epoch: 397/1000 Iteration: 795 Train loss: 0.005585\n",
      "Epoch: 398/1000 Iteration: 797 Train loss: 0.005583\n",
      "Epoch: 399/1000 Iteration: 799 Train loss: 0.005580\n",
      "Epoch: 400/1000 Iteration: 801 Train loss: 0.005578\n",
      "Epoch: 401/1000 Iteration: 803 Train loss: 0.005575\n",
      "Epoch: 402/1000 Iteration: 805 Train loss: 0.005573\n",
      "Epoch: 403/1000 Iteration: 807 Train loss: 0.005570\n",
      "Epoch: 404/1000 Iteration: 809 Train loss: 0.005568\n",
      "Epoch: 405/1000 Iteration: 811 Train loss: 0.005566\n",
      "Epoch: 406/1000 Iteration: 813 Train loss: 0.005563\n",
      "Epoch: 407/1000 Iteration: 815 Train loss: 0.005561\n",
      "Epoch: 408/1000 Iteration: 817 Train loss: 0.005558\n",
      "Epoch: 409/1000 Iteration: 819 Train loss: 0.005556\n",
      "Epoch: 410/1000 Iteration: 821 Train loss: 0.005553\n",
      "Epoch: 411/1000 Iteration: 823 Train loss: 0.005551\n",
      "Epoch: 412/1000 Iteration: 825 Train loss: 0.005549\n",
      "Epoch: 413/1000 Iteration: 827 Train loss: 0.005546\n",
      "Epoch: 414/1000 Iteration: 829 Train loss: 0.005544\n",
      "Epoch: 415/1000 Iteration: 831 Train loss: 0.005541\n",
      "Epoch: 416/1000 Iteration: 833 Train loss: 0.005539\n",
      "Epoch: 417/1000 Iteration: 835 Train loss: 0.005536\n",
      "Epoch: 418/1000 Iteration: 837 Train loss: 0.005534\n",
      "Epoch: 419/1000 Iteration: 839 Train loss: 0.005532\n",
      "Epoch: 420/1000 Iteration: 841 Train loss: 0.005529\n",
      "Epoch: 421/1000 Iteration: 843 Train loss: 0.005527\n",
      "Epoch: 422/1000 Iteration: 845 Train loss: 0.005524\n",
      "Epoch: 423/1000 Iteration: 847 Train loss: 0.005522\n",
      "Epoch: 424/1000 Iteration: 849 Train loss: 0.005519\n",
      "Epoch: 425/1000 Iteration: 851 Train loss: 0.005517\n",
      "Epoch: 426/1000 Iteration: 853 Train loss: 0.005514\n",
      "Epoch: 427/1000 Iteration: 855 Train loss: 0.005512\n",
      "Epoch: 428/1000 Iteration: 857 Train loss: 0.005510\n",
      "Epoch: 429/1000 Iteration: 859 Train loss: 0.005507\n",
      "Epoch: 430/1000 Iteration: 861 Train loss: 0.005505\n",
      "Epoch: 431/1000 Iteration: 863 Train loss: 0.005502\n",
      "Epoch: 432/1000 Iteration: 865 Train loss: 0.005500\n",
      "Epoch: 433/1000 Iteration: 867 Train loss: 0.005497\n",
      "Epoch: 434/1000 Iteration: 869 Train loss: 0.005495\n",
      "Epoch: 435/1000 Iteration: 871 Train loss: 0.005492\n",
      "Epoch: 436/1000 Iteration: 873 Train loss: 0.005490\n",
      "Epoch: 437/1000 Iteration: 875 Train loss: 0.005487\n",
      "Epoch: 438/1000 Iteration: 877 Train loss: 0.005485\n",
      "Epoch: 439/1000 Iteration: 879 Train loss: 0.005483\n",
      "Epoch: 440/1000 Iteration: 881 Train loss: 0.005480\n",
      "Epoch: 441/1000 Iteration: 883 Train loss: 0.005478\n",
      "Epoch: 442/1000 Iteration: 885 Train loss: 0.005475\n",
      "Epoch: 443/1000 Iteration: 887 Train loss: 0.005473\n",
      "Epoch: 444/1000 Iteration: 889 Train loss: 0.005470\n",
      "Epoch: 445/1000 Iteration: 891 Train loss: 0.005468\n",
      "Epoch: 446/1000 Iteration: 893 Train loss: 0.005465\n",
      "Epoch: 447/1000 Iteration: 895 Train loss: 0.005463\n",
      "Epoch: 448/1000 Iteration: 897 Train loss: 0.005460\n",
      "Epoch: 449/1000 Iteration: 899 Train loss: 0.005458\n",
      "Epoch: 450/1000 Iteration: 901 Train loss: 0.005456\n",
      "Epoch: 451/1000 Iteration: 903 Train loss: 0.005453\n",
      "Epoch: 452/1000 Iteration: 905 Train loss: 0.005451\n",
      "Epoch: 453/1000 Iteration: 907 Train loss: 0.005448\n",
      "Epoch: 454/1000 Iteration: 909 Train loss: 0.005446\n",
      "Epoch: 455/1000 Iteration: 911 Train loss: 0.005443\n",
      "Epoch: 456/1000 Iteration: 913 Train loss: 0.005441\n",
      "Epoch: 457/1000 Iteration: 915 Train loss: 0.005438\n",
      "Epoch: 458/1000 Iteration: 917 Train loss: 0.005436\n",
      "Epoch: 459/1000 Iteration: 919 Train loss: 0.005433\n",
      "Epoch: 460/1000 Iteration: 921 Train loss: 0.005431\n",
      "Epoch: 461/1000 Iteration: 923 Train loss: 0.005428\n",
      "Epoch: 462/1000 Iteration: 925 Train loss: 0.005426\n",
      "Epoch: 463/1000 Iteration: 927 Train loss: 0.005424\n",
      "Epoch: 464/1000 Iteration: 929 Train loss: 0.005421\n",
      "Epoch: 465/1000 Iteration: 931 Train loss: 0.005419\n",
      "Epoch: 466/1000 Iteration: 933 Train loss: 0.005416\n",
      "Epoch: 467/1000 Iteration: 935 Train loss: 0.005414\n",
      "Epoch: 468/1000 Iteration: 937 Train loss: 0.005411\n",
      "Epoch: 469/1000 Iteration: 939 Train loss: 0.005409\n",
      "Epoch: 470/1000 Iteration: 941 Train loss: 0.005406\n",
      "Epoch: 471/1000 Iteration: 943 Train loss: 0.005404\n",
      "Epoch: 472/1000 Iteration: 945 Train loss: 0.005401\n",
      "Epoch: 473/1000 Iteration: 947 Train loss: 0.005399\n",
      "Epoch: 474/1000 Iteration: 949 Train loss: 0.005397\n",
      "Epoch: 475/1000 Iteration: 951 Train loss: 0.005394\n",
      "Epoch: 476/1000 Iteration: 953 Train loss: 0.005392\n",
      "Epoch: 477/1000 Iteration: 955 Train loss: 0.005389\n",
      "Epoch: 478/1000 Iteration: 957 Train loss: 0.005387\n",
      "Epoch: 479/1000 Iteration: 959 Train loss: 0.005384\n",
      "Epoch: 480/1000 Iteration: 961 Train loss: 0.005382\n",
      "Epoch: 481/1000 Iteration: 963 Train loss: 0.005379\n",
      "Epoch: 482/1000 Iteration: 965 Train loss: 0.005377\n",
      "Epoch: 483/1000 Iteration: 967 Train loss: 0.005374\n",
      "Epoch: 484/1000 Iteration: 969 Train loss: 0.005372\n",
      "Epoch: 485/1000 Iteration: 971 Train loss: 0.005370\n",
      "Epoch: 486/1000 Iteration: 973 Train loss: 0.005367\n",
      "Epoch: 487/1000 Iteration: 975 Train loss: 0.005365\n",
      "Epoch: 488/1000 Iteration: 977 Train loss: 0.005362\n",
      "Epoch: 489/1000 Iteration: 979 Train loss: 0.005360\n",
      "Epoch: 490/1000 Iteration: 981 Train loss: 0.005357\n",
      "Epoch: 491/1000 Iteration: 983 Train loss: 0.005355\n",
      "Epoch: 492/1000 Iteration: 985 Train loss: 0.005352\n",
      "Epoch: 493/1000 Iteration: 987 Train loss: 0.005350\n",
      "Epoch: 494/1000 Iteration: 989 Train loss: 0.005347\n",
      "Epoch: 495/1000 Iteration: 991 Train loss: 0.005345\n",
      "Epoch: 496/1000 Iteration: 993 Train loss: 0.005343\n",
      "Epoch: 497/1000 Iteration: 995 Train loss: 0.005340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 498/1000 Iteration: 997 Train loss: 0.005338\n",
      "Epoch: 499/1000 Iteration: 999 Train loss: 0.005335\n",
      "Epoch: 500/1000 Iteration: 1001 Train loss: 0.005333\n",
      "Epoch: 501/1000 Iteration: 1003 Train loss: 0.005330\n",
      "Epoch: 502/1000 Iteration: 1005 Train loss: 0.005328\n",
      "Epoch: 503/1000 Iteration: 1007 Train loss: 0.005325\n",
      "Epoch: 504/1000 Iteration: 1009 Train loss: 0.005323\n",
      "Epoch: 505/1000 Iteration: 1011 Train loss: 0.005321\n",
      "Epoch: 506/1000 Iteration: 1013 Train loss: 0.005318\n",
      "Epoch: 507/1000 Iteration: 1015 Train loss: 0.005316\n",
      "Epoch: 508/1000 Iteration: 1017 Train loss: 0.005313\n",
      "Epoch: 509/1000 Iteration: 1019 Train loss: 0.005311\n",
      "Epoch: 510/1000 Iteration: 1021 Train loss: 0.005308\n",
      "Epoch: 511/1000 Iteration: 1023 Train loss: 0.005306\n",
      "Epoch: 512/1000 Iteration: 1025 Train loss: 0.005304\n",
      "Epoch: 513/1000 Iteration: 1027 Train loss: 0.005301\n",
      "Epoch: 514/1000 Iteration: 1029 Train loss: 0.005299\n",
      "Epoch: 515/1000 Iteration: 1031 Train loss: 0.005296\n",
      "Epoch: 516/1000 Iteration: 1033 Train loss: 0.005294\n",
      "Epoch: 517/1000 Iteration: 1035 Train loss: 0.005291\n",
      "Epoch: 518/1000 Iteration: 1037 Train loss: 0.005289\n",
      "Epoch: 519/1000 Iteration: 1039 Train loss: 0.005286\n",
      "Epoch: 520/1000 Iteration: 1041 Train loss: 0.005284\n",
      "Epoch: 521/1000 Iteration: 1043 Train loss: 0.005282\n",
      "Epoch: 522/1000 Iteration: 1045 Train loss: 0.005279\n",
      "Epoch: 523/1000 Iteration: 1047 Train loss: 0.005277\n",
      "Epoch: 524/1000 Iteration: 1049 Train loss: 0.005274\n",
      "Epoch: 525/1000 Iteration: 1051 Train loss: 0.005272\n",
      "Epoch: 526/1000 Iteration: 1053 Train loss: 0.005270\n",
      "Epoch: 527/1000 Iteration: 1055 Train loss: 0.005267\n",
      "Epoch: 528/1000 Iteration: 1057 Train loss: 0.005265\n",
      "Epoch: 529/1000 Iteration: 1059 Train loss: 0.005262\n",
      "Epoch: 530/1000 Iteration: 1061 Train loss: 0.005260\n",
      "Epoch: 531/1000 Iteration: 1063 Train loss: 0.005257\n",
      "Epoch: 532/1000 Iteration: 1065 Train loss: 0.005255\n",
      "Epoch: 533/1000 Iteration: 1067 Train loss: 0.005253\n",
      "Epoch: 534/1000 Iteration: 1069 Train loss: 0.005250\n",
      "Epoch: 535/1000 Iteration: 1071 Train loss: 0.005248\n",
      "Epoch: 536/1000 Iteration: 1073 Train loss: 0.005245\n",
      "Epoch: 537/1000 Iteration: 1075 Train loss: 0.005243\n",
      "Epoch: 538/1000 Iteration: 1077 Train loss: 0.005241\n",
      "Epoch: 539/1000 Iteration: 1079 Train loss: 0.005238\n",
      "Epoch: 540/1000 Iteration: 1081 Train loss: 0.005236\n",
      "Epoch: 541/1000 Iteration: 1083 Train loss: 0.005233\n",
      "Epoch: 542/1000 Iteration: 1085 Train loss: 0.005231\n",
      "Epoch: 543/1000 Iteration: 1087 Train loss: 0.005228\n",
      "Epoch: 544/1000 Iteration: 1089 Train loss: 0.005226\n",
      "Epoch: 545/1000 Iteration: 1091 Train loss: 0.005224\n",
      "Epoch: 546/1000 Iteration: 1093 Train loss: 0.005221\n",
      "Epoch: 547/1000 Iteration: 1095 Train loss: 0.005219\n",
      "Epoch: 548/1000 Iteration: 1097 Train loss: 0.005216\n",
      "Epoch: 549/1000 Iteration: 1099 Train loss: 0.005214\n",
      "Epoch: 550/1000 Iteration: 1101 Train loss: 0.005212\n",
      "Epoch: 551/1000 Iteration: 1103 Train loss: 0.005209\n",
      "Epoch: 552/1000 Iteration: 1105 Train loss: 0.005207\n",
      "Epoch: 553/1000 Iteration: 1107 Train loss: 0.005205\n",
      "Epoch: 554/1000 Iteration: 1109 Train loss: 0.005202\n",
      "Epoch: 555/1000 Iteration: 1111 Train loss: 0.005200\n",
      "Epoch: 556/1000 Iteration: 1113 Train loss: 0.005197\n",
      "Epoch: 557/1000 Iteration: 1115 Train loss: 0.005195\n",
      "Epoch: 558/1000 Iteration: 1117 Train loss: 0.005193\n",
      "Epoch: 559/1000 Iteration: 1119 Train loss: 0.005190\n",
      "Epoch: 560/1000 Iteration: 1121 Train loss: 0.005188\n",
      "Epoch: 561/1000 Iteration: 1123 Train loss: 0.005185\n",
      "Epoch: 562/1000 Iteration: 1125 Train loss: 0.005183\n",
      "Epoch: 563/1000 Iteration: 1127 Train loss: 0.005181\n",
      "Epoch: 564/1000 Iteration: 1129 Train loss: 0.005178\n",
      "Epoch: 565/1000 Iteration: 1131 Train loss: 0.005176\n",
      "Epoch: 566/1000 Iteration: 1133 Train loss: 0.005174\n",
      "Epoch: 567/1000 Iteration: 1135 Train loss: 0.005171\n",
      "Epoch: 568/1000 Iteration: 1137 Train loss: 0.005169\n",
      "Epoch: 569/1000 Iteration: 1139 Train loss: 0.005166\n",
      "Epoch: 570/1000 Iteration: 1141 Train loss: 0.005164\n",
      "Epoch: 571/1000 Iteration: 1143 Train loss: 0.005162\n",
      "Epoch: 572/1000 Iteration: 1145 Train loss: 0.005159\n",
      "Epoch: 573/1000 Iteration: 1147 Train loss: 0.005157\n",
      "Epoch: 574/1000 Iteration: 1149 Train loss: 0.005155\n",
      "Epoch: 575/1000 Iteration: 1151 Train loss: 0.005152\n",
      "Epoch: 576/1000 Iteration: 1153 Train loss: 0.005150\n",
      "Epoch: 577/1000 Iteration: 1155 Train loss: 0.005147\n",
      "Epoch: 578/1000 Iteration: 1157 Train loss: 0.005145\n",
      "Epoch: 579/1000 Iteration: 1159 Train loss: 0.005143\n",
      "Epoch: 580/1000 Iteration: 1161 Train loss: 0.005140\n",
      "Epoch: 581/1000 Iteration: 1163 Train loss: 0.005138\n",
      "Epoch: 582/1000 Iteration: 1165 Train loss: 0.005136\n",
      "Epoch: 583/1000 Iteration: 1167 Train loss: 0.005133\n",
      "Epoch: 584/1000 Iteration: 1169 Train loss: 0.005131\n",
      "Epoch: 585/1000 Iteration: 1171 Train loss: 0.005129\n",
      "Epoch: 586/1000 Iteration: 1173 Train loss: 0.005126\n",
      "Epoch: 587/1000 Iteration: 1175 Train loss: 0.005124\n",
      "Epoch: 588/1000 Iteration: 1177 Train loss: 0.005122\n",
      "Epoch: 589/1000 Iteration: 1179 Train loss: 0.005119\n",
      "Epoch: 590/1000 Iteration: 1181 Train loss: 0.005117\n",
      "Epoch: 591/1000 Iteration: 1183 Train loss: 0.005115\n",
      "Epoch: 592/1000 Iteration: 1185 Train loss: 0.005112\n",
      "Epoch: 593/1000 Iteration: 1187 Train loss: 0.005110\n",
      "Epoch: 594/1000 Iteration: 1189 Train loss: 0.005108\n",
      "Epoch: 595/1000 Iteration: 1191 Train loss: 0.005105\n",
      "Epoch: 596/1000 Iteration: 1193 Train loss: 0.005103\n",
      "Epoch: 597/1000 Iteration: 1195 Train loss: 0.005101\n",
      "Epoch: 598/1000 Iteration: 1197 Train loss: 0.005098\n",
      "Epoch: 599/1000 Iteration: 1199 Train loss: 0.005096\n",
      "Epoch: 600/1000 Iteration: 1201 Train loss: 0.005094\n",
      "Epoch: 601/1000 Iteration: 1203 Train loss: 0.005091\n",
      "Epoch: 602/1000 Iteration: 1205 Train loss: 0.005089\n",
      "Epoch: 603/1000 Iteration: 1207 Train loss: 0.005087\n",
      "Epoch: 604/1000 Iteration: 1209 Train loss: 0.005084\n",
      "Epoch: 605/1000 Iteration: 1211 Train loss: 0.005082\n",
      "Epoch: 606/1000 Iteration: 1213 Train loss: 0.005080\n",
      "Epoch: 607/1000 Iteration: 1215 Train loss: 0.005077\n",
      "Epoch: 608/1000 Iteration: 1217 Train loss: 0.005075\n",
      "Epoch: 609/1000 Iteration: 1219 Train loss: 0.005073\n",
      "Epoch: 610/1000 Iteration: 1221 Train loss: 0.005070\n",
      "Epoch: 611/1000 Iteration: 1223 Train loss: 0.005068\n",
      "Epoch: 612/1000 Iteration: 1225 Train loss: 0.005066\n",
      "Epoch: 613/1000 Iteration: 1227 Train loss: 0.005063\n",
      "Epoch: 614/1000 Iteration: 1229 Train loss: 0.005061\n",
      "Epoch: 615/1000 Iteration: 1231 Train loss: 0.005059\n",
      "Epoch: 616/1000 Iteration: 1233 Train loss: 0.005056\n",
      "Epoch: 617/1000 Iteration: 1235 Train loss: 0.005054\n",
      "Epoch: 618/1000 Iteration: 1237 Train loss: 0.005052\n",
      "Epoch: 619/1000 Iteration: 1239 Train loss: 0.005050\n",
      "Epoch: 620/1000 Iteration: 1241 Train loss: 0.005047\n",
      "Epoch: 621/1000 Iteration: 1243 Train loss: 0.005045\n",
      "Epoch: 622/1000 Iteration: 1245 Train loss: 0.005043\n",
      "Epoch: 623/1000 Iteration: 1247 Train loss: 0.005040\n",
      "Epoch: 624/1000 Iteration: 1249 Train loss: 0.005038\n",
      "Epoch: 625/1000 Iteration: 1251 Train loss: 0.005036\n",
      "Epoch: 626/1000 Iteration: 1253 Train loss: 0.005033\n",
      "Epoch: 627/1000 Iteration: 1255 Train loss: 0.005031\n",
      "Epoch: 628/1000 Iteration: 1257 Train loss: 0.005029\n",
      "Epoch: 629/1000 Iteration: 1259 Train loss: 0.005027\n",
      "Epoch: 630/1000 Iteration: 1261 Train loss: 0.005024\n",
      "Epoch: 631/1000 Iteration: 1263 Train loss: 0.005022\n",
      "Epoch: 632/1000 Iteration: 1265 Train loss: 0.005020\n",
      "Epoch: 633/1000 Iteration: 1267 Train loss: 0.005017\n",
      "Epoch: 634/1000 Iteration: 1269 Train loss: 0.005015\n",
      "Epoch: 635/1000 Iteration: 1271 Train loss: 0.005013\n",
      "Epoch: 636/1000 Iteration: 1273 Train loss: 0.005011\n",
      "Epoch: 637/1000 Iteration: 1275 Train loss: 0.005008\n",
      "Epoch: 638/1000 Iteration: 1277 Train loss: 0.005006\n",
      "Epoch: 639/1000 Iteration: 1279 Train loss: 0.005004\n",
      "Epoch: 640/1000 Iteration: 1281 Train loss: 0.005001\n",
      "Epoch: 641/1000 Iteration: 1283 Train loss: 0.004999\n",
      "Epoch: 642/1000 Iteration: 1285 Train loss: 0.004997\n",
      "Epoch: 643/1000 Iteration: 1287 Train loss: 0.004995\n",
      "Epoch: 644/1000 Iteration: 1289 Train loss: 0.004992\n",
      "Epoch: 645/1000 Iteration: 1291 Train loss: 0.004990\n",
      "Epoch: 646/1000 Iteration: 1293 Train loss: 0.004988\n",
      "Epoch: 647/1000 Iteration: 1295 Train loss: 0.004986\n",
      "Epoch: 648/1000 Iteration: 1297 Train loss: 0.004983\n",
      "Epoch: 649/1000 Iteration: 1299 Train loss: 0.004981\n",
      "Epoch: 650/1000 Iteration: 1301 Train loss: 0.004979\n",
      "Epoch: 651/1000 Iteration: 1303 Train loss: 0.004977\n",
      "Epoch: 652/1000 Iteration: 1305 Train loss: 0.004974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 653/1000 Iteration: 1307 Train loss: 0.004972\n",
      "Epoch: 654/1000 Iteration: 1309 Train loss: 0.004970\n",
      "Epoch: 655/1000 Iteration: 1311 Train loss: 0.004968\n",
      "Epoch: 656/1000 Iteration: 1313 Train loss: 0.004965\n",
      "Epoch: 657/1000 Iteration: 1315 Train loss: 0.004963\n",
      "Epoch: 658/1000 Iteration: 1317 Train loss: 0.004961\n",
      "Epoch: 659/1000 Iteration: 1319 Train loss: 0.004959\n",
      "Epoch: 660/1000 Iteration: 1321 Train loss: 0.004956\n",
      "Epoch: 661/1000 Iteration: 1323 Train loss: 0.004954\n",
      "Epoch: 662/1000 Iteration: 1325 Train loss: 0.004952\n",
      "Epoch: 663/1000 Iteration: 1327 Train loss: 0.004950\n",
      "Epoch: 664/1000 Iteration: 1329 Train loss: 0.004947\n",
      "Epoch: 665/1000 Iteration: 1331 Train loss: 0.004945\n",
      "Epoch: 666/1000 Iteration: 1333 Train loss: 0.004943\n",
      "Epoch: 667/1000 Iteration: 1335 Train loss: 0.004941\n",
      "Epoch: 668/1000 Iteration: 1337 Train loss: 0.004938\n",
      "Epoch: 669/1000 Iteration: 1339 Train loss: 0.004936\n",
      "Epoch: 670/1000 Iteration: 1341 Train loss: 0.004934\n",
      "Epoch: 671/1000 Iteration: 1343 Train loss: 0.004932\n",
      "Epoch: 672/1000 Iteration: 1345 Train loss: 0.004930\n",
      "Epoch: 673/1000 Iteration: 1347 Train loss: 0.004927\n",
      "Epoch: 674/1000 Iteration: 1349 Train loss: 0.004925\n",
      "Epoch: 675/1000 Iteration: 1351 Train loss: 0.004923\n",
      "Epoch: 676/1000 Iteration: 1353 Train loss: 0.004921\n",
      "Epoch: 677/1000 Iteration: 1355 Train loss: 0.004918\n",
      "Epoch: 678/1000 Iteration: 1357 Train loss: 0.004916\n",
      "Epoch: 679/1000 Iteration: 1359 Train loss: 0.004914\n",
      "Epoch: 680/1000 Iteration: 1361 Train loss: 0.004912\n",
      "Epoch: 681/1000 Iteration: 1363 Train loss: 0.004910\n",
      "Epoch: 682/1000 Iteration: 1365 Train loss: 0.004907\n",
      "Epoch: 683/1000 Iteration: 1367 Train loss: 0.004905\n",
      "Epoch: 684/1000 Iteration: 1369 Train loss: 0.004903\n",
      "Epoch: 685/1000 Iteration: 1371 Train loss: 0.004901\n",
      "Epoch: 686/1000 Iteration: 1373 Train loss: 0.004899\n",
      "Epoch: 687/1000 Iteration: 1375 Train loss: 0.004896\n",
      "Epoch: 688/1000 Iteration: 1377 Train loss: 0.004894\n",
      "Epoch: 689/1000 Iteration: 1379 Train loss: 0.004892\n",
      "Epoch: 690/1000 Iteration: 1381 Train loss: 0.004890\n",
      "Epoch: 691/1000 Iteration: 1383 Train loss: 0.004888\n",
      "Epoch: 692/1000 Iteration: 1385 Train loss: 0.004885\n",
      "Epoch: 693/1000 Iteration: 1387 Train loss: 0.004883\n",
      "Epoch: 694/1000 Iteration: 1389 Train loss: 0.004881\n",
      "Epoch: 695/1000 Iteration: 1391 Train loss: 0.004879\n",
      "Epoch: 696/1000 Iteration: 1393 Train loss: 0.004877\n",
      "Epoch: 697/1000 Iteration: 1395 Train loss: 0.004874\n",
      "Epoch: 698/1000 Iteration: 1397 Train loss: 0.004872\n",
      "Epoch: 699/1000 Iteration: 1399 Train loss: 0.004870\n",
      "Epoch: 700/1000 Iteration: 1401 Train loss: 0.004868\n",
      "Epoch: 701/1000 Iteration: 1403 Train loss: 0.004866\n",
      "Epoch: 702/1000 Iteration: 1405 Train loss: 0.004864\n",
      "Epoch: 703/1000 Iteration: 1407 Train loss: 0.004861\n",
      "Epoch: 704/1000 Iteration: 1409 Train loss: 0.004859\n",
      "Epoch: 705/1000 Iteration: 1411 Train loss: 0.004857\n",
      "Epoch: 706/1000 Iteration: 1413 Train loss: 0.004855\n",
      "Epoch: 707/1000 Iteration: 1415 Train loss: 0.004853\n",
      "Epoch: 708/1000 Iteration: 1417 Train loss: 0.004851\n",
      "Epoch: 709/1000 Iteration: 1419 Train loss: 0.004848\n",
      "Epoch: 710/1000 Iteration: 1421 Train loss: 0.004846\n",
      "Epoch: 711/1000 Iteration: 1423 Train loss: 0.004844\n",
      "Epoch: 712/1000 Iteration: 1425 Train loss: 0.004842\n",
      "Epoch: 713/1000 Iteration: 1427 Train loss: 0.004840\n",
      "Epoch: 714/1000 Iteration: 1429 Train loss: 0.004838\n",
      "Epoch: 715/1000 Iteration: 1431 Train loss: 0.004835\n",
      "Epoch: 716/1000 Iteration: 1433 Train loss: 0.004833\n",
      "Epoch: 717/1000 Iteration: 1435 Train loss: 0.004831\n",
      "Epoch: 718/1000 Iteration: 1437 Train loss: 0.004829\n",
      "Epoch: 719/1000 Iteration: 1439 Train loss: 0.004827\n",
      "Epoch: 720/1000 Iteration: 1441 Train loss: 0.004825\n",
      "Epoch: 721/1000 Iteration: 1443 Train loss: 0.004822\n",
      "Epoch: 722/1000 Iteration: 1445 Train loss: 0.004820\n",
      "Epoch: 723/1000 Iteration: 1447 Train loss: 0.004818\n",
      "Epoch: 724/1000 Iteration: 1449 Train loss: 0.004816\n",
      "Epoch: 725/1000 Iteration: 1451 Train loss: 0.004814\n",
      "Epoch: 726/1000 Iteration: 1453 Train loss: 0.004812\n",
      "Epoch: 727/1000 Iteration: 1455 Train loss: 0.004810\n",
      "Epoch: 728/1000 Iteration: 1457 Train loss: 0.004807\n",
      "Epoch: 729/1000 Iteration: 1459 Train loss: 0.004805\n",
      "Epoch: 730/1000 Iteration: 1461 Train loss: 0.004803\n",
      "Epoch: 731/1000 Iteration: 1463 Train loss: 0.004801\n",
      "Epoch: 732/1000 Iteration: 1465 Train loss: 0.004799\n",
      "Epoch: 733/1000 Iteration: 1467 Train loss: 0.004797\n",
      "Epoch: 734/1000 Iteration: 1469 Train loss: 0.004795\n",
      "Epoch: 735/1000 Iteration: 1471 Train loss: 0.004793\n",
      "Epoch: 736/1000 Iteration: 1473 Train loss: 0.004790\n",
      "Epoch: 737/1000 Iteration: 1475 Train loss: 0.004788\n",
      "Epoch: 738/1000 Iteration: 1477 Train loss: 0.004786\n",
      "Epoch: 739/1000 Iteration: 1479 Train loss: 0.004784\n",
      "Epoch: 740/1000 Iteration: 1481 Train loss: 0.004782\n",
      "Epoch: 741/1000 Iteration: 1483 Train loss: 0.004780\n",
      "Epoch: 742/1000 Iteration: 1485 Train loss: 0.004778\n",
      "Epoch: 743/1000 Iteration: 1487 Train loss: 0.004776\n",
      "Epoch: 744/1000 Iteration: 1489 Train loss: 0.004773\n",
      "Epoch: 745/1000 Iteration: 1491 Train loss: 0.004771\n",
      "Epoch: 746/1000 Iteration: 1493 Train loss: 0.004769\n",
      "Epoch: 747/1000 Iteration: 1495 Train loss: 0.004767\n",
      "Epoch: 748/1000 Iteration: 1497 Train loss: 0.004765\n",
      "Epoch: 749/1000 Iteration: 1499 Train loss: 0.004763\n",
      "Epoch: 750/1000 Iteration: 1501 Train loss: 0.004761\n",
      "Epoch: 751/1000 Iteration: 1503 Train loss: 0.004759\n",
      "Epoch: 752/1000 Iteration: 1505 Train loss: 0.004757\n",
      "Epoch: 753/1000 Iteration: 1507 Train loss: 0.004754\n",
      "Epoch: 754/1000 Iteration: 1509 Train loss: 0.004752\n",
      "Epoch: 755/1000 Iteration: 1511 Train loss: 0.004750\n",
      "Epoch: 756/1000 Iteration: 1513 Train loss: 0.004748\n",
      "Epoch: 757/1000 Iteration: 1515 Train loss: 0.004746\n",
      "Epoch: 758/1000 Iteration: 1517 Train loss: 0.004744\n",
      "Epoch: 759/1000 Iteration: 1519 Train loss: 0.004742\n",
      "Epoch: 760/1000 Iteration: 1521 Train loss: 0.004740\n",
      "Epoch: 761/1000 Iteration: 1523 Train loss: 0.004738\n",
      "Epoch: 762/1000 Iteration: 1525 Train loss: 0.004736\n",
      "Epoch: 763/1000 Iteration: 1527 Train loss: 0.004734\n",
      "Epoch: 764/1000 Iteration: 1529 Train loss: 0.004731\n",
      "Epoch: 765/1000 Iteration: 1531 Train loss: 0.004729\n",
      "Epoch: 766/1000 Iteration: 1533 Train loss: 0.004727\n",
      "Epoch: 767/1000 Iteration: 1535 Train loss: 0.004725\n",
      "Epoch: 768/1000 Iteration: 1537 Train loss: 0.004723\n",
      "Epoch: 769/1000 Iteration: 1539 Train loss: 0.004721\n",
      "Epoch: 770/1000 Iteration: 1541 Train loss: 0.004719\n",
      "Epoch: 771/1000 Iteration: 1543 Train loss: 0.004717\n",
      "Epoch: 772/1000 Iteration: 1545 Train loss: 0.004715\n",
      "Epoch: 773/1000 Iteration: 1547 Train loss: 0.004713\n",
      "Epoch: 774/1000 Iteration: 1549 Train loss: 0.004711\n",
      "Epoch: 775/1000 Iteration: 1551 Train loss: 0.004709\n",
      "Epoch: 776/1000 Iteration: 1553 Train loss: 0.004707\n",
      "Epoch: 777/1000 Iteration: 1555 Train loss: 0.004704\n",
      "Epoch: 778/1000 Iteration: 1557 Train loss: 0.004702\n",
      "Epoch: 779/1000 Iteration: 1559 Train loss: 0.004700\n",
      "Epoch: 780/1000 Iteration: 1561 Train loss: 0.004698\n",
      "Epoch: 781/1000 Iteration: 1563 Train loss: 0.004696\n",
      "Epoch: 782/1000 Iteration: 1565 Train loss: 0.004694\n",
      "Epoch: 783/1000 Iteration: 1567 Train loss: 0.004692\n",
      "Epoch: 784/1000 Iteration: 1569 Train loss: 0.004690\n",
      "Epoch: 785/1000 Iteration: 1571 Train loss: 0.004688\n",
      "Epoch: 786/1000 Iteration: 1573 Train loss: 0.004686\n",
      "Epoch: 787/1000 Iteration: 1575 Train loss: 0.004684\n",
      "Epoch: 788/1000 Iteration: 1577 Train loss: 0.004682\n",
      "Epoch: 789/1000 Iteration: 1579 Train loss: 0.004680\n",
      "Epoch: 790/1000 Iteration: 1581 Train loss: 0.004678\n",
      "Epoch: 791/1000 Iteration: 1583 Train loss: 0.004676\n",
      "Epoch: 792/1000 Iteration: 1585 Train loss: 0.004674\n",
      "Epoch: 793/1000 Iteration: 1587 Train loss: 0.004672\n",
      "Epoch: 794/1000 Iteration: 1589 Train loss: 0.004670\n",
      "Epoch: 795/1000 Iteration: 1591 Train loss: 0.004668\n",
      "Epoch: 796/1000 Iteration: 1593 Train loss: 0.004666\n",
      "Epoch: 797/1000 Iteration: 1595 Train loss: 0.004663\n",
      "Epoch: 798/1000 Iteration: 1597 Train loss: 0.004661\n",
      "Epoch: 799/1000 Iteration: 1599 Train loss: 0.004659\n",
      "Epoch: 800/1000 Iteration: 1601 Train loss: 0.004657\n",
      "Epoch: 801/1000 Iteration: 1603 Train loss: 0.004655\n",
      "Epoch: 802/1000 Iteration: 1605 Train loss: 0.004653\n",
      "Epoch: 803/1000 Iteration: 1607 Train loss: 0.004651\n",
      "Epoch: 804/1000 Iteration: 1609 Train loss: 0.004649\n",
      "Epoch: 805/1000 Iteration: 1611 Train loss: 0.004647\n",
      "Epoch: 806/1000 Iteration: 1613 Train loss: 0.004645\n",
      "Epoch: 807/1000 Iteration: 1615 Train loss: 0.004643\n",
      "Epoch: 808/1000 Iteration: 1617 Train loss: 0.004641\n",
      "Epoch: 809/1000 Iteration: 1619 Train loss: 0.004639\n",
      "Epoch: 810/1000 Iteration: 1621 Train loss: 0.004637\n",
      "Epoch: 811/1000 Iteration: 1623 Train loss: 0.004635\n",
      "Epoch: 812/1000 Iteration: 1625 Train loss: 0.004633\n",
      "Epoch: 813/1000 Iteration: 1627 Train loss: 0.004631\n",
      "Epoch: 814/1000 Iteration: 1629 Train loss: 0.004629\n",
      "Epoch: 815/1000 Iteration: 1631 Train loss: 0.004627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 816/1000 Iteration: 1633 Train loss: 0.004625\n",
      "Epoch: 817/1000 Iteration: 1635 Train loss: 0.004623\n",
      "Epoch: 818/1000 Iteration: 1637 Train loss: 0.004621\n",
      "Epoch: 819/1000 Iteration: 1639 Train loss: 0.004619\n",
      "Epoch: 820/1000 Iteration: 1641 Train loss: 0.004617\n",
      "Epoch: 821/1000 Iteration: 1643 Train loss: 0.004615\n",
      "Epoch: 822/1000 Iteration: 1645 Train loss: 0.004613\n",
      "Epoch: 823/1000 Iteration: 1647 Train loss: 0.004611\n",
      "Epoch: 824/1000 Iteration: 1649 Train loss: 0.004609\n",
      "Epoch: 825/1000 Iteration: 1651 Train loss: 0.004607\n",
      "Epoch: 826/1000 Iteration: 1653 Train loss: 0.004605\n",
      "Epoch: 827/1000 Iteration: 1655 Train loss: 0.004603\n",
      "Epoch: 828/1000 Iteration: 1657 Train loss: 0.004601\n",
      "Epoch: 829/1000 Iteration: 1659 Train loss: 0.004599\n",
      "Epoch: 830/1000 Iteration: 1661 Train loss: 0.004597\n",
      "Epoch: 831/1000 Iteration: 1663 Train loss: 0.004595\n",
      "Epoch: 832/1000 Iteration: 1665 Train loss: 0.004593\n",
      "Epoch: 833/1000 Iteration: 1667 Train loss: 0.004591\n",
      "Epoch: 834/1000 Iteration: 1669 Train loss: 0.004589\n",
      "Epoch: 835/1000 Iteration: 1671 Train loss: 0.004587\n",
      "Epoch: 836/1000 Iteration: 1673 Train loss: 0.004585\n",
      "Epoch: 837/1000 Iteration: 1675 Train loss: 0.004583\n",
      "Epoch: 838/1000 Iteration: 1677 Train loss: 0.004581\n",
      "Epoch: 839/1000 Iteration: 1679 Train loss: 0.004579\n",
      "Epoch: 840/1000 Iteration: 1681 Train loss: 0.004577\n",
      "Epoch: 841/1000 Iteration: 1683 Train loss: 0.004575\n",
      "Epoch: 842/1000 Iteration: 1685 Train loss: 0.004573\n",
      "Epoch: 843/1000 Iteration: 1687 Train loss: 0.004571\n",
      "Epoch: 844/1000 Iteration: 1689 Train loss: 0.004569\n",
      "Epoch: 845/1000 Iteration: 1691 Train loss: 0.004567\n",
      "Epoch: 846/1000 Iteration: 1693 Train loss: 0.004565\n",
      "Epoch: 847/1000 Iteration: 1695 Train loss: 0.004564\n",
      "Epoch: 848/1000 Iteration: 1697 Train loss: 0.004562\n",
      "Epoch: 849/1000 Iteration: 1699 Train loss: 0.004560\n",
      "Epoch: 850/1000 Iteration: 1701 Train loss: 0.004558\n",
      "Epoch: 851/1000 Iteration: 1703 Train loss: 0.004556\n",
      "Epoch: 852/1000 Iteration: 1705 Train loss: 0.004554\n",
      "Epoch: 853/1000 Iteration: 1707 Train loss: 0.004552\n",
      "Epoch: 854/1000 Iteration: 1709 Train loss: 0.004550\n",
      "Epoch: 855/1000 Iteration: 1711 Train loss: 0.004548\n",
      "Epoch: 856/1000 Iteration: 1713 Train loss: 0.004546\n",
      "Epoch: 857/1000 Iteration: 1715 Train loss: 0.004544\n",
      "Epoch: 858/1000 Iteration: 1717 Train loss: 0.004542\n",
      "Epoch: 859/1000 Iteration: 1719 Train loss: 0.004540\n",
      "Epoch: 860/1000 Iteration: 1721 Train loss: 0.004538\n",
      "Epoch: 861/1000 Iteration: 1723 Train loss: 0.004536\n",
      "Epoch: 862/1000 Iteration: 1725 Train loss: 0.004534\n",
      "Epoch: 863/1000 Iteration: 1727 Train loss: 0.004532\n",
      "Epoch: 864/1000 Iteration: 1729 Train loss: 0.004530\n",
      "Epoch: 865/1000 Iteration: 1731 Train loss: 0.004528\n",
      "Epoch: 866/1000 Iteration: 1733 Train loss: 0.004526\n",
      "Epoch: 867/1000 Iteration: 1735 Train loss: 0.004524\n",
      "Epoch: 868/1000 Iteration: 1737 Train loss: 0.004523\n",
      "Epoch: 869/1000 Iteration: 1739 Train loss: 0.004521\n",
      "Epoch: 870/1000 Iteration: 1741 Train loss: 0.004519\n",
      "Epoch: 871/1000 Iteration: 1743 Train loss: 0.004517\n",
      "Epoch: 872/1000 Iteration: 1745 Train loss: 0.004515\n",
      "Epoch: 873/1000 Iteration: 1747 Train loss: 0.004513\n",
      "Epoch: 874/1000 Iteration: 1749 Train loss: 0.004511\n",
      "Epoch: 875/1000 Iteration: 1751 Train loss: 0.004509\n",
      "Epoch: 876/1000 Iteration: 1753 Train loss: 0.004507\n",
      "Epoch: 877/1000 Iteration: 1755 Train loss: 0.004505\n",
      "Epoch: 878/1000 Iteration: 1757 Train loss: 0.004503\n",
      "Epoch: 879/1000 Iteration: 1759 Train loss: 0.004501\n",
      "Epoch: 880/1000 Iteration: 1761 Train loss: 0.004499\n",
      "Epoch: 881/1000 Iteration: 1763 Train loss: 0.004498\n",
      "Epoch: 882/1000 Iteration: 1765 Train loss: 0.004496\n",
      "Epoch: 883/1000 Iteration: 1767 Train loss: 0.004494\n",
      "Epoch: 884/1000 Iteration: 1769 Train loss: 0.004492\n",
      "Epoch: 885/1000 Iteration: 1771 Train loss: 0.004490\n",
      "Epoch: 886/1000 Iteration: 1773 Train loss: 0.004488\n",
      "Epoch: 887/1000 Iteration: 1775 Train loss: 0.004486\n",
      "Epoch: 888/1000 Iteration: 1777 Train loss: 0.004484\n",
      "Epoch: 889/1000 Iteration: 1779 Train loss: 0.004482\n",
      "Epoch: 890/1000 Iteration: 1781 Train loss: 0.004480\n",
      "Epoch: 891/1000 Iteration: 1783 Train loss: 0.004478\n",
      "Epoch: 892/1000 Iteration: 1785 Train loss: 0.004476\n",
      "Epoch: 893/1000 Iteration: 1787 Train loss: 0.004475\n",
      "Epoch: 894/1000 Iteration: 1789 Train loss: 0.004473\n",
      "Epoch: 895/1000 Iteration: 1791 Train loss: 0.004471\n",
      "Epoch: 896/1000 Iteration: 1793 Train loss: 0.004469\n",
      "Epoch: 897/1000 Iteration: 1795 Train loss: 0.004467\n",
      "Epoch: 898/1000 Iteration: 1797 Train loss: 0.004465\n",
      "Epoch: 899/1000 Iteration: 1799 Train loss: 0.004463\n",
      "Epoch: 900/1000 Iteration: 1801 Train loss: 0.004461\n",
      "Epoch: 901/1000 Iteration: 1803 Train loss: 0.004459\n",
      "Epoch: 902/1000 Iteration: 1805 Train loss: 0.004458\n",
      "Epoch: 903/1000 Iteration: 1807 Train loss: 0.004456\n",
      "Epoch: 904/1000 Iteration: 1809 Train loss: 0.004454\n",
      "Epoch: 905/1000 Iteration: 1811 Train loss: 0.004452\n",
      "Epoch: 906/1000 Iteration: 1813 Train loss: 0.004450\n",
      "Epoch: 907/1000 Iteration: 1815 Train loss: 0.004448\n",
      "Epoch: 908/1000 Iteration: 1817 Train loss: 0.004446\n",
      "Epoch: 909/1000 Iteration: 1819 Train loss: 0.004444\n",
      "Epoch: 910/1000 Iteration: 1821 Train loss: 0.004442\n",
      "Epoch: 911/1000 Iteration: 1823 Train loss: 0.004441\n",
      "Epoch: 912/1000 Iteration: 1825 Train loss: 0.004439\n",
      "Epoch: 913/1000 Iteration: 1827 Train loss: 0.004437\n",
      "Epoch: 914/1000 Iteration: 1829 Train loss: 0.004435\n",
      "Epoch: 915/1000 Iteration: 1831 Train loss: 0.004433\n",
      "Epoch: 916/1000 Iteration: 1833 Train loss: 0.004431\n",
      "Epoch: 917/1000 Iteration: 1835 Train loss: 0.004429\n",
      "Epoch: 918/1000 Iteration: 1837 Train loss: 0.004427\n",
      "Epoch: 919/1000 Iteration: 1839 Train loss: 0.004426\n",
      "Epoch: 920/1000 Iteration: 1841 Train loss: 0.004424\n",
      "Epoch: 921/1000 Iteration: 1843 Train loss: 0.004422\n",
      "Epoch: 922/1000 Iteration: 1845 Train loss: 0.004420\n",
      "Epoch: 923/1000 Iteration: 1847 Train loss: 0.004418\n",
      "Epoch: 924/1000 Iteration: 1849 Train loss: 0.004416\n",
      "Epoch: 925/1000 Iteration: 1851 Train loss: 0.004414\n",
      "Epoch: 926/1000 Iteration: 1853 Train loss: 0.004412\n",
      "Epoch: 927/1000 Iteration: 1855 Train loss: 0.004411\n",
      "Epoch: 928/1000 Iteration: 1857 Train loss: 0.004409\n",
      "Epoch: 929/1000 Iteration: 1859 Train loss: 0.004407\n",
      "Epoch: 930/1000 Iteration: 1861 Train loss: 0.004405\n",
      "Epoch: 931/1000 Iteration: 1863 Train loss: 0.004403\n",
      "Epoch: 932/1000 Iteration: 1865 Train loss: 0.004401\n",
      "Epoch: 933/1000 Iteration: 1867 Train loss: 0.004399\n",
      "Epoch: 934/1000 Iteration: 1869 Train loss: 0.004398\n",
      "Epoch: 935/1000 Iteration: 1871 Train loss: 0.004396\n",
      "Epoch: 936/1000 Iteration: 1873 Train loss: 0.004394\n",
      "Epoch: 937/1000 Iteration: 1875 Train loss: 0.004392\n",
      "Epoch: 938/1000 Iteration: 1877 Train loss: 0.004390\n",
      "Epoch: 939/1000 Iteration: 1879 Train loss: 0.004388\n",
      "Epoch: 940/1000 Iteration: 1881 Train loss: 0.004387\n",
      "Epoch: 941/1000 Iteration: 1883 Train loss: 0.004385\n",
      "Epoch: 942/1000 Iteration: 1885 Train loss: 0.004383\n",
      "Epoch: 943/1000 Iteration: 1887 Train loss: 0.004381\n",
      "Epoch: 944/1000 Iteration: 1889 Train loss: 0.004379\n",
      "Epoch: 945/1000 Iteration: 1891 Train loss: 0.004377\n",
      "Epoch: 946/1000 Iteration: 1893 Train loss: 0.004376\n",
      "Epoch: 947/1000 Iteration: 1895 Train loss: 0.004374\n",
      "Epoch: 948/1000 Iteration: 1897 Train loss: 0.004372\n",
      "Epoch: 949/1000 Iteration: 1899 Train loss: 0.004370\n",
      "Epoch: 950/1000 Iteration: 1901 Train loss: 0.004368\n",
      "Epoch: 951/1000 Iteration: 1903 Train loss: 0.004366\n",
      "Epoch: 952/1000 Iteration: 1905 Train loss: 0.004365\n",
      "Epoch: 953/1000 Iteration: 1907 Train loss: 0.004363\n",
      "Epoch: 954/1000 Iteration: 1909 Train loss: 0.004361\n",
      "Epoch: 955/1000 Iteration: 1911 Train loss: 0.004359\n",
      "Epoch: 956/1000 Iteration: 1913 Train loss: 0.004357\n",
      "Epoch: 957/1000 Iteration: 1915 Train loss: 0.004355\n",
      "Epoch: 958/1000 Iteration: 1917 Train loss: 0.004354\n",
      "Epoch: 959/1000 Iteration: 1919 Train loss: 0.004352\n",
      "Epoch: 960/1000 Iteration: 1921 Train loss: 0.004350\n",
      "Epoch: 961/1000 Iteration: 1923 Train loss: 0.004348\n",
      "Epoch: 962/1000 Iteration: 1925 Train loss: 0.004346\n",
      "Epoch: 963/1000 Iteration: 1927 Train loss: 0.004344\n",
      "Epoch: 964/1000 Iteration: 1929 Train loss: 0.004343\n",
      "Epoch: 965/1000 Iteration: 1931 Train loss: 0.004341\n",
      "Epoch: 966/1000 Iteration: 1933 Train loss: 0.004339\n",
      "Epoch: 967/1000 Iteration: 1935 Train loss: 0.004337\n",
      "Epoch: 968/1000 Iteration: 1937 Train loss: 0.004335\n",
      "Epoch: 969/1000 Iteration: 1939 Train loss: 0.004334\n",
      "Epoch: 970/1000 Iteration: 1941 Train loss: 0.004332\n",
      "Epoch: 971/1000 Iteration: 1943 Train loss: 0.004330\n",
      "Epoch: 972/1000 Iteration: 1945 Train loss: 0.004328\n",
      "Epoch: 973/1000 Iteration: 1947 Train loss: 0.004326\n",
      "Epoch: 974/1000 Iteration: 1949 Train loss: 0.004325\n",
      "Epoch: 975/1000 Iteration: 1951 Train loss: 0.004323\n",
      "Epoch: 976/1000 Iteration: 1953 Train loss: 0.004321\n",
      "Epoch: 977/1000 Iteration: 1955 Train loss: 0.004319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 978/1000 Iteration: 1957 Train loss: 0.004317\n",
      "Epoch: 979/1000 Iteration: 1959 Train loss: 0.004316\n",
      "Epoch: 980/1000 Iteration: 1961 Train loss: 0.004314\n",
      "Epoch: 981/1000 Iteration: 1963 Train loss: 0.004312\n",
      "Epoch: 982/1000 Iteration: 1965 Train loss: 0.004310\n",
      "Epoch: 983/1000 Iteration: 1967 Train loss: 0.004308\n",
      "Epoch: 984/1000 Iteration: 1969 Train loss: 0.004307\n",
      "Epoch: 985/1000 Iteration: 1971 Train loss: 0.004305\n",
      "Epoch: 986/1000 Iteration: 1973 Train loss: 0.004303\n",
      "Epoch: 987/1000 Iteration: 1975 Train loss: 0.004301\n",
      "Epoch: 988/1000 Iteration: 1977 Train loss: 0.004299\n",
      "Epoch: 989/1000 Iteration: 1979 Train loss: 0.004298\n",
      "Epoch: 990/1000 Iteration: 1981 Train loss: 0.004296\n",
      "Epoch: 991/1000 Iteration: 1983 Train loss: 0.004294\n",
      "Epoch: 992/1000 Iteration: 1985 Train loss: 0.004292\n",
      "Epoch: 993/1000 Iteration: 1987 Train loss: 0.004290\n",
      "Epoch: 994/1000 Iteration: 1989 Train loss: 0.004289\n",
      "Epoch: 995/1000 Iteration: 1991 Train loss: 0.004287\n",
      "Epoch: 996/1000 Iteration: 1993 Train loss: 0.004285\n",
      "Epoch: 997/1000 Iteration: 1995 Train loss: 0.004283\n",
      "Epoch: 998/1000 Iteration: 1997 Train loss: 0.004282\n",
      "Epoch: 999/1000 Iteration: 1999 Train loss: 0.004280\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./models/BusinessRiskModel/model_4/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff978f9c630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn_3/Reshape_1:0\", shape=(?, 5, 10), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from ./models/BusinessRisk-lstm/caiwu_predict.ckpt\n",
      "[[0.31339307363088215], [0.44824200150085197], [0.5147971952368775], [0.6276184487903889], [0.7437064711475722]]\n",
      "[[0.448242  ]\n",
      " [0.5147972 ]\n",
      " [0.62761845]\n",
      " [0.74370647]\n",
      " [0.77789485]]\n",
      "[[0.5147972 ]\n",
      " [0.62761845]\n",
      " [0.74370647]\n",
      " [0.77789485]\n",
      " [0.83280933]]\n",
      "[[0.62761845]\n",
      " [0.74370647]\n",
      " [0.77789485]\n",
      " [0.83280933]\n",
      " [0.87131929]]\n",
      "[[0.74370647]\n",
      " [0.77789485]\n",
      " [0.83280933]\n",
      " [0.87131929]\n",
      " [0.89928275]]\n",
      "预测值： [[5.5191938 ]\n",
      " [5.89039202]\n",
      " [6.15070281]\n",
      " [6.33972376]]\n",
      "真实值： [[5.92401827]\n",
      " [5.77068446]\n",
      " [6.84187   ]\n",
      " [7.02052901]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff880122cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************************\n",
      "\n",
      "*********************************迭代结束*********************************\n",
      "\n",
      "**************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdate\n",
    "%matplotlib inline \n",
    "\n",
    "# 加载数据\n",
    "path = \"./dataSets/房地产行业财务风险数据.xlsx\"\n",
    "# skipfooter=3\n",
    "\n",
    "Matrix_pre = np.zeros(shape=(4,1))\n",
    "\n",
    "#print (Matrix_pre)\n",
    "\n",
    "for i in range(4,5):\n",
    "    \n",
    "    print('**************************************************************************\\n')\n",
    "    print('**************************当前是第'+str(i)+'列数据,共16列**************************\\n')\n",
    "    print('**************************************************************************\\n')\n",
    "\n",
    "    dataset = pd.read_excel(path, sheet_name = 'Sheet3', usecols= [i],  nrows = 20)\n",
    "    dataset = np.array(dataset)\n",
    "\n",
    "    print(dataset)\n",
    "\n",
    "    plt.plot(dataset)\n",
    "    plt.show()\n",
    "\n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataSet = scaler.fit_transform(dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # split into train and test sets; 80% 是训练数据，其余是测试数据\n",
    "    train_size = int(len(dataSet) * 0.8)\n",
    "    test_size = len(dataSet) - train_size\n",
    "    train, test = dataSet[0:train_size], dataSet[train_size:len(dataSet)]\n",
    "\n",
    "    # 数据格式转化(t,t+1)\n",
    "    def convert_data(data, time_step=1):\n",
    "        data_X,data_Y = [],[]  \n",
    "        for i in range(len(data) - time_step - 1):\n",
    "            x = data[i: (i + time_step)]  \n",
    "            y = data[i+1:i + time_step+1]      \n",
    "            data_X.append(x.tolist())\n",
    "            data_Y.append(y.tolist()) \n",
    "        return data_X, data_Y\n",
    "\n",
    "    # fix random seed for reproducibility\n",
    "    np.random.seed(7)\n",
    "\n",
    "    # use this function to prepare the train and test datasets for modeling\n",
    "    #time_step=5\n",
    "    time_step = 5    #时间步\n",
    "    train_x, train_y = convert_data(train, time_step)\n",
    "    test_x, test_y = convert_data(test, time_step)\n",
    "\n",
    "    #———————————————————形成训练集—————————————————————\n",
    "    #设置常量\n",
    "    hidden_unit = 10       #hidden layer units 记忆和储存过去状态的节点个数\n",
    "    batch_size = 4    #每一批次训练多少个样例\n",
    "    input_size = 1      #输入层维度\n",
    "    output_size = 1     #输出层维度\n",
    "    lr = 0.0001#学习率\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # LSTM 的 X 需要有这样的结构： [samples, time steps, features]，所以做一下变换\n",
    "    X = tf.placeholder(tf.float32, [None,time_step,input_size], name=\"inputs\")    #每批次输入网络的tensor\n",
    "    Y = tf.placeholder(tf.float32, [None,time_step,output_size], name=\"outputs\")   #每批次tensor对应的标签\n",
    "    # 输入层、输出层权重、偏置\n",
    "    with tf.name_scope('layer'):\n",
    "            with tf.name_scope('weights'):\n",
    "                weights={\n",
    "                         'in':tf.Variable(tf.random_normal([input_size,hidden_unit])),\n",
    "                         'out':tf.Variable(tf.random_normal([hidden_unit,1]))\n",
    "                         }\n",
    "            with tf.name_scope('biases'):\n",
    "                biases={\n",
    "                        'in':tf.Variable(tf.constant(0.1,shape=[hidden_unit,])),\n",
    "                        'out':tf.Variable(tf.constant(0.1,shape=[1,]))\n",
    "                        }\n",
    "\n",
    "    def lstm(batch):  #参数：输入网络批次数目\n",
    "\n",
    "        w_in = weights['in']\n",
    "        b_in = biases['in']\n",
    "        input = tf.reshape(X,[-1,input_size])  #需要将tensor转成2维进行计算，计算后的结果作为隐藏层的输入\n",
    "      \n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_unit) #10个节点\n",
    "        input_lstm = tf.matmul(input, w_in) + b_in\n",
    "        input_lstm = tf.reshape(input_lstm, [-1, time_step, hidden_unit])  #将tensor转成3维，作为lstm cell的输入      \n",
    "        print(input_lstm)\n",
    "        init_state = lstm_cell.zero_state(batch,dtype = tf.float32)\n",
    "        # output_rnn是记录lstm每个隐状态输出节点的结果，final_states是最后一个cell的结果，数据格式为tuple\n",
    "        output_rnn, final_states = tf.nn.dynamic_rnn(\n",
    "            lstm_cell, \n",
    "            input_lstm, \n",
    "            initial_state = init_state, \n",
    "            dtype = tf.float32) \n",
    "\n",
    "        output = tf.reshape(output_rnn, [-1, hidden_unit]) #  作为输出层的输入\n",
    "        w_out = weights['out']\n",
    "        b_out = biases['out']\n",
    "         # 预测数据\n",
    "        multi = tf.matmul(output, w_out)\n",
    "        pred = tf.add(multi, b_out, name='preds') \n",
    "        return pred, final_states\n",
    "\n",
    "    train_loss = []\n",
    "    def train_lstm():   \n",
    "        global batch_size\n",
    "        iteration = 1\n",
    "        epochs = 1000\n",
    "    #     with tf.variable_scope(\"sec_lstm\"):\n",
    "        pred, _ = lstm(batch_size)\n",
    "        # 损失函数\n",
    "        loss = tf.reduce_mean(tf.square(tf.reshape(pred,[-1])-tf.reshape(Y, [-1])))\n",
    "        #tf.summary.scalar('loss_function', loss)\n",
    "        train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        with tf.Session() as sess:\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # summaries合并\n",
    "            #merged = tf.summary.merge_all()    \n",
    "            # 写到指定的磁盘路径中\n",
    "            #train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "            # 重复训练5000次\n",
    "            for e in range(epochs):\n",
    "                step=0\n",
    "                start = 0\n",
    "                end = start + batch_size\n",
    "                while(end < len(train_x)):\n",
    "                    x = train_x[start:end]\n",
    "                    y = train_y[start:end]\n",
    "                    _,loss_ = sess.run([train_op, loss], feed_dict = {X: x, Y:y, keep_prob : 0.3})\n",
    "                    start += batch_size\n",
    "                    end = start + batch_size\n",
    "                    # 每10步保存一次参数\n",
    "                    if step% 10 == 0:                    \n",
    "                        print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                        \"Iteration: {:d}\".format(iteration),\n",
    "                        \"Train loss: {:6f}\".format(loss_))\n",
    "                        #train_writer.add_summary(summary, e);\n",
    "\n",
    "                    train_loss.append(loss_)\n",
    "                    iteration += 1  \n",
    "                    step += 1\n",
    "            saver.save(sess, \"./models/BusinessRisk-lstm/caiwu_predict.ckpt\")\n",
    "             # 保存二进制模型\n",
    "            builder = tf.saved_model.builder.SavedModelBuilder(\"./models/BusinessRiskModel/model_\"+ str(i))\n",
    "            builder.add_meta_graph_and_variables(sess, ['mytag'])\n",
    "            builder.save()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #绘训练过程指标图\n",
    "            t = np.arange(iteration - 1)\n",
    "            plt.figure(figsize = (9,6))\n",
    "            plt.plot(t, np.array(train_loss),  'r-')\n",
    "            plt.xlabel(\"iteration\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend(['train'], loc='upper right')\n",
    "            plt.show()        \n",
    "\n",
    "    with tf.variable_scope('rnn', reuse=tf.AUTO_REUSE):\n",
    "            train_lstm()\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import math\n",
    "\n",
    "    def prediction():\n",
    "\n",
    "            pred, _ = lstm(1)  # 预测时只输入[1,time_step,inputSize]的测试数据\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            #预测季度\n",
    "            pre_quarter = 4\n",
    "            with tf.Session() as sess:\n",
    "                # 参数恢复\n",
    "                module_file = tf.train.latest_checkpoint(\"./models/BusinessRisk-lstm/\")\n",
    "                saver.restore(sess, module_file)\n",
    "                # 取训练集最后一行为测试样本. shape=[1,time_step,inputSize]\n",
    "                prev_seq = train_x[-1]\n",
    "                print(prev_seq)\n",
    "                predict = []\n",
    "                # 得到之后10个季度的预测结果\n",
    "                for i in range(pre_quarter):\n",
    "                    next_seq = sess.run(pred,feed_dict={X:[prev_seq]})\n",
    "                    predict.append(next_seq[-1])   \n",
    "                    #每次得到最后一个时间步的预测结果，与之前的数据加在一起，形成新的测试样本\n",
    "                    #np.vstack()表示垂直（按照行顺序）的把数组给堆叠起来。\n",
    "                    prev_seq = np.vstack((prev_seq[1:],next_seq[-1]))\n",
    "                    print(prev_seq)\n",
    "                #得到实际预测值\n",
    "                predictY = scaler.inverse_transform(predict)\n",
    "\n",
    "                testY = scaler.inverse_transform(test)\n",
    "                print(\"预测值：\", predictY)\n",
    "                print(\"真实值：\",testY )\n",
    "\n",
    "\n",
    "                #Matrix_pre[0] = predictY\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #print (C)\n",
    "\n",
    "                global Matrix_pre\n",
    "                Matrix_pre = (np.hstack((Matrix_pre,predictY)))\n",
    "\n",
    "\n",
    "                #以折线图表示结果\n",
    "                plt.figure()\n",
    "                plt.title(\"lead index\")\n",
    "                plt.plot(list(range(len(testY))), testY, 'cx--', list(range(len(predict))), predictY, 'b--')\n",
    "                plt.xlabel(\"date-num\")\n",
    "                plt.ylabel(\"index\")\n",
    "                plt.legend(['train', 'pred'], loc='upper right')\n",
    "                plt.plot()\n",
    "                plt.show()\n",
    "\n",
    "    with tf.variable_scope('rnn', reuse = tf.AUTO_REUSE):\n",
    "        prediction() \n",
    "\n",
    "#存进excel文件\n",
    "Matrix_input = pd.DataFrame(Matrix_pre)\n",
    "writer = pd.ExcelWriter('./房地产行业财务风险预测数据.xlsx')\n",
    "Matrix_input.to_excel(writer, float_format='%.2f', header = False, index = False,) # float_format 控制精度\n",
    "writer.save()\n",
    "\n",
    "print('**************************************************************************\\n')\n",
    "print('*********************************迭代结束*********************************\\n')\n",
    "print('**************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
