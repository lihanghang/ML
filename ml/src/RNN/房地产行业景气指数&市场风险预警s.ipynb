{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************************\n",
      "\n",
      "**************************当前是第1列数据,共46列**************************\n",
      "\n",
      "**************************************************************************\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\北京市项目\\\\房地产行业景气指数&市场风险预警\\\\房地产行业景气指数&市场风险预警.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2ed420ce2a82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'**************************************************************************\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m48\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, **kwds)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     return io.parse(\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\北京市项目\\\\房地产行业景气指数&市场风险预警\\\\房地产行业景气指数&市场风险预警.xlsx'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdate\n",
    "%matplotlib inline \n",
    "\n",
    "# 加载数据\n",
    "path = r\"D:\\北京市项目\\房地产行业景气指数&市场风险预警\\房地产行业景气指数&市场风险预警.xlsx\"\n",
    "# skipfooter=3\n",
    "\n",
    "Matrix_pre = np.zeros(shape=(10,1))\n",
    "\n",
    "#print (Matrix_pre)\n",
    "\n",
    "for i in range(1,47):\n",
    "    \n",
    "    print('**************************************************************************\\n')\n",
    "    print('**************************当前是第'+str(i)+'列数据,共46列**************************\\n')\n",
    "    print('**************************************************************************\\n')\n",
    "\n",
    "    dataset = pd.read_excel(path, sheet_name = 'index', usecols= [i], skiprows = [0,1], nrows = 48)\n",
    "    dataset = np.array(dataset)\n",
    "\n",
    "    print(dataset)\n",
    "\n",
    "    plt.plot(dataset)\n",
    "    plt.show()\n",
    "\n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataSet = scaler.fit_transform(dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # split into train and test sets; 80% 是训练数据，其余是测试数据\n",
    "    train_size = int(len(dataSet) * 0.8)\n",
    "    test_size = len(dataSet) - train_size\n",
    "    train, test = dataSet[0:train_size], dataSet[train_size:len(dataSet)]\n",
    "\n",
    "    # 数据格式转化(t,t+1)\n",
    "    def convert_data(data, time_step=1):\n",
    "        data_X,data_Y = [],[]  \n",
    "        for i in range(len(data) - time_step - 1):\n",
    "            x = data[i: (i + time_step)]  \n",
    "            y = data[i+1:i + time_step+1]      \n",
    "            data_X.append(x.tolist())\n",
    "            data_Y.append(y.tolist()) \n",
    "        return data_X, data_Y\n",
    "\n",
    "    # fix random seed for reproducibility\n",
    "    np.random.seed(7)\n",
    "\n",
    "    # use this function to prepare the train and test datasets for modeling\n",
    "    #time_step=5\n",
    "    time_step = 5      #时间步\n",
    "    train_x, train_y = convert_data(train, time_step)\n",
    "    test_x, test_y = convert_data(test, time_step)\n",
    "\n",
    "    #———————————————————形成训练集—————————————————————\n",
    "    #设置常量\n",
    "    hidden_unit = 10       #hidden layer units 记忆和储存过去状态的节点个数\n",
    "    batch_size = 4    #每一批次训练多少个样例\n",
    "    input_size = 1      #输入层维度\n",
    "    output_size = 1     #输出层维度\n",
    "    lr = 0.0001       #学习率\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # LSTM 的 X 需要有这样的结构： [samples, time steps, features]，所以做一下变换\n",
    "    with tf.name_scope('inputs'):\n",
    "        X = tf.placeholder(tf.float32, [None,time_step,input_size])    #每批次输入网络的tensor\n",
    "        Y = tf.placeholder(tf.float32, [None,time_step,output_size])   #每批次tensor对应的标签\n",
    "    # 输入层、输出层权重、偏置\n",
    "    with tf.name_scope('layer'):\n",
    "            with tf.name_scope('weights'):\n",
    "                weights={\n",
    "                         'in':tf.Variable(tf.random_normal([input_size,hidden_unit])),\n",
    "                         'out':tf.Variable(tf.random_normal([hidden_unit,1]))\n",
    "                         }\n",
    "            with tf.name_scope('biases'):\n",
    "                biases={\n",
    "                        'in':tf.Variable(tf.constant(0.1,shape=[hidden_unit,])),\n",
    "                        'out':tf.Variable(tf.constant(0.1,shape=[1,]))\n",
    "                        }\n",
    "\n",
    "    def lstm(batch):  #参数：输入网络批次数目\n",
    "\n",
    "        w_in = weights['in']\n",
    "        b_in = biases['in']\n",
    "        input = tf.reshape(X,[-1,input_size])  #需要将tensor转成2维进行计算，计算后的结果作为隐藏层的输入\n",
    "        with tf.name_scope('cell'):\n",
    "\n",
    "            lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_unit) #10个节点\n",
    "            input_lstm = tf.matmul(input, w_in) + b_in\n",
    "            input_lstm = tf.reshape(input_lstm, [-1, time_step, hidden_unit])  #将tensor转成3维，作为lstm cell的输入      \n",
    "            print(input_lstm)\n",
    "            init_state = lstm_cell.zero_state(batch,dtype = tf.float32)\n",
    "            # output_rnn是记录lstm每个隐状态输出节点的结果，final_states是最后一个cell的结果，数据格式为tuple\n",
    "            output_rnn, final_states = tf.nn.dynamic_rnn(\n",
    "                lstm_cell, \n",
    "                input_lstm, \n",
    "                initial_state = init_state, \n",
    "                dtype = tf.float32) \n",
    "\n",
    "            output = tf.reshape(output_rnn, [-1, hidden_unit]) #  作为输出层的输入\n",
    "            w_out = weights['out']\n",
    "            b_out = biases['out']\n",
    "            # 预测数据\n",
    "            multi = tf.matmul(output, w_out)\n",
    "            pred = tf.add(multi, b_out, name='pred')  \n",
    "            return pred, final_states\n",
    "\n",
    "    train_loss = []\n",
    "    def train_lstm():   \n",
    "        global batch_size\n",
    "        iteration = 1\n",
    "        epochs = 1000\n",
    "    #     with tf.variable_scope(\"sec_lstm\"):\n",
    "        pred, _ = lstm(batch_size)\n",
    "        # 损失函数\n",
    "        loss = tf.reduce_mean(tf.square(tf.reshape(pred,[-1])-tf.reshape(Y, [-1])))\n",
    "        #tf.summary.scalar('loss_function', loss)\n",
    "        train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        with tf.Session() as sess:\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # summaries合并\n",
    "            #merged = tf.summary.merge_all()    \n",
    "            # 写到指定的磁盘路径中\n",
    "            #train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "            # 重复训练5000次\n",
    "            for e in range(epochs):\n",
    "                step=0\n",
    "                start = 0\n",
    "                end = start + batch_size\n",
    "                while(end < len(train_x)):\n",
    "                    x = train_x[start:end]\n",
    "                    y = train_y[start:end]\n",
    "                    _,loss_ = sess.run([train_op, loss], feed_dict = {X: x, Y:y, keep_prob : 0.3})\n",
    "                    start += batch_size\n",
    "                    end = start + batch_size\n",
    "                    # 每10步保存一次参数\n",
    "                    if step% 10 == 0:                    \n",
    "                        print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                        \"Iteration: {:d}\".format(iteration),\n",
    "                        \"Train loss: {:6f}\".format(loss_))\n",
    "                        #train_writer.add_summary(summary, e);\n",
    "\n",
    "                    train_loss.append(loss_)\n",
    "                    iteration += 1  \n",
    "                    step += 1\n",
    "            saver.save(sess, \"./BusinessRisk-lstm/predict.ckpt\")\n",
    "            #绘训练过程指标图\n",
    "            t = np.arange(iteration - 1)\n",
    "            plt.figure(figsize = (9,6))\n",
    "            plt.plot(t, np.array(train_loss),  'r-')\n",
    "            plt.xlabel(\"iteration\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend(['train'], loc='upper right')\n",
    "            plt.show()        \n",
    "\n",
    "    with tf.variable_scope('rnn', reuse=tf.AUTO_REUSE):\n",
    "        train_lstm()\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import math\n",
    "\n",
    "    def prediction():\n",
    "\n",
    "            pred, _ = lstm(1)  # 预测时只输入[1,time_step,inputSize]的测试数据\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            #预测季度\n",
    "            pre_quarter = 10\n",
    "            with tf.Session() as sess:\n",
    "                # 参数恢复\n",
    "                module_file = tf.train.latest_checkpoint(r\"C:\\Users\\63039\\checkpoints-lstm\")\n",
    "                saver.restore(sess, module_file)\n",
    "                # 取训练集最后一行为测试样本. shape=[1,time_step,inputSize]\n",
    "                prev_seq = train_x[-1]\n",
    "                predict = []\n",
    "                # 得到之后10个季度的预测结果\n",
    "                for i in range(pre_quarter):\n",
    "                    next_seq = sess.run(pred,feed_dict={X:[prev_seq]})\n",
    "                    predict.append(next_seq[-1])   \n",
    "                    #每次得到最后一个时间步的预测结果，与之前的数据加在一起，形成新的测试样本\n",
    "                    #np.vstack()表示垂直（按照行顺序）的把数组给堆叠起来。\n",
    "                    prev_seq = np.vstack((prev_seq[1:],next_seq[-1]))\n",
    "\n",
    "                #得到实际预测值\n",
    "                predictY = scaler.inverse_transform(predict)\n",
    "\n",
    "                testY = scaler.inverse_transform(test)\n",
    "                print(\"预测值：\", predictY)\n",
    "                print(\"真实值：\",testY )\n",
    "\n",
    "\n",
    "                #Matrix_pre[0] = predictY\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #print (C)\n",
    "\n",
    "                global Matrix_pre\n",
    "                Matrix_pre = (np.hstack((Matrix_pre,predictY)))\n",
    "\n",
    "\n",
    "                #以折线图表示结果\n",
    "                plt.figure()\n",
    "                plt.title(\"lead index\")\n",
    "                plt.plot(list(range(len(testY))), testY, 'cx--', list(range(len(predict))), predictY, 'b--')\n",
    "                plt.xlabel(\"date-num\")\n",
    "                plt.ylabel(\"index\")\n",
    "                plt.legend(['train', 'pred'], loc='upper right')\n",
    "                plt.plot()\n",
    "                plt.show()\n",
    "\n",
    "    with tf.variable_scope('rnn', reuse = tf.AUTO_REUSE):\n",
    "        prediction() \n",
    "\n",
    "#存进excel文件\n",
    "# Matrix_input = pd.DataFrame(Matrix_pre)\n",
    "# writer = pd.ExcelWriter('D:\\北京市项目\\房地产行业景气指数&市场风险预警\\房地产行业景气指数&市场风险预警预测数据.xlsx')\n",
    "# Matrix_input.to_excel(writer, float_format='%.2f', header = False, index = False,) # float_format 控制精度\n",
    "# writer.save()\n",
    "\n",
    "print('**************************************************************************\\n')\n",
    "print('*********************************迭代结束*********************************\\n')\n",
    "print('**************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
