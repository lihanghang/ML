{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************************\n",
      "\n",
      "**************************当前是第2列数据,共5列**************************\n",
      "\n",
      "**************************************************************************\n",
      "\n",
      "[[1.09256945]\n",
      " [1.25013976]\n",
      " [1.60097745]\n",
      " [1.85251215]\n",
      " [1.83826751]\n",
      " [2.11512636]\n",
      " [1.90600543]\n",
      " [2.01218402]\n",
      " [2.16902467]\n",
      " [2.37626266]\n",
      " [2.63042645]\n",
      " [2.49268357]\n",
      " [2.39459608]\n",
      " [2.37891035]\n",
      " [2.44842327]\n",
      " [2.29480996]\n",
      " [2.29908227]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdff3f1c240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn_6/Reshape_1:0\", shape=(?, 5, 10), dtype=float32)\n",
      "Epoch: 0/1000 Iteration: 1 Train loss: 0.053362\n",
      "Epoch: 1/1000 Iteration: 2 Train loss: 0.052540\n",
      "Epoch: 2/1000 Iteration: 3 Train loss: 0.051736\n",
      "Epoch: 3/1000 Iteration: 4 Train loss: 0.050952\n",
      "Epoch: 4/1000 Iteration: 5 Train loss: 0.050187\n",
      "Epoch: 5/1000 Iteration: 6 Train loss: 0.049442\n",
      "Epoch: 6/1000 Iteration: 7 Train loss: 0.048717\n",
      "Epoch: 7/1000 Iteration: 8 Train loss: 0.048012\n",
      "Epoch: 8/1000 Iteration: 9 Train loss: 0.047327\n",
      "Epoch: 9/1000 Iteration: 10 Train loss: 0.046663\n",
      "Epoch: 10/1000 Iteration: 11 Train loss: 0.046019\n",
      "Epoch: 11/1000 Iteration: 12 Train loss: 0.045396\n",
      "Epoch: 12/1000 Iteration: 13 Train loss: 0.044793\n",
      "Epoch: 13/1000 Iteration: 14 Train loss: 0.044210\n",
      "Epoch: 14/1000 Iteration: 15 Train loss: 0.043648\n",
      "Epoch: 15/1000 Iteration: 16 Train loss: 0.043107\n",
      "Epoch: 16/1000 Iteration: 17 Train loss: 0.042585\n",
      "Epoch: 17/1000 Iteration: 18 Train loss: 0.042084\n",
      "Epoch: 18/1000 Iteration: 19 Train loss: 0.041602\n",
      "Epoch: 19/1000 Iteration: 20 Train loss: 0.041140\n",
      "Epoch: 20/1000 Iteration: 21 Train loss: 0.040698\n",
      "Epoch: 21/1000 Iteration: 22 Train loss: 0.040274\n",
      "Epoch: 22/1000 Iteration: 23 Train loss: 0.039868\n",
      "Epoch: 23/1000 Iteration: 24 Train loss: 0.039481\n",
      "Epoch: 24/1000 Iteration: 25 Train loss: 0.039112\n",
      "Epoch: 25/1000 Iteration: 26 Train loss: 0.038759\n",
      "Epoch: 26/1000 Iteration: 27 Train loss: 0.038424\n",
      "Epoch: 27/1000 Iteration: 28 Train loss: 0.038104\n",
      "Epoch: 28/1000 Iteration: 29 Train loss: 0.037801\n",
      "Epoch: 29/1000 Iteration: 30 Train loss: 0.037512\n",
      "Epoch: 30/1000 Iteration: 31 Train loss: 0.037237\n",
      "Epoch: 31/1000 Iteration: 32 Train loss: 0.036977\n",
      "Epoch: 32/1000 Iteration: 33 Train loss: 0.036729\n",
      "Epoch: 33/1000 Iteration: 34 Train loss: 0.036494\n",
      "Epoch: 34/1000 Iteration: 35 Train loss: 0.036271\n",
      "Epoch: 35/1000 Iteration: 36 Train loss: 0.036059\n",
      "Epoch: 36/1000 Iteration: 37 Train loss: 0.035858\n",
      "Epoch: 37/1000 Iteration: 38 Train loss: 0.035667\n",
      "Epoch: 38/1000 Iteration: 39 Train loss: 0.035485\n",
      "Epoch: 39/1000 Iteration: 40 Train loss: 0.035312\n",
      "Epoch: 40/1000 Iteration: 41 Train loss: 0.035146\n",
      "Epoch: 41/1000 Iteration: 42 Train loss: 0.034989\n",
      "Epoch: 42/1000 Iteration: 43 Train loss: 0.034838\n",
      "Epoch: 43/1000 Iteration: 44 Train loss: 0.034694\n",
      "Epoch: 44/1000 Iteration: 45 Train loss: 0.034555\n",
      "Epoch: 45/1000 Iteration: 46 Train loss: 0.034422\n",
      "Epoch: 46/1000 Iteration: 47 Train loss: 0.034294\n",
      "Epoch: 47/1000 Iteration: 48 Train loss: 0.034170\n",
      "Epoch: 48/1000 Iteration: 49 Train loss: 0.034051\n",
      "Epoch: 49/1000 Iteration: 50 Train loss: 0.033935\n",
      "Epoch: 50/1000 Iteration: 51 Train loss: 0.033823\n",
      "Epoch: 51/1000 Iteration: 52 Train loss: 0.033713\n",
      "Epoch: 52/1000 Iteration: 53 Train loss: 0.033606\n",
      "Epoch: 53/1000 Iteration: 54 Train loss: 0.033502\n",
      "Epoch: 54/1000 Iteration: 55 Train loss: 0.033400\n",
      "Epoch: 55/1000 Iteration: 56 Train loss: 0.033300\n",
      "Epoch: 56/1000 Iteration: 57 Train loss: 0.033201\n",
      "Epoch: 57/1000 Iteration: 58 Train loss: 0.033104\n",
      "Epoch: 58/1000 Iteration: 59 Train loss: 0.033008\n",
      "Epoch: 59/1000 Iteration: 60 Train loss: 0.032914\n",
      "Epoch: 60/1000 Iteration: 61 Train loss: 0.032820\n",
      "Epoch: 61/1000 Iteration: 62 Train loss: 0.032728\n",
      "Epoch: 62/1000 Iteration: 63 Train loss: 0.032636\n",
      "Epoch: 63/1000 Iteration: 64 Train loss: 0.032545\n",
      "Epoch: 64/1000 Iteration: 65 Train loss: 0.032454\n",
      "Epoch: 65/1000 Iteration: 66 Train loss: 0.032364\n",
      "Epoch: 66/1000 Iteration: 67 Train loss: 0.032275\n",
      "Epoch: 67/1000 Iteration: 68 Train loss: 0.032186\n",
      "Epoch: 68/1000 Iteration: 69 Train loss: 0.032097\n",
      "Epoch: 69/1000 Iteration: 70 Train loss: 0.032009\n",
      "Epoch: 70/1000 Iteration: 71 Train loss: 0.031921\n",
      "Epoch: 71/1000 Iteration: 72 Train loss: 0.031833\n",
      "Epoch: 72/1000 Iteration: 73 Train loss: 0.031746\n",
      "Epoch: 73/1000 Iteration: 74 Train loss: 0.031659\n",
      "Epoch: 74/1000 Iteration: 75 Train loss: 0.031572\n",
      "Epoch: 75/1000 Iteration: 76 Train loss: 0.031485\n",
      "Epoch: 76/1000 Iteration: 77 Train loss: 0.031398\n",
      "Epoch: 77/1000 Iteration: 78 Train loss: 0.031312\n",
      "Epoch: 78/1000 Iteration: 79 Train loss: 0.031226\n",
      "Epoch: 79/1000 Iteration: 80 Train loss: 0.031140\n",
      "Epoch: 80/1000 Iteration: 81 Train loss: 0.031054\n",
      "Epoch: 81/1000 Iteration: 82 Train loss: 0.030968\n",
      "Epoch: 82/1000 Iteration: 83 Train loss: 0.030883\n",
      "Epoch: 83/1000 Iteration: 84 Train loss: 0.030797\n",
      "Epoch: 84/1000 Iteration: 85 Train loss: 0.030712\n",
      "Epoch: 85/1000 Iteration: 86 Train loss: 0.030627\n",
      "Epoch: 86/1000 Iteration: 87 Train loss: 0.030543\n",
      "Epoch: 87/1000 Iteration: 88 Train loss: 0.030458\n",
      "Epoch: 88/1000 Iteration: 89 Train loss: 0.030374\n",
      "Epoch: 89/1000 Iteration: 90 Train loss: 0.030290\n",
      "Epoch: 90/1000 Iteration: 91 Train loss: 0.030206\n",
      "Epoch: 91/1000 Iteration: 92 Train loss: 0.030122\n",
      "Epoch: 92/1000 Iteration: 93 Train loss: 0.030039\n",
      "Epoch: 93/1000 Iteration: 94 Train loss: 0.029955\n",
      "Epoch: 94/1000 Iteration: 95 Train loss: 0.029872\n",
      "Epoch: 95/1000 Iteration: 96 Train loss: 0.029789\n",
      "Epoch: 96/1000 Iteration: 97 Train loss: 0.029707\n",
      "Epoch: 97/1000 Iteration: 98 Train loss: 0.029624\n",
      "Epoch: 98/1000 Iteration: 99 Train loss: 0.029542\n",
      "Epoch: 99/1000 Iteration: 100 Train loss: 0.029460\n",
      "Epoch: 100/1000 Iteration: 101 Train loss: 0.029378\n",
      "Epoch: 101/1000 Iteration: 102 Train loss: 0.029296\n",
      "Epoch: 102/1000 Iteration: 103 Train loss: 0.029215\n",
      "Epoch: 103/1000 Iteration: 104 Train loss: 0.029134\n",
      "Epoch: 104/1000 Iteration: 105 Train loss: 0.029053\n",
      "Epoch: 105/1000 Iteration: 106 Train loss: 0.028972\n",
      "Epoch: 106/1000 Iteration: 107 Train loss: 0.028891\n",
      "Epoch: 107/1000 Iteration: 108 Train loss: 0.028811\n",
      "Epoch: 108/1000 Iteration: 109 Train loss: 0.028731\n",
      "Epoch: 109/1000 Iteration: 110 Train loss: 0.028651\n",
      "Epoch: 110/1000 Iteration: 111 Train loss: 0.028571\n",
      "Epoch: 111/1000 Iteration: 112 Train loss: 0.028491\n",
      "Epoch: 112/1000 Iteration: 113 Train loss: 0.028412\n",
      "Epoch: 113/1000 Iteration: 114 Train loss: 0.028333\n",
      "Epoch: 114/1000 Iteration: 115 Train loss: 0.028254\n",
      "Epoch: 115/1000 Iteration: 116 Train loss: 0.028175\n",
      "Epoch: 116/1000 Iteration: 117 Train loss: 0.028097\n",
      "Epoch: 117/1000 Iteration: 118 Train loss: 0.028018\n",
      "Epoch: 118/1000 Iteration: 119 Train loss: 0.027940\n",
      "Epoch: 119/1000 Iteration: 120 Train loss: 0.027863\n",
      "Epoch: 120/1000 Iteration: 121 Train loss: 0.027785\n",
      "Epoch: 121/1000 Iteration: 122 Train loss: 0.027707\n",
      "Epoch: 122/1000 Iteration: 123 Train loss: 0.027630\n",
      "Epoch: 123/1000 Iteration: 124 Train loss: 0.027553\n",
      "Epoch: 124/1000 Iteration: 125 Train loss: 0.027476\n",
      "Epoch: 125/1000 Iteration: 126 Train loss: 0.027400\n",
      "Epoch: 126/1000 Iteration: 127 Train loss: 0.027323\n",
      "Epoch: 127/1000 Iteration: 128 Train loss: 0.027247\n",
      "Epoch: 128/1000 Iteration: 129 Train loss: 0.027171\n",
      "Epoch: 129/1000 Iteration: 130 Train loss: 0.027095\n",
      "Epoch: 130/1000 Iteration: 131 Train loss: 0.027020\n",
      "Epoch: 131/1000 Iteration: 132 Train loss: 0.026944\n",
      "Epoch: 132/1000 Iteration: 133 Train loss: 0.026869\n",
      "Epoch: 133/1000 Iteration: 134 Train loss: 0.026794\n",
      "Epoch: 134/1000 Iteration: 135 Train loss: 0.026720\n",
      "Epoch: 135/1000 Iteration: 136 Train loss: 0.026645\n",
      "Epoch: 136/1000 Iteration: 137 Train loss: 0.026571\n",
      "Epoch: 137/1000 Iteration: 138 Train loss: 0.026497\n",
      "Epoch: 138/1000 Iteration: 139 Train loss: 0.026423\n",
      "Epoch: 139/1000 Iteration: 140 Train loss: 0.026349\n",
      "Epoch: 140/1000 Iteration: 141 Train loss: 0.026276\n",
      "Epoch: 141/1000 Iteration: 142 Train loss: 0.026203\n",
      "Epoch: 142/1000 Iteration: 143 Train loss: 0.026130\n",
      "Epoch: 143/1000 Iteration: 144 Train loss: 0.026057\n",
      "Epoch: 144/1000 Iteration: 145 Train loss: 0.025984\n",
      "Epoch: 145/1000 Iteration: 146 Train loss: 0.025912\n",
      "Epoch: 146/1000 Iteration: 147 Train loss: 0.025840\n",
      "Epoch: 147/1000 Iteration: 148 Train loss: 0.025768\n",
      "Epoch: 148/1000 Iteration: 149 Train loss: 0.025696\n",
      "Epoch: 149/1000 Iteration: 150 Train loss: 0.025625\n",
      "Epoch: 150/1000 Iteration: 151 Train loss: 0.025554\n",
      "Epoch: 151/1000 Iteration: 152 Train loss: 0.025482\n",
      "Epoch: 152/1000 Iteration: 153 Train loss: 0.025412\n",
      "Epoch: 153/1000 Iteration: 154 Train loss: 0.025341\n",
      "Epoch: 154/1000 Iteration: 155 Train loss: 0.025271\n",
      "Epoch: 155/1000 Iteration: 156 Train loss: 0.025200\n",
      "Epoch: 156/1000 Iteration: 157 Train loss: 0.025130\n",
      "Epoch: 157/1000 Iteration: 158 Train loss: 0.025061\n",
      "Epoch: 158/1000 Iteration: 159 Train loss: 0.024991\n",
      "Epoch: 159/1000 Iteration: 160 Train loss: 0.024922\n",
      "Epoch: 160/1000 Iteration: 161 Train loss: 0.024853\n",
      "Epoch: 161/1000 Iteration: 162 Train loss: 0.024784\n",
      "Epoch: 162/1000 Iteration: 163 Train loss: 0.024715\n",
      "Epoch: 163/1000 Iteration: 164 Train loss: 0.024647\n",
      "Epoch: 164/1000 Iteration: 165 Train loss: 0.024578\n",
      "Epoch: 165/1000 Iteration: 166 Train loss: 0.024510\n",
      "Epoch: 166/1000 Iteration: 167 Train loss: 0.024442\n",
      "Epoch: 167/1000 Iteration: 168 Train loss: 0.024375\n",
      "Epoch: 168/1000 Iteration: 169 Train loss: 0.024307\n",
      "Epoch: 169/1000 Iteration: 170 Train loss: 0.024240\n",
      "Epoch: 170/1000 Iteration: 171 Train loss: 0.024173\n",
      "Epoch: 171/1000 Iteration: 172 Train loss: 0.024106\n",
      "Epoch: 172/1000 Iteration: 173 Train loss: 0.024040\n",
      "Epoch: 173/1000 Iteration: 174 Train loss: 0.023973\n",
      "Epoch: 174/1000 Iteration: 175 Train loss: 0.023907\n",
      "Epoch: 175/1000 Iteration: 176 Train loss: 0.023841\n",
      "Epoch: 176/1000 Iteration: 177 Train loss: 0.023776\n",
      "Epoch: 177/1000 Iteration: 178 Train loss: 0.023710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 178/1000 Iteration: 179 Train loss: 0.023645\n",
      "Epoch: 179/1000 Iteration: 180 Train loss: 0.023580\n",
      "Epoch: 180/1000 Iteration: 181 Train loss: 0.023515\n",
      "Epoch: 181/1000 Iteration: 182 Train loss: 0.023450\n",
      "Epoch: 182/1000 Iteration: 183 Train loss: 0.023386\n",
      "Epoch: 183/1000 Iteration: 184 Train loss: 0.023322\n",
      "Epoch: 184/1000 Iteration: 185 Train loss: 0.023258\n",
      "Epoch: 185/1000 Iteration: 186 Train loss: 0.023194\n",
      "Epoch: 186/1000 Iteration: 187 Train loss: 0.023131\n",
      "Epoch: 187/1000 Iteration: 188 Train loss: 0.023067\n",
      "Epoch: 188/1000 Iteration: 189 Train loss: 0.023004\n",
      "Epoch: 189/1000 Iteration: 190 Train loss: 0.022941\n",
      "Epoch: 190/1000 Iteration: 191 Train loss: 0.022878\n",
      "Epoch: 191/1000 Iteration: 192 Train loss: 0.022816\n",
      "Epoch: 192/1000 Iteration: 193 Train loss: 0.022754\n",
      "Epoch: 193/1000 Iteration: 194 Train loss: 0.022691\n",
      "Epoch: 194/1000 Iteration: 195 Train loss: 0.022630\n",
      "Epoch: 195/1000 Iteration: 196 Train loss: 0.022568\n",
      "Epoch: 196/1000 Iteration: 197 Train loss: 0.022506\n",
      "Epoch: 197/1000 Iteration: 198 Train loss: 0.022445\n",
      "Epoch: 198/1000 Iteration: 199 Train loss: 0.022384\n",
      "Epoch: 199/1000 Iteration: 200 Train loss: 0.022323\n",
      "Epoch: 200/1000 Iteration: 201 Train loss: 0.022263\n",
      "Epoch: 201/1000 Iteration: 202 Train loss: 0.022202\n",
      "Epoch: 202/1000 Iteration: 203 Train loss: 0.022142\n",
      "Epoch: 203/1000 Iteration: 204 Train loss: 0.022082\n",
      "Epoch: 204/1000 Iteration: 205 Train loss: 0.022022\n",
      "Epoch: 205/1000 Iteration: 206 Train loss: 0.021963\n",
      "Epoch: 206/1000 Iteration: 207 Train loss: 0.021903\n",
      "Epoch: 207/1000 Iteration: 208 Train loss: 0.021844\n",
      "Epoch: 208/1000 Iteration: 209 Train loss: 0.021785\n",
      "Epoch: 209/1000 Iteration: 210 Train loss: 0.021726\n",
      "Epoch: 210/1000 Iteration: 211 Train loss: 0.021668\n",
      "Epoch: 211/1000 Iteration: 212 Train loss: 0.021610\n",
      "Epoch: 212/1000 Iteration: 213 Train loss: 0.021551\n",
      "Epoch: 213/1000 Iteration: 214 Train loss: 0.021494\n",
      "Epoch: 214/1000 Iteration: 215 Train loss: 0.021436\n",
      "Epoch: 215/1000 Iteration: 216 Train loss: 0.021378\n",
      "Epoch: 216/1000 Iteration: 217 Train loss: 0.021321\n",
      "Epoch: 217/1000 Iteration: 218 Train loss: 0.021264\n",
      "Epoch: 218/1000 Iteration: 219 Train loss: 0.021207\n",
      "Epoch: 219/1000 Iteration: 220 Train loss: 0.021150\n",
      "Epoch: 220/1000 Iteration: 221 Train loss: 0.021094\n",
      "Epoch: 221/1000 Iteration: 222 Train loss: 0.021038\n",
      "Epoch: 222/1000 Iteration: 223 Train loss: 0.020981\n",
      "Epoch: 223/1000 Iteration: 224 Train loss: 0.020926\n",
      "Epoch: 224/1000 Iteration: 225 Train loss: 0.020870\n",
      "Epoch: 225/1000 Iteration: 226 Train loss: 0.020814\n",
      "Epoch: 226/1000 Iteration: 227 Train loss: 0.020759\n",
      "Epoch: 227/1000 Iteration: 228 Train loss: 0.020704\n",
      "Epoch: 228/1000 Iteration: 229 Train loss: 0.020649\n",
      "Epoch: 229/1000 Iteration: 230 Train loss: 0.020595\n",
      "Epoch: 230/1000 Iteration: 231 Train loss: 0.020540\n",
      "Epoch: 231/1000 Iteration: 232 Train loss: 0.020486\n",
      "Epoch: 232/1000 Iteration: 233 Train loss: 0.020432\n",
      "Epoch: 233/1000 Iteration: 234 Train loss: 0.020378\n",
      "Epoch: 234/1000 Iteration: 235 Train loss: 0.020324\n",
      "Epoch: 235/1000 Iteration: 236 Train loss: 0.020271\n",
      "Epoch: 236/1000 Iteration: 237 Train loss: 0.020217\n",
      "Epoch: 237/1000 Iteration: 238 Train loss: 0.020164\n",
      "Epoch: 238/1000 Iteration: 239 Train loss: 0.020112\n",
      "Epoch: 239/1000 Iteration: 240 Train loss: 0.020059\n",
      "Epoch: 240/1000 Iteration: 241 Train loss: 0.020006\n",
      "Epoch: 241/1000 Iteration: 242 Train loss: 0.019954\n",
      "Epoch: 242/1000 Iteration: 243 Train loss: 0.019902\n",
      "Epoch: 243/1000 Iteration: 244 Train loss: 0.019850\n",
      "Epoch: 244/1000 Iteration: 245 Train loss: 0.019798\n",
      "Epoch: 245/1000 Iteration: 246 Train loss: 0.019747\n",
      "Epoch: 246/1000 Iteration: 247 Train loss: 0.019696\n",
      "Epoch: 247/1000 Iteration: 248 Train loss: 0.019645\n",
      "Epoch: 248/1000 Iteration: 249 Train loss: 0.019594\n",
      "Epoch: 249/1000 Iteration: 250 Train loss: 0.019543\n",
      "Epoch: 250/1000 Iteration: 251 Train loss: 0.019492\n",
      "Epoch: 251/1000 Iteration: 252 Train loss: 0.019442\n",
      "Epoch: 252/1000 Iteration: 253 Train loss: 0.019392\n",
      "Epoch: 253/1000 Iteration: 254 Train loss: 0.019342\n",
      "Epoch: 254/1000 Iteration: 255 Train loss: 0.019292\n",
      "Epoch: 255/1000 Iteration: 256 Train loss: 0.019243\n",
      "Epoch: 256/1000 Iteration: 257 Train loss: 0.019193\n",
      "Epoch: 257/1000 Iteration: 258 Train loss: 0.019144\n",
      "Epoch: 258/1000 Iteration: 259 Train loss: 0.019095\n",
      "Epoch: 259/1000 Iteration: 260 Train loss: 0.019047\n",
      "Epoch: 260/1000 Iteration: 261 Train loss: 0.018998\n",
      "Epoch: 261/1000 Iteration: 262 Train loss: 0.018950\n",
      "Epoch: 262/1000 Iteration: 263 Train loss: 0.018901\n",
      "Epoch: 263/1000 Iteration: 264 Train loss: 0.018853\n",
      "Epoch: 264/1000 Iteration: 265 Train loss: 0.018806\n",
      "Epoch: 265/1000 Iteration: 266 Train loss: 0.018758\n",
      "Epoch: 266/1000 Iteration: 267 Train loss: 0.018710\n",
      "Epoch: 267/1000 Iteration: 268 Train loss: 0.018663\n",
      "Epoch: 268/1000 Iteration: 269 Train loss: 0.018616\n",
      "Epoch: 269/1000 Iteration: 270 Train loss: 0.018569\n",
      "Epoch: 270/1000 Iteration: 271 Train loss: 0.018523\n",
      "Epoch: 271/1000 Iteration: 272 Train loss: 0.018476\n",
      "Epoch: 272/1000 Iteration: 273 Train loss: 0.018430\n",
      "Epoch: 273/1000 Iteration: 274 Train loss: 0.018384\n",
      "Epoch: 274/1000 Iteration: 275 Train loss: 0.018338\n",
      "Epoch: 275/1000 Iteration: 276 Train loss: 0.018292\n",
      "Epoch: 276/1000 Iteration: 277 Train loss: 0.018246\n",
      "Epoch: 277/1000 Iteration: 278 Train loss: 0.018201\n",
      "Epoch: 278/1000 Iteration: 279 Train loss: 0.018156\n",
      "Epoch: 279/1000 Iteration: 280 Train loss: 0.018111\n",
      "Epoch: 280/1000 Iteration: 281 Train loss: 0.018066\n",
      "Epoch: 281/1000 Iteration: 282 Train loss: 0.018021\n",
      "Epoch: 282/1000 Iteration: 283 Train loss: 0.017977\n",
      "Epoch: 283/1000 Iteration: 284 Train loss: 0.017933\n",
      "Epoch: 284/1000 Iteration: 285 Train loss: 0.017889\n",
      "Epoch: 285/1000 Iteration: 286 Train loss: 0.017845\n",
      "Epoch: 286/1000 Iteration: 287 Train loss: 0.017801\n",
      "Epoch: 287/1000 Iteration: 288 Train loss: 0.017757\n",
      "Epoch: 288/1000 Iteration: 289 Train loss: 0.017714\n",
      "Epoch: 289/1000 Iteration: 290 Train loss: 0.017671\n",
      "Epoch: 290/1000 Iteration: 291 Train loss: 0.017628\n",
      "Epoch: 291/1000 Iteration: 292 Train loss: 0.017585\n",
      "Epoch: 292/1000 Iteration: 293 Train loss: 0.017542\n",
      "Epoch: 293/1000 Iteration: 294 Train loss: 0.017500\n",
      "Epoch: 294/1000 Iteration: 295 Train loss: 0.017458\n",
      "Epoch: 295/1000 Iteration: 296 Train loss: 0.017416\n",
      "Epoch: 296/1000 Iteration: 297 Train loss: 0.017374\n",
      "Epoch: 297/1000 Iteration: 298 Train loss: 0.017332\n",
      "Epoch: 298/1000 Iteration: 299 Train loss: 0.017290\n",
      "Epoch: 299/1000 Iteration: 300 Train loss: 0.017249\n",
      "Epoch: 300/1000 Iteration: 301 Train loss: 0.017208\n",
      "Epoch: 301/1000 Iteration: 302 Train loss: 0.017167\n",
      "Epoch: 302/1000 Iteration: 303 Train loss: 0.017126\n",
      "Epoch: 303/1000 Iteration: 304 Train loss: 0.017085\n",
      "Epoch: 304/1000 Iteration: 305 Train loss: 0.017045\n",
      "Epoch: 305/1000 Iteration: 306 Train loss: 0.017004\n",
      "Epoch: 306/1000 Iteration: 307 Train loss: 0.016964\n",
      "Epoch: 307/1000 Iteration: 308 Train loss: 0.016924\n",
      "Epoch: 308/1000 Iteration: 309 Train loss: 0.016885\n",
      "Epoch: 309/1000 Iteration: 310 Train loss: 0.016845\n",
      "Epoch: 310/1000 Iteration: 311 Train loss: 0.016805\n",
      "Epoch: 311/1000 Iteration: 312 Train loss: 0.016766\n",
      "Epoch: 312/1000 Iteration: 313 Train loss: 0.016727\n",
      "Epoch: 313/1000 Iteration: 314 Train loss: 0.016688\n",
      "Epoch: 314/1000 Iteration: 315 Train loss: 0.016649\n",
      "Epoch: 315/1000 Iteration: 316 Train loss: 0.016611\n",
      "Epoch: 316/1000 Iteration: 317 Train loss: 0.016572\n",
      "Epoch: 317/1000 Iteration: 318 Train loss: 0.016534\n",
      "Epoch: 318/1000 Iteration: 319 Train loss: 0.016496\n",
      "Epoch: 319/1000 Iteration: 320 Train loss: 0.016458\n",
      "Epoch: 320/1000 Iteration: 321 Train loss: 0.016420\n",
      "Epoch: 321/1000 Iteration: 322 Train loss: 0.016383\n",
      "Epoch: 322/1000 Iteration: 323 Train loss: 0.016346\n",
      "Epoch: 323/1000 Iteration: 324 Train loss: 0.016308\n",
      "Epoch: 324/1000 Iteration: 325 Train loss: 0.016271\n",
      "Epoch: 325/1000 Iteration: 326 Train loss: 0.016234\n",
      "Epoch: 326/1000 Iteration: 327 Train loss: 0.016198\n",
      "Epoch: 327/1000 Iteration: 328 Train loss: 0.016161\n",
      "Epoch: 328/1000 Iteration: 329 Train loss: 0.016125\n",
      "Epoch: 329/1000 Iteration: 330 Train loss: 0.016089\n",
      "Epoch: 330/1000 Iteration: 331 Train loss: 0.016052\n",
      "Epoch: 331/1000 Iteration: 332 Train loss: 0.016017\n",
      "Epoch: 332/1000 Iteration: 333 Train loss: 0.015981\n",
      "Epoch: 333/1000 Iteration: 334 Train loss: 0.015945\n",
      "Epoch: 334/1000 Iteration: 335 Train loss: 0.015910\n",
      "Epoch: 335/1000 Iteration: 336 Train loss: 0.015875\n",
      "Epoch: 336/1000 Iteration: 337 Train loss: 0.015840\n",
      "Epoch: 337/1000 Iteration: 338 Train loss: 0.015805\n",
      "Epoch: 338/1000 Iteration: 339 Train loss: 0.015770\n",
      "Epoch: 339/1000 Iteration: 340 Train loss: 0.015735\n",
      "Epoch: 340/1000 Iteration: 341 Train loss: 0.015701\n",
      "Epoch: 341/1000 Iteration: 342 Train loss: 0.015667\n",
      "Epoch: 342/1000 Iteration: 343 Train loss: 0.015633\n",
      "Epoch: 343/1000 Iteration: 344 Train loss: 0.015599\n",
      "Epoch: 344/1000 Iteration: 345 Train loss: 0.015565\n",
      "Epoch: 345/1000 Iteration: 346 Train loss: 0.015531\n",
      "Epoch: 346/1000 Iteration: 347 Train loss: 0.015498\n",
      "Epoch: 347/1000 Iteration: 348 Train loss: 0.015465\n",
      "Epoch: 348/1000 Iteration: 349 Train loss: 0.015432\n",
      "Epoch: 349/1000 Iteration: 350 Train loss: 0.015399\n",
      "Epoch: 350/1000 Iteration: 351 Train loss: 0.015366\n",
      "Epoch: 351/1000 Iteration: 352 Train loss: 0.015333\n",
      "Epoch: 352/1000 Iteration: 353 Train loss: 0.015301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 353/1000 Iteration: 354 Train loss: 0.015268\n",
      "Epoch: 354/1000 Iteration: 355 Train loss: 0.015236\n",
      "Epoch: 355/1000 Iteration: 356 Train loss: 0.015204\n",
      "Epoch: 356/1000 Iteration: 357 Train loss: 0.015172\n",
      "Epoch: 357/1000 Iteration: 358 Train loss: 0.015140\n",
      "Epoch: 358/1000 Iteration: 359 Train loss: 0.015109\n",
      "Epoch: 359/1000 Iteration: 360 Train loss: 0.015077\n",
      "Epoch: 360/1000 Iteration: 361 Train loss: 0.015046\n",
      "Epoch: 361/1000 Iteration: 362 Train loss: 0.015015\n",
      "Epoch: 362/1000 Iteration: 363 Train loss: 0.014984\n",
      "Epoch: 363/1000 Iteration: 364 Train loss: 0.014953\n",
      "Epoch: 364/1000 Iteration: 365 Train loss: 0.014923\n",
      "Epoch: 365/1000 Iteration: 366 Train loss: 0.014892\n",
      "Epoch: 366/1000 Iteration: 367 Train loss: 0.014862\n",
      "Epoch: 367/1000 Iteration: 368 Train loss: 0.014832\n",
      "Epoch: 368/1000 Iteration: 369 Train loss: 0.014802\n",
      "Epoch: 369/1000 Iteration: 370 Train loss: 0.014772\n",
      "Epoch: 370/1000 Iteration: 371 Train loss: 0.014742\n",
      "Epoch: 371/1000 Iteration: 372 Train loss: 0.014712\n",
      "Epoch: 372/1000 Iteration: 373 Train loss: 0.014683\n",
      "Epoch: 373/1000 Iteration: 374 Train loss: 0.014654\n",
      "Epoch: 374/1000 Iteration: 375 Train loss: 0.014625\n",
      "Epoch: 375/1000 Iteration: 376 Train loss: 0.014596\n",
      "Epoch: 376/1000 Iteration: 377 Train loss: 0.014567\n",
      "Epoch: 377/1000 Iteration: 378 Train loss: 0.014538\n",
      "Epoch: 378/1000 Iteration: 379 Train loss: 0.014509\n",
      "Epoch: 379/1000 Iteration: 380 Train loss: 0.014481\n",
      "Epoch: 380/1000 Iteration: 381 Train loss: 0.014453\n",
      "Epoch: 381/1000 Iteration: 382 Train loss: 0.014425\n",
      "Epoch: 382/1000 Iteration: 383 Train loss: 0.014397\n",
      "Epoch: 383/1000 Iteration: 384 Train loss: 0.014369\n",
      "Epoch: 384/1000 Iteration: 385 Train loss: 0.014341\n",
      "Epoch: 385/1000 Iteration: 386 Train loss: 0.014314\n",
      "Epoch: 386/1000 Iteration: 387 Train loss: 0.014286\n",
      "Epoch: 387/1000 Iteration: 388 Train loss: 0.014259\n",
      "Epoch: 388/1000 Iteration: 389 Train loss: 0.014232\n",
      "Epoch: 389/1000 Iteration: 390 Train loss: 0.014205\n",
      "Epoch: 390/1000 Iteration: 391 Train loss: 0.014178\n",
      "Epoch: 391/1000 Iteration: 392 Train loss: 0.014151\n",
      "Epoch: 392/1000 Iteration: 393 Train loss: 0.014125\n",
      "Epoch: 393/1000 Iteration: 394 Train loss: 0.014098\n",
      "Epoch: 394/1000 Iteration: 395 Train loss: 0.014072\n",
      "Epoch: 395/1000 Iteration: 396 Train loss: 0.014046\n",
      "Epoch: 396/1000 Iteration: 397 Train loss: 0.014020\n",
      "Epoch: 397/1000 Iteration: 398 Train loss: 0.013994\n",
      "Epoch: 398/1000 Iteration: 399 Train loss: 0.013968\n",
      "Epoch: 399/1000 Iteration: 400 Train loss: 0.013943\n",
      "Epoch: 400/1000 Iteration: 401 Train loss: 0.013917\n",
      "Epoch: 401/1000 Iteration: 402 Train loss: 0.013892\n",
      "Epoch: 402/1000 Iteration: 403 Train loss: 0.013867\n",
      "Epoch: 403/1000 Iteration: 404 Train loss: 0.013842\n",
      "Epoch: 404/1000 Iteration: 405 Train loss: 0.013817\n",
      "Epoch: 405/1000 Iteration: 406 Train loss: 0.013792\n",
      "Epoch: 406/1000 Iteration: 407 Train loss: 0.013767\n",
      "Epoch: 407/1000 Iteration: 408 Train loss: 0.013743\n",
      "Epoch: 408/1000 Iteration: 409 Train loss: 0.013718\n",
      "Epoch: 409/1000 Iteration: 410 Train loss: 0.013694\n",
      "Epoch: 410/1000 Iteration: 411 Train loss: 0.013670\n",
      "Epoch: 411/1000 Iteration: 412 Train loss: 0.013646\n",
      "Epoch: 412/1000 Iteration: 413 Train loss: 0.013622\n",
      "Epoch: 413/1000 Iteration: 414 Train loss: 0.013598\n",
      "Epoch: 414/1000 Iteration: 415 Train loss: 0.013575\n",
      "Epoch: 415/1000 Iteration: 416 Train loss: 0.013551\n",
      "Epoch: 416/1000 Iteration: 417 Train loss: 0.013528\n",
      "Epoch: 417/1000 Iteration: 418 Train loss: 0.013504\n",
      "Epoch: 418/1000 Iteration: 419 Train loss: 0.013481\n",
      "Epoch: 419/1000 Iteration: 420 Train loss: 0.013458\n",
      "Epoch: 420/1000 Iteration: 421 Train loss: 0.013436\n",
      "Epoch: 421/1000 Iteration: 422 Train loss: 0.013413\n",
      "Epoch: 422/1000 Iteration: 423 Train loss: 0.013390\n",
      "Epoch: 423/1000 Iteration: 424 Train loss: 0.013368\n",
      "Epoch: 424/1000 Iteration: 425 Train loss: 0.013345\n",
      "Epoch: 425/1000 Iteration: 426 Train loss: 0.013323\n",
      "Epoch: 426/1000 Iteration: 427 Train loss: 0.013301\n",
      "Epoch: 427/1000 Iteration: 428 Train loss: 0.013279\n",
      "Epoch: 428/1000 Iteration: 429 Train loss: 0.013257\n",
      "Epoch: 429/1000 Iteration: 430 Train loss: 0.013235\n",
      "Epoch: 430/1000 Iteration: 431 Train loss: 0.013214\n",
      "Epoch: 431/1000 Iteration: 432 Train loss: 0.013192\n",
      "Epoch: 432/1000 Iteration: 433 Train loss: 0.013171\n",
      "Epoch: 433/1000 Iteration: 434 Train loss: 0.013150\n",
      "Epoch: 434/1000 Iteration: 435 Train loss: 0.013129\n",
      "Epoch: 435/1000 Iteration: 436 Train loss: 0.013108\n",
      "Epoch: 436/1000 Iteration: 437 Train loss: 0.013087\n",
      "Epoch: 437/1000 Iteration: 438 Train loss: 0.013066\n",
      "Epoch: 438/1000 Iteration: 439 Train loss: 0.013045\n",
      "Epoch: 439/1000 Iteration: 440 Train loss: 0.013025\n",
      "Epoch: 440/1000 Iteration: 441 Train loss: 0.013004\n",
      "Epoch: 441/1000 Iteration: 442 Train loss: 0.012984\n",
      "Epoch: 442/1000 Iteration: 443 Train loss: 0.012964\n",
      "Epoch: 443/1000 Iteration: 444 Train loss: 0.012944\n",
      "Epoch: 444/1000 Iteration: 445 Train loss: 0.012924\n",
      "Epoch: 445/1000 Iteration: 446 Train loss: 0.012904\n",
      "Epoch: 446/1000 Iteration: 447 Train loss: 0.012884\n",
      "Epoch: 447/1000 Iteration: 448 Train loss: 0.012864\n",
      "Epoch: 448/1000 Iteration: 449 Train loss: 0.012845\n",
      "Epoch: 449/1000 Iteration: 450 Train loss: 0.012825\n",
      "Epoch: 450/1000 Iteration: 451 Train loss: 0.012806\n",
      "Epoch: 451/1000 Iteration: 452 Train loss: 0.012787\n",
      "Epoch: 452/1000 Iteration: 453 Train loss: 0.012768\n",
      "Epoch: 453/1000 Iteration: 454 Train loss: 0.012749\n",
      "Epoch: 454/1000 Iteration: 455 Train loss: 0.012730\n",
      "Epoch: 455/1000 Iteration: 456 Train loss: 0.012711\n",
      "Epoch: 456/1000 Iteration: 457 Train loss: 0.012692\n",
      "Epoch: 457/1000 Iteration: 458 Train loss: 0.012674\n",
      "Epoch: 458/1000 Iteration: 459 Train loss: 0.012655\n",
      "Epoch: 459/1000 Iteration: 460 Train loss: 0.012637\n",
      "Epoch: 460/1000 Iteration: 461 Train loss: 0.012619\n",
      "Epoch: 461/1000 Iteration: 462 Train loss: 0.012601\n",
      "Epoch: 462/1000 Iteration: 463 Train loss: 0.012583\n",
      "Epoch: 463/1000 Iteration: 464 Train loss: 0.012565\n",
      "Epoch: 464/1000 Iteration: 465 Train loss: 0.012547\n",
      "Epoch: 465/1000 Iteration: 466 Train loss: 0.012529\n",
      "Epoch: 466/1000 Iteration: 467 Train loss: 0.012512\n",
      "Epoch: 467/1000 Iteration: 468 Train loss: 0.012494\n",
      "Epoch: 468/1000 Iteration: 469 Train loss: 0.012477\n",
      "Epoch: 469/1000 Iteration: 470 Train loss: 0.012460\n",
      "Epoch: 470/1000 Iteration: 471 Train loss: 0.012442\n",
      "Epoch: 471/1000 Iteration: 472 Train loss: 0.012425\n",
      "Epoch: 472/1000 Iteration: 473 Train loss: 0.012408\n",
      "Epoch: 473/1000 Iteration: 474 Train loss: 0.012391\n",
      "Epoch: 474/1000 Iteration: 475 Train loss: 0.012375\n",
      "Epoch: 475/1000 Iteration: 476 Train loss: 0.012358\n",
      "Epoch: 476/1000 Iteration: 477 Train loss: 0.012341\n",
      "Epoch: 477/1000 Iteration: 478 Train loss: 0.012325\n",
      "Epoch: 478/1000 Iteration: 479 Train loss: 0.012308\n",
      "Epoch: 479/1000 Iteration: 480 Train loss: 0.012292\n",
      "Epoch: 480/1000 Iteration: 481 Train loss: 0.012276\n",
      "Epoch: 481/1000 Iteration: 482 Train loss: 0.012260\n",
      "Epoch: 482/1000 Iteration: 483 Train loss: 0.012244\n",
      "Epoch: 483/1000 Iteration: 484 Train loss: 0.012228\n",
      "Epoch: 484/1000 Iteration: 485 Train loss: 0.012212\n",
      "Epoch: 485/1000 Iteration: 486 Train loss: 0.012196\n",
      "Epoch: 486/1000 Iteration: 487 Train loss: 0.012180\n",
      "Epoch: 487/1000 Iteration: 488 Train loss: 0.012165\n",
      "Epoch: 488/1000 Iteration: 489 Train loss: 0.012149\n",
      "Epoch: 489/1000 Iteration: 490 Train loss: 0.012134\n",
      "Epoch: 490/1000 Iteration: 491 Train loss: 0.012119\n",
      "Epoch: 491/1000 Iteration: 492 Train loss: 0.012103\n",
      "Epoch: 492/1000 Iteration: 493 Train loss: 0.012088\n",
      "Epoch: 493/1000 Iteration: 494 Train loss: 0.012073\n",
      "Epoch: 494/1000 Iteration: 495 Train loss: 0.012058\n",
      "Epoch: 495/1000 Iteration: 496 Train loss: 0.012043\n",
      "Epoch: 496/1000 Iteration: 497 Train loss: 0.012029\n",
      "Epoch: 497/1000 Iteration: 498 Train loss: 0.012014\n",
      "Epoch: 498/1000 Iteration: 499 Train loss: 0.011999\n",
      "Epoch: 499/1000 Iteration: 500 Train loss: 0.011985\n",
      "Epoch: 500/1000 Iteration: 501 Train loss: 0.011970\n",
      "Epoch: 501/1000 Iteration: 502 Train loss: 0.011956\n",
      "Epoch: 502/1000 Iteration: 503 Train loss: 0.011942\n",
      "Epoch: 503/1000 Iteration: 504 Train loss: 0.011928\n",
      "Epoch: 504/1000 Iteration: 505 Train loss: 0.011914\n",
      "Epoch: 505/1000 Iteration: 506 Train loss: 0.011900\n",
      "Epoch: 506/1000 Iteration: 507 Train loss: 0.011886\n",
      "Epoch: 507/1000 Iteration: 508 Train loss: 0.011872\n",
      "Epoch: 508/1000 Iteration: 509 Train loss: 0.011858\n",
      "Epoch: 509/1000 Iteration: 510 Train loss: 0.011844\n",
      "Epoch: 510/1000 Iteration: 511 Train loss: 0.011831\n",
      "Epoch: 511/1000 Iteration: 512 Train loss: 0.011817\n",
      "Epoch: 512/1000 Iteration: 513 Train loss: 0.011804\n",
      "Epoch: 513/1000 Iteration: 514 Train loss: 0.011790\n",
      "Epoch: 514/1000 Iteration: 515 Train loss: 0.011777\n",
      "Epoch: 515/1000 Iteration: 516 Train loss: 0.011764\n",
      "Epoch: 516/1000 Iteration: 517 Train loss: 0.011751\n",
      "Epoch: 517/1000 Iteration: 518 Train loss: 0.011738\n",
      "Epoch: 518/1000 Iteration: 519 Train loss: 0.011725\n",
      "Epoch: 519/1000 Iteration: 520 Train loss: 0.011712\n",
      "Epoch: 520/1000 Iteration: 521 Train loss: 0.011699\n",
      "Epoch: 521/1000 Iteration: 522 Train loss: 0.011686\n",
      "Epoch: 522/1000 Iteration: 523 Train loss: 0.011673\n",
      "Epoch: 523/1000 Iteration: 524 Train loss: 0.011661\n",
      "Epoch: 524/1000 Iteration: 525 Train loss: 0.011648\n",
      "Epoch: 525/1000 Iteration: 526 Train loss: 0.011636\n",
      "Epoch: 526/1000 Iteration: 527 Train loss: 0.011623\n",
      "Epoch: 527/1000 Iteration: 528 Train loss: 0.011611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 528/1000 Iteration: 529 Train loss: 0.011599\n",
      "Epoch: 529/1000 Iteration: 530 Train loss: 0.011587\n",
      "Epoch: 530/1000 Iteration: 531 Train loss: 0.011575\n",
      "Epoch: 531/1000 Iteration: 532 Train loss: 0.011562\n",
      "Epoch: 532/1000 Iteration: 533 Train loss: 0.011551\n",
      "Epoch: 533/1000 Iteration: 534 Train loss: 0.011539\n",
      "Epoch: 534/1000 Iteration: 535 Train loss: 0.011527\n",
      "Epoch: 535/1000 Iteration: 536 Train loss: 0.011515\n",
      "Epoch: 536/1000 Iteration: 537 Train loss: 0.011503\n",
      "Epoch: 537/1000 Iteration: 538 Train loss: 0.011492\n",
      "Epoch: 538/1000 Iteration: 539 Train loss: 0.011480\n",
      "Epoch: 539/1000 Iteration: 540 Train loss: 0.011469\n",
      "Epoch: 540/1000 Iteration: 541 Train loss: 0.011457\n",
      "Epoch: 541/1000 Iteration: 542 Train loss: 0.011446\n",
      "Epoch: 542/1000 Iteration: 543 Train loss: 0.011434\n",
      "Epoch: 543/1000 Iteration: 544 Train loss: 0.011423\n",
      "Epoch: 544/1000 Iteration: 545 Train loss: 0.011412\n",
      "Epoch: 545/1000 Iteration: 546 Train loss: 0.011401\n",
      "Epoch: 546/1000 Iteration: 547 Train loss: 0.011390\n",
      "Epoch: 547/1000 Iteration: 548 Train loss: 0.011379\n",
      "Epoch: 548/1000 Iteration: 549 Train loss: 0.011368\n",
      "Epoch: 549/1000 Iteration: 550 Train loss: 0.011357\n",
      "Epoch: 550/1000 Iteration: 551 Train loss: 0.011346\n",
      "Epoch: 551/1000 Iteration: 552 Train loss: 0.011335\n",
      "Epoch: 552/1000 Iteration: 553 Train loss: 0.011325\n",
      "Epoch: 553/1000 Iteration: 554 Train loss: 0.011314\n",
      "Epoch: 554/1000 Iteration: 555 Train loss: 0.011304\n",
      "Epoch: 555/1000 Iteration: 556 Train loss: 0.011293\n",
      "Epoch: 556/1000 Iteration: 557 Train loss: 0.011283\n",
      "Epoch: 557/1000 Iteration: 558 Train loss: 0.011272\n",
      "Epoch: 558/1000 Iteration: 559 Train loss: 0.011262\n",
      "Epoch: 559/1000 Iteration: 560 Train loss: 0.011252\n",
      "Epoch: 560/1000 Iteration: 561 Train loss: 0.011241\n",
      "Epoch: 561/1000 Iteration: 562 Train loss: 0.011231\n",
      "Epoch: 562/1000 Iteration: 563 Train loss: 0.011221\n",
      "Epoch: 563/1000 Iteration: 564 Train loss: 0.011211\n",
      "Epoch: 564/1000 Iteration: 565 Train loss: 0.011201\n",
      "Epoch: 565/1000 Iteration: 566 Train loss: 0.011191\n",
      "Epoch: 566/1000 Iteration: 567 Train loss: 0.011181\n",
      "Epoch: 567/1000 Iteration: 568 Train loss: 0.011171\n",
      "Epoch: 568/1000 Iteration: 569 Train loss: 0.011161\n",
      "Epoch: 569/1000 Iteration: 570 Train loss: 0.011152\n",
      "Epoch: 570/1000 Iteration: 571 Train loss: 0.011142\n",
      "Epoch: 571/1000 Iteration: 572 Train loss: 0.011132\n",
      "Epoch: 572/1000 Iteration: 573 Train loss: 0.011123\n",
      "Epoch: 573/1000 Iteration: 574 Train loss: 0.011113\n",
      "Epoch: 574/1000 Iteration: 575 Train loss: 0.011104\n",
      "Epoch: 575/1000 Iteration: 576 Train loss: 0.011094\n",
      "Epoch: 576/1000 Iteration: 577 Train loss: 0.011085\n",
      "Epoch: 577/1000 Iteration: 578 Train loss: 0.011076\n",
      "Epoch: 578/1000 Iteration: 579 Train loss: 0.011066\n",
      "Epoch: 579/1000 Iteration: 580 Train loss: 0.011057\n",
      "Epoch: 580/1000 Iteration: 581 Train loss: 0.011048\n",
      "Epoch: 581/1000 Iteration: 582 Train loss: 0.011039\n",
      "Epoch: 582/1000 Iteration: 583 Train loss: 0.011030\n",
      "Epoch: 583/1000 Iteration: 584 Train loss: 0.011021\n",
      "Epoch: 584/1000 Iteration: 585 Train loss: 0.011012\n",
      "Epoch: 585/1000 Iteration: 586 Train loss: 0.011003\n",
      "Epoch: 586/1000 Iteration: 587 Train loss: 0.010994\n",
      "Epoch: 587/1000 Iteration: 588 Train loss: 0.010985\n",
      "Epoch: 588/1000 Iteration: 589 Train loss: 0.010976\n",
      "Epoch: 589/1000 Iteration: 590 Train loss: 0.010967\n",
      "Epoch: 590/1000 Iteration: 591 Train loss: 0.010959\n",
      "Epoch: 591/1000 Iteration: 592 Train loss: 0.010950\n",
      "Epoch: 592/1000 Iteration: 593 Train loss: 0.010941\n",
      "Epoch: 593/1000 Iteration: 594 Train loss: 0.010933\n",
      "Epoch: 594/1000 Iteration: 595 Train loss: 0.010924\n",
      "Epoch: 595/1000 Iteration: 596 Train loss: 0.010916\n",
      "Epoch: 596/1000 Iteration: 597 Train loss: 0.010907\n",
      "Epoch: 597/1000 Iteration: 598 Train loss: 0.010899\n",
      "Epoch: 598/1000 Iteration: 599 Train loss: 0.010890\n",
      "Epoch: 599/1000 Iteration: 600 Train loss: 0.010882\n",
      "Epoch: 600/1000 Iteration: 601 Train loss: 0.010874\n",
      "Epoch: 601/1000 Iteration: 602 Train loss: 0.010865\n",
      "Epoch: 602/1000 Iteration: 603 Train loss: 0.010857\n",
      "Epoch: 603/1000 Iteration: 604 Train loss: 0.010849\n",
      "Epoch: 604/1000 Iteration: 605 Train loss: 0.010841\n",
      "Epoch: 605/1000 Iteration: 606 Train loss: 0.010833\n",
      "Epoch: 606/1000 Iteration: 607 Train loss: 0.010825\n",
      "Epoch: 607/1000 Iteration: 608 Train loss: 0.010817\n",
      "Epoch: 608/1000 Iteration: 609 Train loss: 0.010809\n",
      "Epoch: 609/1000 Iteration: 610 Train loss: 0.010801\n",
      "Epoch: 610/1000 Iteration: 611 Train loss: 0.010793\n",
      "Epoch: 611/1000 Iteration: 612 Train loss: 0.010785\n",
      "Epoch: 612/1000 Iteration: 613 Train loss: 0.010777\n",
      "Epoch: 613/1000 Iteration: 614 Train loss: 0.010769\n",
      "Epoch: 614/1000 Iteration: 615 Train loss: 0.010761\n",
      "Epoch: 615/1000 Iteration: 616 Train loss: 0.010754\n",
      "Epoch: 616/1000 Iteration: 617 Train loss: 0.010746\n",
      "Epoch: 617/1000 Iteration: 618 Train loss: 0.010738\n",
      "Epoch: 618/1000 Iteration: 619 Train loss: 0.010731\n",
      "Epoch: 619/1000 Iteration: 620 Train loss: 0.010723\n",
      "Epoch: 620/1000 Iteration: 621 Train loss: 0.010716\n",
      "Epoch: 621/1000 Iteration: 622 Train loss: 0.010708\n",
      "Epoch: 622/1000 Iteration: 623 Train loss: 0.010701\n",
      "Epoch: 623/1000 Iteration: 624 Train loss: 0.010693\n",
      "Epoch: 624/1000 Iteration: 625 Train loss: 0.010686\n",
      "Epoch: 625/1000 Iteration: 626 Train loss: 0.010678\n",
      "Epoch: 626/1000 Iteration: 627 Train loss: 0.010671\n",
      "Epoch: 627/1000 Iteration: 628 Train loss: 0.010664\n",
      "Epoch: 628/1000 Iteration: 629 Train loss: 0.010656\n",
      "Epoch: 629/1000 Iteration: 630 Train loss: 0.010649\n",
      "Epoch: 630/1000 Iteration: 631 Train loss: 0.010642\n",
      "Epoch: 631/1000 Iteration: 632 Train loss: 0.010635\n",
      "Epoch: 632/1000 Iteration: 633 Train loss: 0.010627\n",
      "Epoch: 633/1000 Iteration: 634 Train loss: 0.010620\n",
      "Epoch: 634/1000 Iteration: 635 Train loss: 0.010613\n",
      "Epoch: 635/1000 Iteration: 636 Train loss: 0.010606\n",
      "Epoch: 636/1000 Iteration: 637 Train loss: 0.010599\n",
      "Epoch: 637/1000 Iteration: 638 Train loss: 0.010592\n",
      "Epoch: 638/1000 Iteration: 639 Train loss: 0.010585\n",
      "Epoch: 639/1000 Iteration: 640 Train loss: 0.010578\n",
      "Epoch: 640/1000 Iteration: 641 Train loss: 0.010571\n",
      "Epoch: 641/1000 Iteration: 642 Train loss: 0.010564\n",
      "Epoch: 642/1000 Iteration: 643 Train loss: 0.010557\n",
      "Epoch: 643/1000 Iteration: 644 Train loss: 0.010550\n",
      "Epoch: 644/1000 Iteration: 645 Train loss: 0.010544\n",
      "Epoch: 645/1000 Iteration: 646 Train loss: 0.010537\n",
      "Epoch: 646/1000 Iteration: 647 Train loss: 0.010530\n",
      "Epoch: 647/1000 Iteration: 648 Train loss: 0.010523\n",
      "Epoch: 648/1000 Iteration: 649 Train loss: 0.010517\n",
      "Epoch: 649/1000 Iteration: 650 Train loss: 0.010510\n",
      "Epoch: 650/1000 Iteration: 651 Train loss: 0.010503\n",
      "Epoch: 651/1000 Iteration: 652 Train loss: 0.010497\n",
      "Epoch: 652/1000 Iteration: 653 Train loss: 0.010490\n",
      "Epoch: 653/1000 Iteration: 654 Train loss: 0.010483\n",
      "Epoch: 654/1000 Iteration: 655 Train loss: 0.010477\n",
      "Epoch: 655/1000 Iteration: 656 Train loss: 0.010470\n",
      "Epoch: 656/1000 Iteration: 657 Train loss: 0.010464\n",
      "Epoch: 657/1000 Iteration: 658 Train loss: 0.010457\n",
      "Epoch: 658/1000 Iteration: 659 Train loss: 0.010451\n",
      "Epoch: 659/1000 Iteration: 660 Train loss: 0.010444\n",
      "Epoch: 660/1000 Iteration: 661 Train loss: 0.010438\n",
      "Epoch: 661/1000 Iteration: 662 Train loss: 0.010432\n",
      "Epoch: 662/1000 Iteration: 663 Train loss: 0.010425\n",
      "Epoch: 663/1000 Iteration: 664 Train loss: 0.010419\n",
      "Epoch: 664/1000 Iteration: 665 Train loss: 0.010412\n",
      "Epoch: 665/1000 Iteration: 666 Train loss: 0.010406\n",
      "Epoch: 666/1000 Iteration: 667 Train loss: 0.010400\n",
      "Epoch: 667/1000 Iteration: 668 Train loss: 0.010394\n",
      "Epoch: 668/1000 Iteration: 669 Train loss: 0.010387\n",
      "Epoch: 669/1000 Iteration: 670 Train loss: 0.010381\n",
      "Epoch: 670/1000 Iteration: 671 Train loss: 0.010375\n",
      "Epoch: 671/1000 Iteration: 672 Train loss: 0.010369\n",
      "Epoch: 672/1000 Iteration: 673 Train loss: 0.010363\n",
      "Epoch: 673/1000 Iteration: 674 Train loss: 0.010357\n",
      "Epoch: 674/1000 Iteration: 675 Train loss: 0.010350\n",
      "Epoch: 675/1000 Iteration: 676 Train loss: 0.010344\n",
      "Epoch: 676/1000 Iteration: 677 Train loss: 0.010338\n",
      "Epoch: 677/1000 Iteration: 678 Train loss: 0.010332\n",
      "Epoch: 678/1000 Iteration: 679 Train loss: 0.010326\n",
      "Epoch: 679/1000 Iteration: 680 Train loss: 0.010320\n",
      "Epoch: 680/1000 Iteration: 681 Train loss: 0.010314\n",
      "Epoch: 681/1000 Iteration: 682 Train loss: 0.010308\n",
      "Epoch: 682/1000 Iteration: 683 Train loss: 0.010302\n",
      "Epoch: 683/1000 Iteration: 684 Train loss: 0.010296\n",
      "Epoch: 684/1000 Iteration: 685 Train loss: 0.010290\n",
      "Epoch: 685/1000 Iteration: 686 Train loss: 0.010285\n",
      "Epoch: 686/1000 Iteration: 687 Train loss: 0.010279\n",
      "Epoch: 687/1000 Iteration: 688 Train loss: 0.010273\n",
      "Epoch: 688/1000 Iteration: 689 Train loss: 0.010267\n",
      "Epoch: 689/1000 Iteration: 690 Train loss: 0.010261\n",
      "Epoch: 690/1000 Iteration: 691 Train loss: 0.010255\n",
      "Epoch: 691/1000 Iteration: 692 Train loss: 0.010250\n",
      "Epoch: 692/1000 Iteration: 693 Train loss: 0.010244\n",
      "Epoch: 693/1000 Iteration: 694 Train loss: 0.010238\n",
      "Epoch: 694/1000 Iteration: 695 Train loss: 0.010232\n",
      "Epoch: 695/1000 Iteration: 696 Train loss: 0.010227\n",
      "Epoch: 696/1000 Iteration: 697 Train loss: 0.010221\n",
      "Epoch: 697/1000 Iteration: 698 Train loss: 0.010215\n",
      "Epoch: 698/1000 Iteration: 699 Train loss: 0.010210\n",
      "Epoch: 699/1000 Iteration: 700 Train loss: 0.010204\n",
      "Epoch: 700/1000 Iteration: 701 Train loss: 0.010198\n",
      "Epoch: 701/1000 Iteration: 702 Train loss: 0.010193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 702/1000 Iteration: 703 Train loss: 0.010187\n",
      "Epoch: 703/1000 Iteration: 704 Train loss: 0.010182\n",
      "Epoch: 704/1000 Iteration: 705 Train loss: 0.010176\n",
      "Epoch: 705/1000 Iteration: 706 Train loss: 0.010170\n",
      "Epoch: 706/1000 Iteration: 707 Train loss: 0.010165\n",
      "Epoch: 707/1000 Iteration: 708 Train loss: 0.010159\n",
      "Epoch: 708/1000 Iteration: 709 Train loss: 0.010154\n",
      "Epoch: 709/1000 Iteration: 710 Train loss: 0.010148\n",
      "Epoch: 710/1000 Iteration: 711 Train loss: 0.010143\n",
      "Epoch: 711/1000 Iteration: 712 Train loss: 0.010137\n",
      "Epoch: 712/1000 Iteration: 713 Train loss: 0.010132\n",
      "Epoch: 713/1000 Iteration: 714 Train loss: 0.010127\n",
      "Epoch: 714/1000 Iteration: 715 Train loss: 0.010121\n",
      "Epoch: 715/1000 Iteration: 716 Train loss: 0.010116\n",
      "Epoch: 716/1000 Iteration: 717 Train loss: 0.010110\n",
      "Epoch: 717/1000 Iteration: 718 Train loss: 0.010105\n",
      "Epoch: 718/1000 Iteration: 719 Train loss: 0.010100\n",
      "Epoch: 719/1000 Iteration: 720 Train loss: 0.010094\n",
      "Epoch: 720/1000 Iteration: 721 Train loss: 0.010089\n",
      "Epoch: 721/1000 Iteration: 722 Train loss: 0.010084\n",
      "Epoch: 722/1000 Iteration: 723 Train loss: 0.010078\n",
      "Epoch: 723/1000 Iteration: 724 Train loss: 0.010073\n",
      "Epoch: 724/1000 Iteration: 725 Train loss: 0.010068\n",
      "Epoch: 725/1000 Iteration: 726 Train loss: 0.010062\n",
      "Epoch: 726/1000 Iteration: 727 Train loss: 0.010057\n",
      "Epoch: 727/1000 Iteration: 728 Train loss: 0.010052\n",
      "Epoch: 728/1000 Iteration: 729 Train loss: 0.010047\n",
      "Epoch: 729/1000 Iteration: 730 Train loss: 0.010042\n",
      "Epoch: 730/1000 Iteration: 731 Train loss: 0.010036\n",
      "Epoch: 731/1000 Iteration: 732 Train loss: 0.010031\n",
      "Epoch: 732/1000 Iteration: 733 Train loss: 0.010026\n",
      "Epoch: 733/1000 Iteration: 734 Train loss: 0.010021\n",
      "Epoch: 734/1000 Iteration: 735 Train loss: 0.010016\n",
      "Epoch: 735/1000 Iteration: 736 Train loss: 0.010011\n",
      "Epoch: 736/1000 Iteration: 737 Train loss: 0.010005\n",
      "Epoch: 737/1000 Iteration: 738 Train loss: 0.010000\n",
      "Epoch: 738/1000 Iteration: 739 Train loss: 0.009995\n",
      "Epoch: 739/1000 Iteration: 740 Train loss: 0.009990\n",
      "Epoch: 740/1000 Iteration: 741 Train loss: 0.009985\n",
      "Epoch: 741/1000 Iteration: 742 Train loss: 0.009980\n",
      "Epoch: 742/1000 Iteration: 743 Train loss: 0.009975\n",
      "Epoch: 743/1000 Iteration: 744 Train loss: 0.009970\n",
      "Epoch: 744/1000 Iteration: 745 Train loss: 0.009965\n",
      "Epoch: 745/1000 Iteration: 746 Train loss: 0.009960\n",
      "Epoch: 746/1000 Iteration: 747 Train loss: 0.009955\n",
      "Epoch: 747/1000 Iteration: 748 Train loss: 0.009950\n",
      "Epoch: 748/1000 Iteration: 749 Train loss: 0.009945\n",
      "Epoch: 749/1000 Iteration: 750 Train loss: 0.009940\n",
      "Epoch: 750/1000 Iteration: 751 Train loss: 0.009935\n",
      "Epoch: 751/1000 Iteration: 752 Train loss: 0.009930\n",
      "Epoch: 752/1000 Iteration: 753 Train loss: 0.009925\n",
      "Epoch: 753/1000 Iteration: 754 Train loss: 0.009920\n",
      "Epoch: 754/1000 Iteration: 755 Train loss: 0.009915\n",
      "Epoch: 755/1000 Iteration: 756 Train loss: 0.009910\n",
      "Epoch: 756/1000 Iteration: 757 Train loss: 0.009905\n",
      "Epoch: 757/1000 Iteration: 758 Train loss: 0.009900\n",
      "Epoch: 758/1000 Iteration: 759 Train loss: 0.009895\n",
      "Epoch: 759/1000 Iteration: 760 Train loss: 0.009890\n",
      "Epoch: 760/1000 Iteration: 761 Train loss: 0.009886\n",
      "Epoch: 761/1000 Iteration: 762 Train loss: 0.009881\n",
      "Epoch: 762/1000 Iteration: 763 Train loss: 0.009876\n",
      "Epoch: 763/1000 Iteration: 764 Train loss: 0.009871\n",
      "Epoch: 764/1000 Iteration: 765 Train loss: 0.009866\n",
      "Epoch: 765/1000 Iteration: 766 Train loss: 0.009861\n",
      "Epoch: 766/1000 Iteration: 767 Train loss: 0.009857\n",
      "Epoch: 767/1000 Iteration: 768 Train loss: 0.009852\n",
      "Epoch: 768/1000 Iteration: 769 Train loss: 0.009847\n",
      "Epoch: 769/1000 Iteration: 770 Train loss: 0.009842\n",
      "Epoch: 770/1000 Iteration: 771 Train loss: 0.009837\n",
      "Epoch: 771/1000 Iteration: 772 Train loss: 0.009833\n",
      "Epoch: 772/1000 Iteration: 773 Train loss: 0.009828\n",
      "Epoch: 773/1000 Iteration: 774 Train loss: 0.009823\n",
      "Epoch: 774/1000 Iteration: 775 Train loss: 0.009818\n",
      "Epoch: 775/1000 Iteration: 776 Train loss: 0.009814\n",
      "Epoch: 776/1000 Iteration: 777 Train loss: 0.009809\n",
      "Epoch: 777/1000 Iteration: 778 Train loss: 0.009804\n",
      "Epoch: 778/1000 Iteration: 779 Train loss: 0.009799\n",
      "Epoch: 779/1000 Iteration: 780 Train loss: 0.009795\n",
      "Epoch: 780/1000 Iteration: 781 Train loss: 0.009790\n",
      "Epoch: 781/1000 Iteration: 782 Train loss: 0.009785\n",
      "Epoch: 782/1000 Iteration: 783 Train loss: 0.009781\n",
      "Epoch: 783/1000 Iteration: 784 Train loss: 0.009776\n",
      "Epoch: 784/1000 Iteration: 785 Train loss: 0.009771\n",
      "Epoch: 785/1000 Iteration: 786 Train loss: 0.009767\n",
      "Epoch: 786/1000 Iteration: 787 Train loss: 0.009762\n",
      "Epoch: 787/1000 Iteration: 788 Train loss: 0.009757\n",
      "Epoch: 788/1000 Iteration: 789 Train loss: 0.009753\n",
      "Epoch: 789/1000 Iteration: 790 Train loss: 0.009748\n",
      "Epoch: 790/1000 Iteration: 791 Train loss: 0.009743\n",
      "Epoch: 791/1000 Iteration: 792 Train loss: 0.009739\n",
      "Epoch: 792/1000 Iteration: 793 Train loss: 0.009734\n",
      "Epoch: 793/1000 Iteration: 794 Train loss: 0.009730\n",
      "Epoch: 794/1000 Iteration: 795 Train loss: 0.009725\n",
      "Epoch: 795/1000 Iteration: 796 Train loss: 0.009720\n",
      "Epoch: 796/1000 Iteration: 797 Train loss: 0.009716\n",
      "Epoch: 797/1000 Iteration: 798 Train loss: 0.009711\n",
      "Epoch: 798/1000 Iteration: 799 Train loss: 0.009707\n",
      "Epoch: 799/1000 Iteration: 800 Train loss: 0.009702\n",
      "Epoch: 800/1000 Iteration: 801 Train loss: 0.009698\n",
      "Epoch: 801/1000 Iteration: 802 Train loss: 0.009693\n",
      "Epoch: 802/1000 Iteration: 803 Train loss: 0.009689\n",
      "Epoch: 803/1000 Iteration: 804 Train loss: 0.009684\n",
      "Epoch: 804/1000 Iteration: 805 Train loss: 0.009680\n",
      "Epoch: 805/1000 Iteration: 806 Train loss: 0.009675\n",
      "Epoch: 806/1000 Iteration: 807 Train loss: 0.009671\n",
      "Epoch: 807/1000 Iteration: 808 Train loss: 0.009666\n",
      "Epoch: 808/1000 Iteration: 809 Train loss: 0.009662\n",
      "Epoch: 809/1000 Iteration: 810 Train loss: 0.009657\n",
      "Epoch: 810/1000 Iteration: 811 Train loss: 0.009653\n",
      "Epoch: 811/1000 Iteration: 812 Train loss: 0.009648\n",
      "Epoch: 812/1000 Iteration: 813 Train loss: 0.009644\n",
      "Epoch: 813/1000 Iteration: 814 Train loss: 0.009639\n",
      "Epoch: 814/1000 Iteration: 815 Train loss: 0.009635\n",
      "Epoch: 815/1000 Iteration: 816 Train loss: 0.009630\n",
      "Epoch: 816/1000 Iteration: 817 Train loss: 0.009626\n",
      "Epoch: 817/1000 Iteration: 818 Train loss: 0.009621\n",
      "Epoch: 818/1000 Iteration: 819 Train loss: 0.009617\n",
      "Epoch: 819/1000 Iteration: 820 Train loss: 0.009613\n",
      "Epoch: 820/1000 Iteration: 821 Train loss: 0.009608\n",
      "Epoch: 821/1000 Iteration: 822 Train loss: 0.009604\n",
      "Epoch: 822/1000 Iteration: 823 Train loss: 0.009599\n",
      "Epoch: 823/1000 Iteration: 824 Train loss: 0.009595\n",
      "Epoch: 824/1000 Iteration: 825 Train loss: 0.009591\n",
      "Epoch: 825/1000 Iteration: 826 Train loss: 0.009586\n",
      "Epoch: 826/1000 Iteration: 827 Train loss: 0.009582\n",
      "Epoch: 827/1000 Iteration: 828 Train loss: 0.009577\n",
      "Epoch: 828/1000 Iteration: 829 Train loss: 0.009573\n",
      "Epoch: 829/1000 Iteration: 830 Train loss: 0.009569\n",
      "Epoch: 830/1000 Iteration: 831 Train loss: 0.009564\n",
      "Epoch: 831/1000 Iteration: 832 Train loss: 0.009560\n",
      "Epoch: 832/1000 Iteration: 833 Train loss: 0.009556\n",
      "Epoch: 833/1000 Iteration: 834 Train loss: 0.009551\n",
      "Epoch: 834/1000 Iteration: 835 Train loss: 0.009547\n",
      "Epoch: 835/1000 Iteration: 836 Train loss: 0.009543\n",
      "Epoch: 836/1000 Iteration: 837 Train loss: 0.009538\n",
      "Epoch: 837/1000 Iteration: 838 Train loss: 0.009534\n",
      "Epoch: 838/1000 Iteration: 839 Train loss: 0.009530\n",
      "Epoch: 839/1000 Iteration: 840 Train loss: 0.009525\n",
      "Epoch: 840/1000 Iteration: 841 Train loss: 0.009521\n",
      "Epoch: 841/1000 Iteration: 842 Train loss: 0.009517\n",
      "Epoch: 842/1000 Iteration: 843 Train loss: 0.009513\n",
      "Epoch: 843/1000 Iteration: 844 Train loss: 0.009508\n",
      "Epoch: 844/1000 Iteration: 845 Train loss: 0.009504\n",
      "Epoch: 845/1000 Iteration: 846 Train loss: 0.009500\n",
      "Epoch: 846/1000 Iteration: 847 Train loss: 0.009495\n",
      "Epoch: 847/1000 Iteration: 848 Train loss: 0.009491\n",
      "Epoch: 848/1000 Iteration: 849 Train loss: 0.009487\n",
      "Epoch: 849/1000 Iteration: 850 Train loss: 0.009483\n",
      "Epoch: 850/1000 Iteration: 851 Train loss: 0.009478\n",
      "Epoch: 851/1000 Iteration: 852 Train loss: 0.009474\n",
      "Epoch: 852/1000 Iteration: 853 Train loss: 0.009470\n",
      "Epoch: 853/1000 Iteration: 854 Train loss: 0.009466\n",
      "Epoch: 854/1000 Iteration: 855 Train loss: 0.009462\n",
      "Epoch: 855/1000 Iteration: 856 Train loss: 0.009457\n",
      "Epoch: 856/1000 Iteration: 857 Train loss: 0.009453\n",
      "Epoch: 857/1000 Iteration: 858 Train loss: 0.009449\n",
      "Epoch: 858/1000 Iteration: 859 Train loss: 0.009445\n",
      "Epoch: 859/1000 Iteration: 860 Train loss: 0.009440\n",
      "Epoch: 860/1000 Iteration: 861 Train loss: 0.009436\n",
      "Epoch: 861/1000 Iteration: 862 Train loss: 0.009432\n",
      "Epoch: 862/1000 Iteration: 863 Train loss: 0.009428\n",
      "Epoch: 863/1000 Iteration: 864 Train loss: 0.009424\n",
      "Epoch: 864/1000 Iteration: 865 Train loss: 0.009420\n",
      "Epoch: 865/1000 Iteration: 866 Train loss: 0.009415\n",
      "Epoch: 866/1000 Iteration: 867 Train loss: 0.009411\n",
      "Epoch: 867/1000 Iteration: 868 Train loss: 0.009407\n",
      "Epoch: 868/1000 Iteration: 869 Train loss: 0.009403\n",
      "Epoch: 869/1000 Iteration: 870 Train loss: 0.009399\n",
      "Epoch: 870/1000 Iteration: 871 Train loss: 0.009395\n",
      "Epoch: 871/1000 Iteration: 872 Train loss: 0.009390\n",
      "Epoch: 872/1000 Iteration: 873 Train loss: 0.009386\n",
      "Epoch: 873/1000 Iteration: 874 Train loss: 0.009382\n",
      "Epoch: 874/1000 Iteration: 875 Train loss: 0.009378\n",
      "Epoch: 875/1000 Iteration: 876 Train loss: 0.009374\n",
      "Epoch: 876/1000 Iteration: 877 Train loss: 0.009370\n",
      "Epoch: 877/1000 Iteration: 878 Train loss: 0.009366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 878/1000 Iteration: 879 Train loss: 0.009362\n",
      "Epoch: 879/1000 Iteration: 880 Train loss: 0.009357\n",
      "Epoch: 880/1000 Iteration: 881 Train loss: 0.009353\n",
      "Epoch: 881/1000 Iteration: 882 Train loss: 0.009349\n",
      "Epoch: 882/1000 Iteration: 883 Train loss: 0.009345\n",
      "Epoch: 883/1000 Iteration: 884 Train loss: 0.009341\n",
      "Epoch: 884/1000 Iteration: 885 Train loss: 0.009337\n",
      "Epoch: 885/1000 Iteration: 886 Train loss: 0.009333\n",
      "Epoch: 886/1000 Iteration: 887 Train loss: 0.009329\n",
      "Epoch: 887/1000 Iteration: 888 Train loss: 0.009325\n",
      "Epoch: 888/1000 Iteration: 889 Train loss: 0.009321\n",
      "Epoch: 889/1000 Iteration: 890 Train loss: 0.009317\n",
      "Epoch: 890/1000 Iteration: 891 Train loss: 0.009313\n",
      "Epoch: 891/1000 Iteration: 892 Train loss: 0.009308\n",
      "Epoch: 892/1000 Iteration: 893 Train loss: 0.009304\n",
      "Epoch: 893/1000 Iteration: 894 Train loss: 0.009300\n",
      "Epoch: 894/1000 Iteration: 895 Train loss: 0.009296\n",
      "Epoch: 895/1000 Iteration: 896 Train loss: 0.009292\n",
      "Epoch: 896/1000 Iteration: 897 Train loss: 0.009288\n",
      "Epoch: 897/1000 Iteration: 898 Train loss: 0.009284\n",
      "Epoch: 898/1000 Iteration: 899 Train loss: 0.009280\n",
      "Epoch: 899/1000 Iteration: 900 Train loss: 0.009276\n",
      "Epoch: 900/1000 Iteration: 901 Train loss: 0.009272\n",
      "Epoch: 901/1000 Iteration: 902 Train loss: 0.009268\n",
      "Epoch: 902/1000 Iteration: 903 Train loss: 0.009264\n",
      "Epoch: 903/1000 Iteration: 904 Train loss: 0.009260\n",
      "Epoch: 904/1000 Iteration: 905 Train loss: 0.009256\n",
      "Epoch: 905/1000 Iteration: 906 Train loss: 0.009252\n",
      "Epoch: 906/1000 Iteration: 907 Train loss: 0.009248\n",
      "Epoch: 907/1000 Iteration: 908 Train loss: 0.009244\n",
      "Epoch: 908/1000 Iteration: 909 Train loss: 0.009240\n",
      "Epoch: 909/1000 Iteration: 910 Train loss: 0.009236\n",
      "Epoch: 910/1000 Iteration: 911 Train loss: 0.009232\n",
      "Epoch: 911/1000 Iteration: 912 Train loss: 0.009228\n",
      "Epoch: 912/1000 Iteration: 913 Train loss: 0.009224\n",
      "Epoch: 913/1000 Iteration: 914 Train loss: 0.009220\n",
      "Epoch: 914/1000 Iteration: 915 Train loss: 0.009216\n",
      "Epoch: 915/1000 Iteration: 916 Train loss: 0.009212\n",
      "Epoch: 916/1000 Iteration: 917 Train loss: 0.009208\n",
      "Epoch: 917/1000 Iteration: 918 Train loss: 0.009204\n",
      "Epoch: 918/1000 Iteration: 919 Train loss: 0.009200\n",
      "Epoch: 919/1000 Iteration: 920 Train loss: 0.009197\n",
      "Epoch: 920/1000 Iteration: 921 Train loss: 0.009193\n",
      "Epoch: 921/1000 Iteration: 922 Train loss: 0.009189\n",
      "Epoch: 922/1000 Iteration: 923 Train loss: 0.009185\n",
      "Epoch: 923/1000 Iteration: 924 Train loss: 0.009181\n",
      "Epoch: 924/1000 Iteration: 925 Train loss: 0.009177\n",
      "Epoch: 925/1000 Iteration: 926 Train loss: 0.009173\n",
      "Epoch: 926/1000 Iteration: 927 Train loss: 0.009169\n",
      "Epoch: 927/1000 Iteration: 928 Train loss: 0.009165\n",
      "Epoch: 928/1000 Iteration: 929 Train loss: 0.009161\n",
      "Epoch: 929/1000 Iteration: 930 Train loss: 0.009157\n",
      "Epoch: 930/1000 Iteration: 931 Train loss: 0.009153\n",
      "Epoch: 931/1000 Iteration: 932 Train loss: 0.009149\n",
      "Epoch: 932/1000 Iteration: 933 Train loss: 0.009146\n",
      "Epoch: 933/1000 Iteration: 934 Train loss: 0.009142\n",
      "Epoch: 934/1000 Iteration: 935 Train loss: 0.009138\n",
      "Epoch: 935/1000 Iteration: 936 Train loss: 0.009134\n",
      "Epoch: 936/1000 Iteration: 937 Train loss: 0.009130\n",
      "Epoch: 937/1000 Iteration: 938 Train loss: 0.009126\n",
      "Epoch: 938/1000 Iteration: 939 Train loss: 0.009122\n",
      "Epoch: 939/1000 Iteration: 940 Train loss: 0.009118\n",
      "Epoch: 940/1000 Iteration: 941 Train loss: 0.009115\n",
      "Epoch: 941/1000 Iteration: 942 Train loss: 0.009111\n",
      "Epoch: 942/1000 Iteration: 943 Train loss: 0.009107\n",
      "Epoch: 943/1000 Iteration: 944 Train loss: 0.009103\n",
      "Epoch: 944/1000 Iteration: 945 Train loss: 0.009099\n",
      "Epoch: 945/1000 Iteration: 946 Train loss: 0.009095\n",
      "Epoch: 946/1000 Iteration: 947 Train loss: 0.009091\n",
      "Epoch: 947/1000 Iteration: 948 Train loss: 0.009087\n",
      "Epoch: 948/1000 Iteration: 949 Train loss: 0.009084\n",
      "Epoch: 949/1000 Iteration: 950 Train loss: 0.009080\n",
      "Epoch: 950/1000 Iteration: 951 Train loss: 0.009076\n",
      "Epoch: 951/1000 Iteration: 952 Train loss: 0.009072\n",
      "Epoch: 952/1000 Iteration: 953 Train loss: 0.009068\n",
      "Epoch: 953/1000 Iteration: 954 Train loss: 0.009064\n",
      "Epoch: 954/1000 Iteration: 955 Train loss: 0.009061\n",
      "Epoch: 955/1000 Iteration: 956 Train loss: 0.009057\n",
      "Epoch: 956/1000 Iteration: 957 Train loss: 0.009053\n",
      "Epoch: 957/1000 Iteration: 958 Train loss: 0.009049\n",
      "Epoch: 958/1000 Iteration: 959 Train loss: 0.009045\n",
      "Epoch: 959/1000 Iteration: 960 Train loss: 0.009042\n",
      "Epoch: 960/1000 Iteration: 961 Train loss: 0.009038\n",
      "Epoch: 961/1000 Iteration: 962 Train loss: 0.009034\n",
      "Epoch: 962/1000 Iteration: 963 Train loss: 0.009030\n",
      "Epoch: 963/1000 Iteration: 964 Train loss: 0.009026\n",
      "Epoch: 964/1000 Iteration: 965 Train loss: 0.009023\n",
      "Epoch: 965/1000 Iteration: 966 Train loss: 0.009019\n",
      "Epoch: 966/1000 Iteration: 967 Train loss: 0.009015\n",
      "Epoch: 967/1000 Iteration: 968 Train loss: 0.009011\n",
      "Epoch: 968/1000 Iteration: 969 Train loss: 0.009007\n",
      "Epoch: 969/1000 Iteration: 970 Train loss: 0.009004\n",
      "Epoch: 970/1000 Iteration: 971 Train loss: 0.009000\n",
      "Epoch: 971/1000 Iteration: 972 Train loss: 0.008996\n",
      "Epoch: 972/1000 Iteration: 973 Train loss: 0.008992\n",
      "Epoch: 973/1000 Iteration: 974 Train loss: 0.008989\n",
      "Epoch: 974/1000 Iteration: 975 Train loss: 0.008985\n",
      "Epoch: 975/1000 Iteration: 976 Train loss: 0.008981\n",
      "Epoch: 976/1000 Iteration: 977 Train loss: 0.008977\n",
      "Epoch: 977/1000 Iteration: 978 Train loss: 0.008974\n",
      "Epoch: 978/1000 Iteration: 979 Train loss: 0.008970\n",
      "Epoch: 979/1000 Iteration: 980 Train loss: 0.008966\n",
      "Epoch: 980/1000 Iteration: 981 Train loss: 0.008962\n",
      "Epoch: 981/1000 Iteration: 982 Train loss: 0.008959\n",
      "Epoch: 982/1000 Iteration: 983 Train loss: 0.008955\n",
      "Epoch: 983/1000 Iteration: 984 Train loss: 0.008951\n",
      "Epoch: 984/1000 Iteration: 985 Train loss: 0.008947\n",
      "Epoch: 985/1000 Iteration: 986 Train loss: 0.008944\n",
      "Epoch: 986/1000 Iteration: 987 Train loss: 0.008940\n",
      "Epoch: 987/1000 Iteration: 988 Train loss: 0.008936\n",
      "Epoch: 988/1000 Iteration: 989 Train loss: 0.008932\n",
      "Epoch: 989/1000 Iteration: 990 Train loss: 0.008929\n",
      "Epoch: 990/1000 Iteration: 991 Train loss: 0.008925\n",
      "Epoch: 991/1000 Iteration: 992 Train loss: 0.008921\n",
      "Epoch: 992/1000 Iteration: 993 Train loss: 0.008918\n",
      "Epoch: 993/1000 Iteration: 994 Train loss: 0.008914\n",
      "Epoch: 994/1000 Iteration: 995 Train loss: 0.008910\n",
      "Epoch: 995/1000 Iteration: 996 Train loss: 0.008906\n",
      "Epoch: 996/1000 Iteration: 997 Train loss: 0.008903\n",
      "Epoch: 997/1000 Iteration: 998 Train loss: 0.008899\n",
      "Epoch: 998/1000 Iteration: 999 Train loss: 0.008895\n",
      "Epoch: 999/1000 Iteration: 1000 Train loss: 0.008892\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Export directory already exists. Please specify a different export directory: ./models/car/model_2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a37e97e942c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rnn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTO_REUSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtrain_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a37e97e942c9>\u001b[0m in \u001b[0;36mtrain_lstm\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./models/car-lstm/car_predicts.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# 保存二进制模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSavedModelBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./models/car/model_\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_meta_graph_and_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'mytag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/saved_model/builder_impl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, export_dir)\u001b[0m\n\u001b[1;32m     88\u001b[0m       raise AssertionError(\n\u001b[1;32m     89\u001b[0m           \u001b[0;34m\"Export directory already exists. Please specify a different export \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m           \"directory: %s\" % export_dir)\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecursive_create_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_export_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Export directory already exists. Please specify a different export directory: ./models/car/model_2"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdate\n",
    "%matplotlib inline \n",
    "\n",
    "# 加载数据\n",
    "path = \"./dataSets/汽车制造企业财务风险评价.xlsx\"\n",
    "\n",
    "Matrix_pre = np.zeros(shape=(4,1))\n",
    "\n",
    "for i in range(2,5):\n",
    "    \n",
    "    print('**************************************************************************\\n')\n",
    "    print('**************************当前是第'+str(i)+'列数据,共5列**************************\\n')\n",
    "    print('**************************************************************************\\n')\n",
    "\n",
    "    dataset = pd.read_excel(path, usecols= [i], sheet_name = \"final_Index\",  nrows = 17)\n",
    "    dataset = np.array(dataset)\n",
    "\n",
    "    print(dataset)\n",
    "\n",
    "    plt.plot(dataset)\n",
    "    plt.show()\n",
    "\n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataSet = scaler.fit_transform(dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # split into train and test sets; 80% 是训练数据，其余是测试数据\n",
    "    train_size = int(len(dataSet) * 0.8)\n",
    "    test_size = len(dataSet) - train_size\n",
    "    train, test = dataSet[0:train_size], dataSet[train_size:len(dataSet)]\n",
    "\n",
    "    # 数据格式转化(t,t+1)\n",
    "    def convert_data(data, time_step=1):\n",
    "        data_X,data_Y = [],[]  \n",
    "        for i in range(len(data) - time_step - 1):\n",
    "            x = data[i: (i + time_step)]  \n",
    "            y = data[i+1:i + time_step+1]      \n",
    "            data_X.append(x.tolist())\n",
    "            data_Y.append(y.tolist()) \n",
    "        return data_X, data_Y\n",
    "\n",
    "    # fix random seed for reproducibility\n",
    "    np.random.seed(7)\n",
    "\n",
    "    # use this function to prepare the train and test datasets for modeling\n",
    "    #time_step=5\n",
    "    time_step = 5      #时间步\n",
    "    train_x, train_y = convert_data(train, time_step)\n",
    "    test_x, test_y = convert_data(test, time_step)\n",
    "\n",
    "    #———————————————————形成训练集—————————————————————\n",
    "    #设置常量\n",
    "    hidden_unit = 10       #hidden layer units 记忆和储存过去状态的节点个数\n",
    "    batch_size = 5    #每一批次训练多少个样例\n",
    "    input_size = 1      #输入层维度\n",
    "    output_size = 1     #输出层维度\n",
    "    lr = 0.00010       #学习率\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # LSTM 的 X 需要有这样的结构： [samples, time steps, features]，所以做一下变换\n",
    "    X = tf.placeholder(tf.float32, [None,time_step,input_size] ,name = 'inputs')    #每批次输入网络的tensor\n",
    "    Y = tf.placeholder(tf.float32, [None,time_step,output_size] ,name = 'outputs')   #每批次tensor对应的标签\n",
    "    # 输入层、输出层权重、偏置\n",
    "    with tf.name_scope('layer'):\n",
    "            with tf.name_scope('weights'):\n",
    "                weights={\n",
    "                         'in':tf.Variable(tf.random_normal([input_size,hidden_unit])),\n",
    "                         'out':tf.Variable(tf.random_normal([hidden_unit,1]))\n",
    "                         }\n",
    "            with tf.name_scope('biases'):\n",
    "                biases={\n",
    "                        'in':tf.Variable(tf.constant(0.1,shape=[hidden_unit,])),\n",
    "                        'out':tf.Variable(tf.constant(0.1,shape=[1,]))\n",
    "                        }\n",
    "\n",
    "    def lstm(batch):  #参数：输入网络批次数目\n",
    "\n",
    "        w_in = weights['in']\n",
    "        b_in = biases['in']\n",
    "        input = tf.reshape(X,[-1,input_size])  #需要将tensor转成2维进行计算，计算后的结果作为隐藏层的输入\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_unit) #10个节点\n",
    "        input_lstm = tf.matmul(input, w_in) + b_in\n",
    "        input_lstm = tf.reshape(input_lstm, [-1, time_step, hidden_unit])  #将tensor转成3维，作为lstm cell的输入      \n",
    "        print(input_lstm)\n",
    "        init_state = lstm_cell.zero_state(batch,dtype = tf.float32)\n",
    "        # output_rnn是记录lstm每个隐状态输出节点的结果，final_states是最后一个cell的结果，数据格式为tuple\n",
    "        output_rnn, final_states = tf.nn.dynamic_rnn(\n",
    "            lstm_cell, \n",
    "            input_lstm, \n",
    "            initial_state = init_state, \n",
    "            dtype = tf.float32) \n",
    "\n",
    "        output = tf.reshape(output_rnn, [-1, hidden_unit]) #  作为输出层的输入\n",
    "        w_out = weights['out']\n",
    "        b_out = biases['out']\n",
    "           # 预测数据\n",
    "        multi = tf.matmul(output, w_out)\n",
    "        pred = tf.add(multi, b_out, name='preds')  \n",
    "        return pred, final_states\n",
    "\n",
    "    train_loss = []\n",
    "    def train_lstm():   \n",
    "        global batch_size\n",
    "        iteration = 1\n",
    "        epochs = 1000\n",
    "    #     with tf.variable_scope(\"sec_lstm\"):\n",
    "        pred, _ = lstm(batch_size)\n",
    "        # 损失函数\n",
    "        loss = tf.reduce_mean(tf.square(tf.reshape(pred,[-1])-tf.reshape(Y, [-1])))\n",
    "        #tf.summary.scalar('loss_function', loss)\n",
    "        train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        with tf.Session() as sess:\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # summaries合并\n",
    "            #merged = tf.summary.merge_all()    \n",
    "            # 写到指定的磁盘路径中\n",
    "            #train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "            # 重复训练5000次\n",
    "            for e in range(epochs):\n",
    "                step=0\n",
    "                start = 0\n",
    "                end = start + batch_size\n",
    "                while(end < len(train_x)):\n",
    "                    x = train_x[start:end]\n",
    "                    y = train_y[start:end]\n",
    "                    _,loss_ = sess.run([train_op, loss], feed_dict = {X: x, Y:y, keep_prob : 0.3})\n",
    "                    start += batch_size\n",
    "                    end = start + batch_size\n",
    "                    # 每10步保存一次参数\n",
    "                    if step% 10 == 0:                    \n",
    "                        print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                        \"Iteration: {:d}\".format(iteration),\n",
    "                        \"Train loss: {:6f}\".format(loss_))\n",
    "                        #train_writer.add_summary(summary, e);\n",
    "\n",
    "                    train_loss.append(loss_)\n",
    "                    iteration += 1  \n",
    "                    step += 1\n",
    "            saver.save(sess, \"./models/car-lstm/car_predicts.ckpt\")\n",
    "            # 保存二进制模型\n",
    "            builder = tf.saved_model.builder.SavedModelBuilder(\"./models/car/model_\"+ str(i))\n",
    "            builder.add_meta_graph_and_variables(sess, ['mytag'])\n",
    "            builder.save()\n",
    "            #绘训练过程指标图\n",
    "            t = np.arange(iteration - 1)\n",
    "            plt.figure(figsize = (9,6))\n",
    "            plt.plot(t, np.array(train_loss),  'r-')\n",
    "            plt.xlabel(\"iteration\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend(['train'], loc='upper right')\n",
    "            plt.show()        \n",
    "\n",
    "    with tf.variable_scope('rnn', reuse=tf.AUTO_REUSE):\n",
    "        train_lstm()\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import math\n",
    "\n",
    "    def prediction():\n",
    "\n",
    "            pred, _ = lstm(1)  # 预测时只输入[1,time_step,inputSize]的测试数据\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            #预测季度\n",
    "            pre_quarter = 4\n",
    "            with tf.Session() as sess:\n",
    "                # 参数恢复\n",
    "                module_file = tf.train.latest_checkpoint(\"./models/car-lstm/\")\n",
    "                saver.restore(sess, module_file)\n",
    "                # 取训练集最后一行为测试样本. shape=[1,time_step,inputSize]\n",
    "                prev_seq = train_x[-1]\n",
    "                predict = []\n",
    "                print(prev_seq)\n",
    "                # 得到之后10个季度的预测结果\n",
    "                for i in range(pre_quarter):\n",
    "                    next_seq = sess.run(pred,feed_dict={X:[prev_seq]})\n",
    "                    predict.append(next_seq[-1])   \n",
    "                    #每次得到最后一个时间步的预测结果，与之前的数据加在一起，形成新的测试样本\n",
    "                    #np.vstack()表示垂直（按照行顺序）的把数组给堆叠起来。\n",
    "                    prev_seq = np.vstack((prev_seq[1:],next_seq[-1]))\n",
    "                    print(prev_seq)\n",
    "                #得到实际预测值\n",
    "                predictY = scaler.inverse_transform(predict)\n",
    "\n",
    "                testY = scaler.inverse_transform(test)\n",
    "                print(\"预测值：\", predictY)\n",
    "                print(\"真实值：\",testY )\n",
    "\n",
    "                global Matrix_pre\n",
    "                Matrix_pre = (np.hstack((Matrix_pre,predictY)))\n",
    "\n",
    "\n",
    "                #以折线图表示结果\n",
    "                plt.figure()\n",
    "                plt.title(\"lead index\")\n",
    "                plt.plot(list(range(len(testY))), testY, 'cx--', list(range(len(predict))), predictY, 'b--')\n",
    "                plt.xlabel(\"date-num\")\n",
    "                plt.ylabel(\"index\")\n",
    "                plt.legend(['train', 'pred'], loc='upper right')\n",
    "                plt.plot()\n",
    "                plt.show()\n",
    "\n",
    "    with tf.variable_scope('rnn', reuse = tf.AUTO_REUSE):\n",
    "        prediction() \n",
    "\n",
    "# #存进excel文件\n",
    "# Matrix_input = pd.DataFrame(Matrix_pre)\n",
    "# writer = pd.ExcelWriter('D:\\北京市项目\\汽车行业财务风险\\汽车行业财务风险预测数据.xlsx')\n",
    "# Matrix_input.to_excel(writer, float_format='%.2f', header = False, index = False,) # float_format 控制精度\n",
    "# writer.save()\n",
    "\n",
    "print('**************************************************************************\\n')\n",
    "print('*********************************迭代结束*********************************\\n')\n",
    "print('**************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
