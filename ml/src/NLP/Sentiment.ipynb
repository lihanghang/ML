{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lihanghang/anaconda3/envs/TensorFlow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 首先加载必用的库\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba # 结巴分词\n",
    "# gensim用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gensim加载预训练中文分词embedding\n",
    "cn_model = KeyedVectors.load_word2vec_format('./Chinese-Word-Vectors/sgns.zhihu.bigram', \n",
    "                                          binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.keyedvectors.KeyedVectors object at 0x7f39367a07b8>\n"
     ]
    }
   ],
   "source": [
    "# 获得样本的索引，样本存放于两个文件夹中，\n",
    "import os\n",
    "pos_txts = os.listdir('./CompanyNewsData/pos')\n",
    "neg_txts = os.listdir('./CompanyNewsData/neg')\n",
    "print(cn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总共: 16343\n"
     ]
    }
   ],
   "source": [
    "print( '样本总共: '+ str(len(pos_txts) + len(neg_txts)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们将所有的评价内容放置到一个list里\n",
    "\n",
    "train_texts_orig = [] # 存储所有评价，每例评价为一条string\n",
    "\n",
    "# 添加完所有样本之后，train_texts_orig为一个含有4000条文本的list\n",
    "# 其中前7769条文本为正面评价，后7769条为负面评价\n",
    "\n",
    "for i in range(1000):\n",
    "    with open('./CompanyNewsData/pos/'+ pos_txts[i], 'r', errors='ignore') as f:\n",
    "        text = f.read().strip()\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()\n",
    "for i in range(1000):\n",
    "    with open('./CompanyNewsData/neg/' + neg_txts[i], 'r', errors='ignore') as f:\n",
    "        text = f.read().strip()\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行分词和tokenize\n",
    "# train_tokens是一个长长的list，其中含有4000个小list，对应每一条评价\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得所有tokens的长度 \n",
    "num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790.948"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 平均tokens的长度\n",
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13045"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最长的评价tokens的长度\n",
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2480"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens的长度为2465时，大约95%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "np.sum( num_tokens < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse = reverse_tokens(train_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'成为具有中国情怀全球视野的人才不仅能肩负起建设祖国的使命而且能承担 世界为人类作贡献的责任“ 无以 非志无以成学”传授知识是教育的重要功能“要在增长知识见识上下功夫教育引导学生珍惜学习时光心无旁骛求知 增长见识丰富学识沿着求真理悟道理明事理的方向前进”在全国教育大会上习近平总书记的谆谆教导殷殷期许值得为人师者和莘莘学子铭记在心当今时代知识更新的节奏不断加快有学者曾根据放射性元素衰变的原理提出“知识半衰期”说法：一个专业人士的知识如果不更新在“半衰期”后基础知识仍可用但其他一半新知识却已落伍据测算1950年前的知识半衰期为50年21世纪知识的半衰期平均为32年IT高级工程师的仅为18年传统学人“书读完了”的感叹再也不会出现如今的学习没有完成时只有进行时还需保持加速度增长知识见识可以说是一辈子的功夫另一方面教育的普及出版的繁荣互联网的兴盛使得知识获取的成本大大下降“学好数理化”不再成为求知边界通识教育正不断打开知识视野因此若以知识宽度论今天学子可以完胜过去然而广度之外同样需要有深度 反能精读以致“韦编三绝”；诱惑少所以心无旁骛不断钻研将有限的注意力资源用到最有效的地方才会有“铁杵磨成针”的成就因此既要重视知识的宽度也要重视学习的深度在努力扩大知识半径的同时避免陷入平面化的“知识焦虑”落入浅尝辄止的学习路径既有知识的宽度又有知识的深度才能在积累知识的基础上形成卓越的见识当前许多领域深入发展创新的重要性愈发凸显芯片技术如何突破人工智能领域如何推进东西方文化如何更好交融这有赖于“博观而约取厚积而薄发”尽管传授高深知识的使命往往在高等教育但是培养钻研精神却需要从小抓起面对全球化竞争教育者需要引导学生摆脱简单的记诵在 上下功夫；关注知识的积累更注重思维的锤炼换句话说 好的教育既教会学生如何数清掉落的苹果更激发学生思考为何苹果是掉下来而 上天知识传承很重要见识的培育更关键在古汉语中“知”与“识”相连又有别 获取信息是第一层级；识是具备见解是更高层级由 方为智慧一方面正所谓“师父领进门修行在个人”注重把所学知识内化于心形成自己的见解才算得上学懂弄通；而另一方面具备 精的信息筛选力去伪存真的知识鉴别力把握时代大势的洞察力方能在信息爆炸的时代从容不迫从路径上说培养见识离不开课堂但也不能囿于课堂既需要言传更需要鼓励学生在敏于求知勤于学习敢于创新勇于实践的过程中去主动领悟与把握今天谈增长知识与见识不仅指向“上下 ” 围绕“纵横 ”新时代社会主义建设者和接班人不仅要有中国情怀而且要有世界眼光和国际视野回顾历史中国在全球化道路上每进一步都离不开全球视野为先导近代化早期有魏源等人“开眼看世界”有 严复等大规模译介外国经典；改革开放初期有勒紧裤腰带公派留学生更有打开国门搞建设；今天面向更高层次的开放以及建立人类命运共同体的目标我们比任何时候都需要有天下观的英才教育引导学生关注世界形势及其发展变化成为具有中国情怀全球视野的人才不仅能肩负起建设祖国的使命而且能承担 世界为人类作贡献的责任荀子的《劝学》为 名篇其 要义在开篇 ：“ 可以已”学无止境 专攻在增长知识见识上下功夫不断锤炼年轻人干事创业的真本领就一定能培养更多社会主义建设者和接班人为中华民族伟大复兴提供最强大的人才支撑'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 经过tokenize再恢复成文本\n",
    "# 可见标点符号都没有了\n",
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由此可见每一个词都对应一个长度为300的向量\n",
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.017840e-01 -1.653400e-01  3.050800e-02 ...  1.065250e-01\n",
      "   5.534360e-01  4.366500e-01]\n",
      " [-6.517470e-01  5.359700e-01  3.402710e-01 ...  8.053990e-01\n",
      "   1.045930e-01  1.936940e-01]\n",
      " [-4.123210e-01  2.282610e-01  2.071140e-01 ...  8.087770e-01\n",
      "   5.675100e-02  4.523740e-01]\n",
      " ...\n",
      " [ 5.849840e-01  1.121180e-01 -6.938330e-01 ... -3.760570e-01\n",
      "   1.203500e-01 -1.059511e+00]\n",
      " [ 1.511710e-01 -3.200000e-04 -3.885760e-01 ... -5.988550e-01\n",
      "   4.273530e-01 -3.922630e-01]\n",
      " [-4.536090e-01 -1.813600e-02 -1.306600e-01 ... -6.608000e-02\n",
      "   3.566680e-01  3.898050e-01]]\n"
     ]
    }
   ],
   "source": [
    "# 只使用前20000个词\n",
    "num_words = 50000\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum(cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_util\n",
    "# embedding_matrix的维度，\n",
    "# 这个维度为keras的要求，后续会在模型中用到\n",
    "pad_sequences = tf.contrib.keras.preprocessing.sequence.pad_sequences\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行padding和truncating， 输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy array\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超出五万个词向量的词用0代替# 超出五万个词 \n",
    "train_pad[ train_pad>=num_words ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9061,    3, 8986, ...,  384,  148,  981], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 可见padding之后前面的tokens全变成0，文本在最后面# 可见padd \n",
    "train_pad[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备target向量，前2000样本为1，后2000为0# 准备targ \n",
    "train_target = np.concatenate( (np.ones(1000),np.zeros(1000)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...  3267   440     0]\n",
      " [    0     0     0 ...    88   432   743]\n",
      " [    0     0     0 ...  4736 47979  8454]\n",
      " ...\n",
      " [    0     0     0 ...    19  4221 14348]\n",
      " [    0     0     0 ...     0  2933   111]\n",
      " [    0     0     0 ...     0  6133     0]]\n"
     ]
    }
   ],
   "source": [
    "#80%的样本用来训练，剩余20%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=12)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           近日北京市食品药品 组织抽检餐饮食品253批次样品其中合格样品235批次不合格样品18批次 记者了解到不合格原因均为餐具上检出 其中不乏知名餐厅如 餐厅公益桥     等按照规定餐具中不得检出 目前已要求食品经营者所在地食品药品监管部门依法对经营不合格产品的食品经营者进行立案调查此次抽检的不合格样品均为餐厅餐具包括北京 餐厅抽检的 粥碗肉饼盘北京 情大年初一 餐厅抽检的小碗北京公益桥  抽检的油条盘饺子 碟凉菜碟粥碗等不合格项目为   为阳性 规定为不得检出此外还包括北京 峨眉 有限公司抽检的红酒杯北京 百货 公司餐饮分公司抽检的 聚 天控股有限公司北京 清真小吃店抽检的  盘等  为 虽不具有 但反映该食品生产经营卫生  是国内外通用的食品污染常用 之一食品中检出 提示被 如    污染的可能性较大其超标可能由于产品的加工原料 受污染或在生产过程中产品 员工器具等生产设备环境的污染或有灭菌工艺的产品灭菌不彻底而导致针对在食品安全监督抽检中发现的不合格产品  已要求食品经营者所在地食品药品监管部门依法对经营不合格产品的食品经营者进行立案调查市食品药品 提醒市民发现食品安全违法行为   进行投诉或举报马上 涉事    针对公益桥  被曝多种餐具检出    方面昨天表示已经将  并向广大市民致歉在此次 部门对北京公益桥  抽检中发现油条盘饺子 碟凉菜碟粥碗均为 不合格对此 总部 了全体连锁企业大会除了责令北京公益桥  立即 ；解除加盟资格同时 总部将进一步展开对全体 连锁店铺的 工作检查行动具体包括开展对 连锁店铺 工作的专项检查专项培训 总部将进一步完善  体系结合 总部内部实验室完成对食品日常的抽检工作；同时 总部将与中国检验认证集团检验有限公司展开深度合作委托其协助完成 连锁店铺 的日常抽样监测工作“ 之前主要是把控原材料的食品安全 事后我们会把餐具的卫生检测也纳入到 体系中”上述负责人说与 小吃店同样  天 管理的   此次也位列 相关负责人表示目前  在本市除了3家 包括 店在内共有15家 “虽然是 但也  天管理餐具被查出 的事我们 ”该负责人表示已经对15家 再次进行食品安全教育并立即对15家 进行 安全工作专项检查 此类事情再度出现“公司对 安全问题‘零容忍’今后一旦出现类似事件将停止其加盟资格”记者张小妹 本文关键词：北京 餐厅餐具检出 部分涉事餐厅已 \n",
      "class:  1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 查看训练样本，确认无误# 查看训练样本 \n",
    "print(reverse_tokens(X_test[0]))\n",
    "print('class: ',y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于CNN进行情感分析\n",
    "\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# 清空图\n",
    "tf.reset_default_graph()\n",
    "# 定义了3种filter，每种100个\n",
    "filters_size = [2, 3, 4]\n",
    "num_filters = 100\n",
    "# 超参数\n",
    "BATCH_SIZE = 128\n",
    "EPOCHES = 50\n",
    "LEARNING_RATE = 0.001\n",
    "L2_LAMBDA = 10\n",
    "KEEP_PROB = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test \n",
    "def get_batch(x, y, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    assert x.shape[0] == y.shape[0], print(\"error shape!\")\n",
    "    # shuffle\n",
    "    if shuffle:\n",
    "        shuffled_index = np.random.permutation(range(x.shape[0]))\n",
    "\n",
    "        x = x[shuffled_index]\n",
    "        y = y[shuffled_index]\n",
    "    \n",
    "    # 统计共几个完整的batch\n",
    "    n_batches = int(x.shape[0] / batch_size)\n",
    "    \n",
    "    for i in range(n_batches - 1):\n",
    "        x_batch = x[i*batch_size: (i+1)*batch_size]\n",
    "        y_batch = y[i*batch_size: (i+1)*batch_size]\n",
    "    \n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.017840e-01 -1.653400e-01  3.050800e-02 ...  1.065250e-01\n",
      "   5.534360e-01  4.366500e-01]\n",
      " [-6.517470e-01  5.359700e-01  3.402710e-01 ...  8.053990e-01\n",
      "   1.045930e-01  1.936940e-01]\n",
      " [-4.123210e-01  2.282610e-01  2.071140e-01 ...  8.087770e-01\n",
      "   5.675100e-02  4.523740e-01]\n",
      " ...\n",
      " [ 5.849840e-01  1.121180e-01 -6.938330e-01 ... -3.760570e-01\n",
      "   1.203500e-01 -1.059511e+00]\n",
      " [ 1.511710e-01 -3.200000e-04 -3.885760e-01 ... -5.988550e-01\n",
      "   4.273530e-01 -3.922630e-01]\n",
      " [-4.536090e-01 -1.813600e-02 -1.306600e-01 ... -6.608000e-02\n",
      "   3.566680e-01  3.898050e-01]]\n"
     ]
    }
   ],
   "source": [
    "static_embeddings = embedding_matrix\n",
    "print(static_embeddings)\n",
    "EMBEDDING_SIZE = embedding_dim\n",
    "# 句子最大长度\n",
    "SENTENCE_LIMIT_SIZE = max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"cnn/placeholders/inputs:0\", shape=(?, 2480), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"cnn\"):\n",
    "        with tf.name_scope(\"placeholders\"):\n",
    "            inputs = tf.placeholder(dtype=tf.int32, shape=(None, max_tokens), name=\"inputs\")\n",
    "            targets = tf.placeholder(dtype=tf.int64, shape=[None], name=\"targets\")\n",
    "            y_one_hot = tf.one_hot( targets , 1 ) # 正负分类\n",
    "            print(inputs)\n",
    "        # embeddings\n",
    "        with tf.name_scope(\"embeddings\"):\n",
    "            # embedding_matrixs = tf.Variable(initial_value=static_embeddings, trainable=False, name=\"embedding_matrixs\")\n",
    "            embed = tf.nn.embedding_lookup(embedding_matrix+1, inputs, name=\"embed\")\n",
    "            # 添加channel维度\n",
    "            embed_expanded = tf.expand_dims(embed, -1, name=\"embed_expand\")\n",
    "\n",
    "        # 用来存储max-pooling的结果\n",
    "        pooled_outputs = []\n",
    "\n",
    "        # 迭代多个filter\n",
    "        for i, filter_size in enumerate(filters_size):\n",
    "            with tf.name_scope(\"conv_maxpool_%s\" % filter_size):\n",
    "                filter_shape = [filter_size, EMBEDDING_SIZE, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, mean=0.0, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.zeros(num_filters), name=\"b\")\n",
    "\n",
    "                conv = tf.nn.conv2d(input=embed_expanded, \n",
    "                                     filter=W, \n",
    "                                     strides=[1, 1, 1, 1], \n",
    "                                     padding=\"VALID\",\n",
    "                                     name=\"conv\")\n",
    "\n",
    "                # 激活\n",
    "                a = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"activations\")\n",
    "                # 池化\n",
    "                max_pooling = tf.nn.max_pool(value=a, \n",
    "                                        ksize=[1, SENTENCE_LIMIT_SIZE - filter_size + 1, 1, 1],\n",
    "                                        strides=[1, 1, 1, 1],\n",
    "                                        padding=\"VALID\",\n",
    "                                        name=\"max_pooling\")\n",
    "                pooled_outputs.append(max_pooling)\n",
    "\n",
    "        # 统计所有的filter\n",
    "        total_filters = num_filters * len(filters_size)\n",
    "        total_pool = tf.concat(pooled_outputs, 3)\n",
    "        flattend_pool = tf.reshape(total_pool, (-1, total_filters))\n",
    "\n",
    "\n",
    "        # dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            dropout = tf.nn.dropout(flattend_pool, KEEP_PROB)\n",
    "\n",
    "        # output\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\"W\", shape=(total_filters, 1), initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.zeros(1), name=\"b\")\n",
    "\n",
    "            logits = tf.add(tf.matmul(dropout, W), b)\n",
    "            # sigmoid\n",
    "            predictions = tf.nn.sigmoid(logits, name=\"predictions\")\n",
    "\n",
    "        # loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_one_hot, logits=logits))\n",
    "            loss = loss + L2_LAMBDA * tf.nn.l2_loss(W)\n",
    "            optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "        # evaluation\n",
    "        with tf.name_scope(\"evaluation\"):\n",
    "            # tf.greater 功能：通过比较a、b两个值的大小来输出对错。大于0.5即输出为负向文本\n",
    "            correct_preds = tf.equal(tf.cast(tf.greater(predictions, 0.5), tf.float32), y_one_hot)\n",
    "            accuracy = tf.reduce_sum(tf.reduce_sum(tf.cast(correct_preds, tf.float32), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储准确率\n",
    "cnn_train_accuracy = []\n",
    "cnn_test_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1, Training loss: 12.0919, Train accuracy: 0.5169, Test accuracy: 0.5550\n",
      "Training epoch: 2, Training loss: 9.3987, Train accuracy: 0.6000, Test accuracy: 0.5850\n",
      "Training epoch: 3, Training loss: 7.6793, Train accuracy: 0.6400, Test accuracy: 0.6725\n",
      "Training epoch: 4, Training loss: 6.4196, Train accuracy: 0.7269, Test accuracy: 0.6650\n",
      "Training epoch: 5, Training loss: 5.3980, Train accuracy: 0.7556, Test accuracy: 0.7250\n",
      "Training epoch: 6, Training loss: 4.5237, Train accuracy: 0.7937, Test accuracy: 0.7550\n",
      "Training epoch: 7, Training loss: 3.7995, Train accuracy: 0.8319, Test accuracy: 0.8300\n",
      "Training epoch: 8, Training loss: 3.1824, Train accuracy: 0.8431, Test accuracy: 0.8225\n",
      "Training epoch: 9, Training loss: 2.6794, Train accuracy: 0.8638, Test accuracy: 0.8325\n",
      "Training epoch: 10, Training loss: 2.2321, Train accuracy: 0.8869, Test accuracy: 0.8700\n",
      "Training epoch: 11, Training loss: 1.8676, Train accuracy: 0.8844, Test accuracy: 0.8700\n",
      "Training epoch: 12, Training loss: 1.5882, Train accuracy: 0.8875, Test accuracy: 0.8925\n",
      "Training epoch: 13, Training loss: 1.3513, Train accuracy: 0.8931, Test accuracy: 0.8900\n",
      "Training epoch: 14, Training loss: 1.1456, Train accuracy: 0.9056, Test accuracy: 0.8925\n",
      "Training epoch: 15, Training loss: 0.9850, Train accuracy: 0.9125, Test accuracy: 0.9050\n",
      "Training epoch: 16, Training loss: 0.8543, Train accuracy: 0.8850, Test accuracy: 0.8925\n",
      "Training epoch: 17, Training loss: 0.7510, Train accuracy: 0.8975, Test accuracy: 0.8950\n",
      "Training epoch: 18, Training loss: 0.6580, Train accuracy: 0.9006, Test accuracy: 0.8950\n",
      "Training epoch: 19, Training loss: 0.5938, Train accuracy: 0.9006, Test accuracy: 0.8925\n",
      "Training epoch: 20, Training loss: 0.5408, Train accuracy: 0.8944, Test accuracy: 0.8850\n",
      "Training epoch: 21, Training loss: 0.4961, Train accuracy: 0.8919, Test accuracy: 0.9000\n",
      "Training epoch: 22, Training loss: 0.4529, Train accuracy: 0.8912, Test accuracy: 0.8975\n",
      "Training epoch: 23, Training loss: 0.4316, Train accuracy: 0.8825, Test accuracy: 0.8775\n",
      "Training epoch: 24, Training loss: 0.4118, Train accuracy: 0.8912, Test accuracy: 0.8975\n",
      "Training epoch: 25, Training loss: 0.3861, Train accuracy: 0.8888, Test accuracy: 0.8850\n",
      "Training epoch: 26, Training loss: 0.3721, Train accuracy: 0.8831, Test accuracy: 0.8750\n",
      "Training epoch: 27, Training loss: 0.3549, Train accuracy: 0.8962, Test accuracy: 0.8925\n",
      "Training epoch: 28, Training loss: 0.3469, Train accuracy: 0.8975, Test accuracy: 0.8875\n",
      "Training epoch: 29, Training loss: 0.3371, Train accuracy: 0.8856, Test accuracy: 0.8775\n",
      "Training epoch: 30, Training loss: 0.3277, Train accuracy: 0.8981, Test accuracy: 0.8875\n",
      "Training epoch: 31, Training loss: 0.3195, Train accuracy: 0.8912, Test accuracy: 0.8850\n",
      "Training epoch: 32, Training loss: 0.3135, Train accuracy: 0.8888, Test accuracy: 0.8800\n",
      "Training epoch: 33, Training loss: 0.3056, Train accuracy: 0.8862, Test accuracy: 0.8825\n",
      "Training epoch: 34, Training loss: 0.3065, Train accuracy: 0.9087, Test accuracy: 0.8975\n",
      "Training epoch: 35, Training loss: 0.3097, Train accuracy: 0.8975, Test accuracy: 0.8825\n",
      "Training epoch: 36, Training loss: 0.2975, Train accuracy: 0.8906, Test accuracy: 0.8950\n",
      "Training epoch: 37, Training loss: 0.2968, Train accuracy: 0.9031, Test accuracy: 0.9025\n",
      "Training epoch: 38, Training loss: 0.2947, Train accuracy: 0.9031, Test accuracy: 0.8950\n",
      "Training epoch: 39, Training loss: 0.2850, Train accuracy: 0.9031, Test accuracy: 0.8900\n",
      "Training epoch: 40, Training loss: 0.2907, Train accuracy: 0.9231, Test accuracy: 0.9025\n",
      "Training epoch: 41, Training loss: 0.2810, Train accuracy: 0.9119, Test accuracy: 0.9050\n",
      "Training epoch: 42, Training loss: 0.2788, Train accuracy: 0.9062, Test accuracy: 0.8950\n",
      "Training epoch: 43, Training loss: 0.2718, Train accuracy: 0.9113, Test accuracy: 0.9100\n",
      "Training epoch: 44, Training loss: 0.2676, Train accuracy: 0.9187, Test accuracy: 0.9100\n",
      "Training epoch: 45, Training loss: 0.2700, Train accuracy: 0.9012, Test accuracy: 0.8925\n",
      "Training epoch: 46, Training loss: 0.2648, Train accuracy: 0.9187, Test accuracy: 0.9175\n",
      "Training epoch: 47, Training loss: 0.2725, Train accuracy: 0.9169, Test accuracy: 0.9125\n",
      "Training epoch: 48, Training loss: 0.2648, Train accuracy: 0.9150, Test accuracy: 0.9075\n",
      "Training epoch: 49, Training loss: 0.2623, Train accuracy: 0.9150, Test accuracy: 0.9275\n",
      "Training epoch: 50, Training loss: 0.2532, Train accuracy: 0.9275, Test accuracy: 0.9250\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "Converted 8 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    writer = tf.summary.FileWriter(\"./graphs/cnn\", tf.get_default_graph())\n",
    "    n_batches = int(X_train.shape[0] / BATCH_SIZE)   \n",
    "    for epoch in range(EPOCHES):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in get_batch(X_train, y_train):\n",
    "            _, l = sess.run([optimizer, loss],\n",
    "                            feed_dict={inputs: x_batch, \n",
    "                                       targets: y_batch})\n",
    "            total_loss += l\n",
    "        \n",
    "        train_corrects = sess.run(accuracy, feed_dict={inputs: X_train, targets: y_train})\n",
    "        train_acc = train_corrects / X_train.shape[0]\n",
    "        cnn_train_accuracy.append(train_acc)\n",
    "        \n",
    "        test_corrects = sess.run(accuracy, feed_dict={inputs: X_test, targets: y_test})\n",
    "        test_acc = test_corrects / X_test.shape[0]\n",
    "        cnn_test_accuracy.append(test_acc)\n",
    "        \n",
    "        print(\"Training epoch: {}, Training loss: {:.4f}, Train accuracy: {:.4f}, Test accuracy: {:.4f}\".format(epoch + 1, \n",
    "                                                                                                                total_loss / n_batches,\n",
    "                                                                                                                train_acc,\n",
    "                                                                                                               test_acc))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/cnn\")\n",
    "    # 保存二进制模型\n",
    "    const_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['cnn/output/predictions'])\n",
    "    #output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, output_node_names=['cnn/output/predictions'])\n",
    "    with tf.gfile.FastGFile(r'Text_Sentiment.pb', mode='wb') as f:\n",
    "        f.write(const_graph.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3912fb6208>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f391308edd8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnn_train_accuracy)\n",
    "plt.plot(cnn_test_accuracy)\n",
    "plt.ylim(ymin=0.5, ymax=1.01)\n",
    "plt.title(\"The accuracy of CNN model\") \n",
    "plt.legend([\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/cnn\n",
      "判断正确的文本数量：370.0\n",
      "测试文本总量：400\n",
      "The textCNN model accuracy on test set: 92.50%\n",
      "2480\n"
     ]
    }
   ],
   "source": [
    "# 在test上的准确率\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"checkpoints/cnn\")\n",
    "    feed = {inputs: X_test, targets: y_test}\n",
    "    total_correct = sess.run(accuracy,\n",
    "                             feed_dict=feed)\n",
    "    print(\"判断正确的文本数量：\"+ str(total_correct))\n",
    "    print(\"测试文本总量：\"+ str(X_test.shape[0]))\n",
    "    print(\"The textCNN model accuracy on test set: {:.2f}%\".format(100 * total_correct / X_test.shape[0]))\n",
    "    preds = sess.run(predictions, feed_dict={inputs: X_test})\n",
    "    print(len(X_test[0]))\n",
    "#     for i in range(len(preds)):\n",
    "#         res = preds[i][0]\n",
    "#         # print(reverse_tokens(X_test[i]))\n",
    "#         print('================================原始文本class==========================================: ',y_test[i])\n",
    "#         if(res>= 0.5):\n",
    "#             print('===========================是一例负面新闻==================','output=%.2f'%res)\n",
    "#         else:           \n",
    "#             print('===========================是一例正面新闻==================','output=%.2f'%res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    # print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\", text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 超出五万个词向量的词用0代替# 超出五万个词 \n",
    "    tokens_pad[ tokens_pad>=num_words ] = 0\n",
    "    #print(tokens_pad)\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph('checkpoints/cnn.meta')\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(\"checkpoints/\"))\n",
    "        result = sess.run(predictions,feed_dict={inputs: tokens_pad})\n",
    "        # 预测\n",
    "        # result = model.predict(x=tokens_pad)\n",
    "        coef = result[0][0]\n",
    "        print(coef)\n",
    "        if coef >= 0.5:\n",
    "            print('===========================是一例负面新闻==================','output=%.2f'%coef)\n",
    "        else:\n",
    "            print('===========================是一例正面新闻==================','output=%.2f'%coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot add op with name cnn/conv_maxpool_2/W/Adam as that name is already used",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-fa816712d227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m '''\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mpredict_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-b744e235ab04>\u001b[0m in \u001b[0;36mpredict_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print(tokens_pad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints/cnn.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpoints/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokens_pad\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1696\u001b[0m                                       \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m                                       \u001b[0mimport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1698\u001b[0;31m                                       **kwargs)\n\u001b[0m\u001b[1;32m   1699\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saver_def\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1700\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimport_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mimport_scoped_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate)\u001b[0m\n\u001b[1;32m    654\u001b[0m     importer.import_graph_def(\n\u001b[1;32m    655\u001b[0m         \u001b[0minput_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimport_scope\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m         producer_op_list=producer_op_list)\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     scope_to_prepend_to_names = \"/\".join(\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    311\u001b[0m           \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m           \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;31m# Maps from a node to the ops it is colocated with, if colocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2631\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2633\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_add_op\u001b[0;34m(self, op)\u001b[0m\n\u001b[1;32m   2310\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2311\u001b[0m         raise ValueError(\"cannot add op with name %s as that name \"\n\u001b[0;32m-> 2312\u001b[0;31m                          \"is already used\" % op.name)\n\u001b[0m\u001b[1;32m   2313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot add op with name cnn/conv_maxpool_2/W/Adam as that name is already used"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "\n",
    "　　咋维权自如公寓甲醛房事件近日热传\n",
    "　　租到甲醛房咋进行索赔\n",
    "　　甲醛超标和致病因果关系认定是难点也是关键点租房时可要求把甲醛检测写进合同\n",
    "　　近日，“自如甲醛房”刷爆网络，成为热点话题。\n",
    "　　入职阿里巴巴杭州总部，半年多后因白血病去世，“阿里P7员工租住自如公寓，得白血病病故”被热传。家人检测了其租住的自如公寓，结果显示甲醛超标。家人为了留存证据，缴付了9月的房租，并将公寓用封条贴住，准备提起法律诉讼。\n",
    "　　实际上，这不是长租公寓平台自如第一次深陷甲醛风波。华商报记者在网上检索“自如甲醛房”，出现1，450，000条相关内容。有多位租客爆料自如等中介收房后快速装修出租，因装修通风期太短、材料等原因导致甲醛超标，租客出现明显病症，身体健康受损。\n",
    "　　华商报记者检索发现，近三年涉及甲醛超标患癌的案例非常多，有涉及办公室的、有公寓楼的，有租住房的，有私家装修的，也有学校教室的，引发多起法律纠纷，甚至出现过群体性事件。\n",
    "　　自如被指操纵甲醛检测结果\n",
    "　　8月30日，自如相关人士回应甲醛超标问题时表示，每套房屋的装修配置不同，通风时长不同，以及季节性气温变化等原因，都会引发甲醛含量变化，并称自如将“从根本上改进”，并在“2018年升级全线产品品质，对标国际标准。”\n",
    "　　话音未落，9月1日晚，有租户向媒体爆料，自如申请的第三方空气检测结果存在被人为操纵的情形，他曾当场拆穿自如甲醛检测人员对数据做手脚。这位租户表示，第一次检查时检测出甲醛含量0.33mg，国家标准是0.1mg，检测结果为不合格。然后对方就给租住房屋做了“治理”。6天后，第二次检测报告显示，甲醛含量0.03mg，显示为合格。但他发现，现场检测人员干预了检测结果。自如工作人员事后告知他，进行检测的公司“没有认证”。在他的要求下，自如方面另找了一家检测机构，但第三次检测结果显示，甲醛依然超标。最后自如和他签了和解协议，给他赔偿4000元，因担心甲醛危及健康，他最终选择了退租。\n",
    "　　甲醛超标与致病因果关系难认定\n",
    "　　西安从事房屋装修的业内人士接受华商报记者采访表示，无论是装修的新房还是租住房，完工后要通风至少3个月。如果再购置家具，晾晒通风的时间则更长。在入住前，都要请专业公司检测甲醛，检测数据可以人为操控。他了解到有客户不放心，要请不同的公司多次检测，“如果是租住公寓，就像媒体报道的自如公寓‘从装修到出租，15天搞定’，为避免类似纠纷，可以要求签合同前检测甲醛。”\n",
    "　　日前，有媒体以“甲醛+健康权”为关键词，在中国裁判文书网上检索到63份裁判文书，其中10起涉及居住、办公场所因有害物质超标起诉装修、出租方的判例。在这些判例中，原告全部败诉。\n",
    "　　陕西海普睿诚律师事务所马秉晨律师接受华商报记者采访表示，这类官司之所以难打，因为租住房甲醛超标与租客致病之间的因果关系难认定。有时，原告、被告请专业机构检测的结果不一致。一旦起诉到法院，法院也会委托第三方机构进行检测，但要证明甲醛超标是唯一的致病原因，往往难度很大。\n",
    "　　>>律师说法\n",
    "　　可明确要求测甲醛写进合同\n",
    "　　消费者租住甲醛房，健康受损如何维权？出租房甲醛超标，官司为啥难打？法律学者杨晨作了相关解读。\n",
    "　　房源检测合格后再出租\n",
    "　　华商报：类似“自如甲醛房”的官司难打在哪？\n",
    "　　杨晨：法院判定侵权责任赔偿，甲醛超标和租客身体致病之间的因果关系认定是关键，也是难点。在我国的装修污染中，苯系物与甲醛超标最为广泛。但从科学上看，我们还不能断言“甲醛导致白血病”。世卫组织下属的国际癌症研究局发布的“将甲醛分类为人类致癌物”的新闻公报，来自10个国家的26位科学家综合评估了甲醛致癌性，并得出了核心结论，即甲醛是人类致癌物，可以引起人类鼻咽癌，但甲醛致白血病的证据还不够充分。不过，也有人类流行病学相关研究结果支持“甲醛导致白血病”。\n",
    "　　华商报：能否以此事件推动国内长租房中介，对房源先行检测合格后再对外出租？\n",
    "　　杨晨：我认为，将所有房源检测合格后再对外出租，这是国内长租房中介公司应尽的商业责任，也是法律责任。因为《消费者权益保护法》规定，消费者有权要求经营者提供的商品和服务，符合保障人身、财产安全的要求。经媒体报道后，甲醛出租房问题引起公众广泛关注，这是一个很好的解决契机，加强房源检测环节，有利于消费者权益保障，也有助于国内长租房市场健康发展。\n",
    "　　出租房甲醛超标谁来监管\n",
    "　　华商报：签订租房合同时，是否可以明确要求把甲醛检测等内容写进合同中？\n",
    "　　杨晨：根据《室内空气质量标准》、《居室空气中甲醛的卫生标准》规定，室内甲醛应低于0.1毫克每立方米的标准，但没有法律法规和标准明确禁止出租甲醛超标的住房。但我国《合同法》规定，“合同的内容由当事人约定”，“质量”也是合同条款的重要内容。在签订租房合同时，可以明确要求把甲醛检测等空气质量内容写进合同中。立法应当明确规定，甲醛不超标是出租房屋安全性的基本标准之一，严禁甲醛超标房流入租房市场。\n",
    "　　华商报：中介公司装修出租房屋，甲醛超标等问题谁来监管？\n",
    "　　杨晨：为了节约成本，有中介公司能省就省，出租房的装修材料容易导致甲醛超标。住建部门不仅承担“规范房地产市场秩序、监督管理房地产市场的责任”，还承担“建筑工程质量安全监管的责任”。中介装修房屋甲醛超标，应当由住建部门来监管。就装修材料的质量问题，应由质检部门来监管。根据《产品质量法》，对生产和故意销售不符合保障人体健康，人身、财产安全的国家标准、行业标准的产品的，“责令停止生产”，“没收违法销售的产品和违法所得，并处违法所得1倍以上5倍以下的罚款，可以吊销营业执照；构成犯罪的，依法追究刑事责任”。\n",
    "　　检测人员做手脚应受行政处罚\n",
    "　　华商报：中介公司的检测报告能信吗？如果检测数据造假，谁来监管？\n",
    "　　杨晨：中介公司是利益关系者，由其出具的检测报告，需要打一个问号，应当由第三方检测机构出具权威报告。根据《产品质量法》第57条，“产品质量检验机构、认证机构伪造检验结果或者出具虚假证明的”，“责令改正，对单位处5万元以上10万元以下的罚款，对直接负责的主管人员和其他直接责任人员处1万元以上5万元以下的罚款”，“有违法所得的，并处没收违法所得”，“情节严重的，取消其检验资格、认证资格”，“构成犯罪的，依法追究刑事责任”。“产品质量检验机构、认证机构出具的检验结果或者证明不实，造成损失的，应当承担相应的赔偿责任”，“造成重大损失的，撤销其检验资格、认证资格”。出现检测数据造假问题，比如甲醛检测人员对数据做手脚，应当受到行政处罚。\n",
    "　　华商报：租住房屋甲醛超标怎样维权？\n",
    "　　杨晨：各自的检测结论不一致，应当由具有认证资格的第三方检测机构，按照规定的流程进行检测。出现租住房屋甲醛超标现象，消费者首先要搜集证据，拿到权威性的检测报告，对自身健康造成损害的医学证明等。在此基础上，一是可以解除合同，二是可以提起民事赔偿诉讼，也可以双方协商赔偿。\n",
    "　　华商报：对承租人健康造成损害的是否可以向房东或中介公司索赔医药费？\n",
    "　　杨晨：从法律上看，承租人可以提出侵权索赔。我国《侵权责任法》明确规定，“因产品存在缺陷造成他人损害的，生产者应当承担侵权责任”“因产品存在缺陷造成损害的，被侵权人可以向产品的生产者请求赔偿，也可以向产品的销售者请求赔偿”。对承租人健康造成损害的，可以向房东或托管中介索赔医药费。不过在司法实践中，维权有一定难度。虽然有少数胜诉例子，但一般情况下，法院认为白血病的病因尚不完全清楚，不少原告诉讼请求都被驳回。\n",
    "　　>>以案说法\n",
    "　　签约后发现甲醛超标\n",
    "　　租房者告赢中介公司\n",
    "　　2015年9月，方某与北京某房屋租赁中介公司签订租房合同，随后发现租住屋甲醛超标。当年10月，方某联系该公司工作人员刘某某，以空气状况存在问题为由要求解除合同，刘某某让方某对房屋空气质量自行进行检测。但中介公司未及时有效回应方某解除合同的要求。\n",
    "　　此后，方某聘请鉴定机构对空气质量进行了鉴定，在收到甲醛超标的鉴定结论后，方某以租住屋甲醛超标损害健康权为由，将该中介公司起诉到法院，要求判令解除租房合同，不承担租金。海淀区法院和北京市一中院，分别作出一审和二审民事判决，判决原告方某胜诉。中介公司不服判决，上诉至北京市高院申请再审。北京市高院审查认为，一、二审法院认定该中介公司对方某委托检测机构出具的检测结论认可，并无不当。2017年8月，北京市高院驳回该中介公司的再审申请，维持原判。华商报记者燕然\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "责任编辑：陈永乐\n",
    "\n",
    "\n",
    "'''\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 模型测试\n",
    "import os\n",
    "testDir = \"./CompanyNewsData/test/\"\n",
    "list = os.listdir(testDir) #列出文件夹下所有的目录与文件\n",
    "for i in range(0,len(list)):\n",
    "    print(list[i])\n",
    "    file = open(testDir+list[i], 'r')\n",
    "    # 读取文件内容\n",
    "    content = file.read()\n",
    "    # 将内容送入模型\n",
    "    predict_sentiment(content)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9542429e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "text = '''据民警介绍，非法团伙与受害学生签订的借款合同中，存在很多的收费条款，\n",
    "并且受害学生除了签订意外，根本再无接触合同的可能，合同也仅此一份，签完合同就会被他们拿走\n",
    "，等到期受害学生还不上时，就以各种条款收取费用，他们虽不会采用身体上的暴力，\n",
    "但 通过威胁、恐吓、聚众造势等对他们的心里造成极大的压力和恐惧心理，甚至会前往学生的\n",
    "老家，在家门口泼油漆、写红字等恶劣的方式比他们想办法还钱！'''\n",
    "text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\", text)\n",
    "# 分词\n",
    "cut = jieba.cut(text)\n",
    "cut_list = [ i for i in cut ]\n",
    "# tokenize\n",
    "for i, word in enumerate(cut_list):\n",
    "    try:\n",
    "        cut_list[i] = cn_model.vocab[word].index\n",
    "    except KeyError:\n",
    "        cut_list[i] = 0\n",
    "# padding\n",
    "tokens_pad = pad_sequences([cut_list], maxlen=2480,\n",
    "                       padding='pre', truncating='pre')\n",
    "# 超出五万个词向量的词用0代替# 超出五万个词 \n",
    "tokens_pad[ tokens_pad>=num_words ] = 0 \n",
    " \n",
    "\n",
    "\n",
    "# 输入\n",
    "with tf.gfile.FastGFile('Text_Sentiment.pb','rb') as f:\n",
    "    # 复制定义好的计算图到新的图中，先创建一个空的图.\n",
    "    graph_def = tf.GraphDef()\n",
    "    # 加载proto-buf中的模型\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    # 最后复制pre-def图的到默认图中.\n",
    "    \n",
    "    _ = tf.import_graph_def(graph_def, name='')    \n",
    "with tf.Session() as sess:\n",
    "    # 初始化所有变量\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    input_x = sess.graph.get_tensor_by_name('cnn/placeholders/inputs:0')\n",
    "    op = sess.graph.get_tensor_by_name('cnn/output/predictions:0')\n",
    "    ret = sess.run(op, feed_dict={input_x: tokens_pad})\n",
    "    finallRes = ret[0][0]\n",
    "    print(finallRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-90-c832f5207257>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-90-c832f5207257>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    print sess.run('w:0')\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def load_model():\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph('checkpoints/cnn.meta')\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(\"model/\"))\n",
    "        print sess.run('w:0')\n",
    "        print sess.run('b:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
