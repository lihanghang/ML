{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-91d7f666a0c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutilities\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import os\n",
    "from utils.utilities import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, labels_train, list_ch_train = read_data(data_path=\"datas\", split=\"train\") # train\n",
    "X_test, labels_test, list_ch_test = read_data(data_path=\"datas\", split=\"test\") # test\n",
    "assert list_ch_train == list_ch_test, \"Mistmatch in channels!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "X_train, X_test = standardize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train, \n",
    "                                                stratify = labels_train,\n",
    "                                                random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_tr = one_hot(lab_tr)\n",
    "y_vld = one_hot(lab_vld)\n",
    "y_test = one_hot(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "\n",
    "lstm_size = 6        # 3 times the amount of channels\n",
    "lstm_layers = 2        # Number of layers\n",
    "batch_size = 100       # Batch size \n",
    "seq_len = 6          # Number of steps\n",
    "learning_rate = 0.0001  # Learning rate (default is 0.001)\n",
    "epochs = 2000\n",
    "\n",
    "# Fixed\n",
    "n_classes = 5\n",
    "n_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "# Construct placeholders\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
    "    labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    # Construct the LSTM inputs and LSTM cells\n",
    "    lstm_in = tf.transpose(inputs_, [1,0,2]) # reshape into (seq_len, N, channels)\n",
    "    lstm_in = tf.reshape(lstm_in, [-1, n_channels]) # Now (seq_len*N, n_channels)\n",
    "    \n",
    "    # To cells\n",
    "    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None) # or tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh?\n",
    "    print(lstm_in)\n",
    "    # Open up the tensor into a list of seq_len pieces\n",
    "    lstm_in = tf.split(lstm_in, seq_len, 0)\n",
    "    \n",
    "    # Add LSTM layers\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"logits/BiasAdd:0\", shape=(100, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32,\n",
    "                                                     initial_state = initial_state)\n",
    "    \n",
    "    # We only need the last output tensor to pass into a classifier\n",
    "    logits = tf.layers.dense(outputs[-1], n_classes, name='logits')\n",
    "    print(logits)\n",
    "    # Cost function and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost) # No grad clipping\n",
    "    \n",
    "    # Grad clipping\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate_)\n",
    "\n",
    "    gradients = train_op.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    optimizer = train_op.apply_gradients(capped_gradients)\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists(r'D:\\py_projects\\ML\\ML_Demo\\financial\\checkpoints') == False):\n",
    "    !mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2000 Iteration: 5 Train loss: 1.609823 Train acc: 0.260000\n",
      "Epoch: 3/2000 Iteration: 10 Train loss: 1.606755 Train acc: 0.330000\n",
      "Epoch: 4/2000 Iteration: 15 Train loss: 1.612811 Train acc: 0.270000\n",
      "Epoch: 6/2000 Iteration: 20 Train loss: 1.606123 Train acc: 0.370000\n",
      "Epoch: 8/2000 Iteration: 25 Train loss: 1.607410 Train acc: 0.390000\n",
      "Epoch: 8/2000 Iteration: 25 Validation loss: 1.607290 Validation acc: 0.470000\n",
      "Epoch: 9/2000 Iteration: 30 Train loss: 1.607510 Train acc: 0.250000\n",
      "Epoch: 11/2000 Iteration: 35 Train loss: 1.605771 Train acc: 0.330000\n",
      "Epoch: 13/2000 Iteration: 40 Train loss: 1.604806 Train acc: 0.370000\n",
      "Epoch: 14/2000 Iteration: 45 Train loss: 1.602869 Train acc: 0.310000\n",
      "Epoch: 16/2000 Iteration: 50 Train loss: 1.609887 Train acc: 0.310000\n",
      "Epoch: 16/2000 Iteration: 50 Validation loss: 1.604394 Validation acc: 0.490000\n",
      "Epoch: 18/2000 Iteration: 55 Train loss: 1.605551 Train acc: 0.430000\n",
      "Epoch: 19/2000 Iteration: 60 Train loss: 1.604468 Train acc: 0.300000\n",
      "Epoch: 21/2000 Iteration: 65 Train loss: 1.605136 Train acc: 0.310000\n",
      "Epoch: 23/2000 Iteration: 70 Train loss: 1.605838 Train acc: 0.400000\n",
      "Epoch: 24/2000 Iteration: 75 Train loss: 1.600333 Train acc: 0.380000\n",
      "Epoch: 24/2000 Iteration: 75 Validation loss: 1.601431 Validation acc: 0.480000\n",
      "Epoch: 26/2000 Iteration: 80 Train loss: 1.601890 Train acc: 0.350000\n",
      "Epoch: 28/2000 Iteration: 85 Train loss: 1.596001 Train acc: 0.450000\n",
      "Epoch: 29/2000 Iteration: 90 Train loss: 1.596811 Train acc: 0.340000\n",
      "Epoch: 31/2000 Iteration: 95 Train loss: 1.600507 Train acc: 0.350000\n",
      "Epoch: 33/2000 Iteration: 100 Train loss: 1.596144 Train acc: 0.480000\n",
      "Epoch: 33/2000 Iteration: 100 Validation loss: 1.598364 Validation acc: 0.490000\n",
      "Epoch: 34/2000 Iteration: 105 Train loss: 1.593084 Train acc: 0.350000\n",
      "Epoch: 36/2000 Iteration: 110 Train loss: 1.598359 Train acc: 0.350000\n",
      "Epoch: 38/2000 Iteration: 115 Train loss: 1.595886 Train acc: 0.440000\n",
      "Epoch: 39/2000 Iteration: 120 Train loss: 1.593477 Train acc: 0.400000\n",
      "Epoch: 41/2000 Iteration: 125 Train loss: 1.597374 Train acc: 0.400000\n",
      "Epoch: 41/2000 Iteration: 125 Validation loss: 1.595305 Validation acc: 0.490000\n",
      "Epoch: 43/2000 Iteration: 130 Train loss: 1.594893 Train acc: 0.540000\n",
      "Epoch: 44/2000 Iteration: 135 Train loss: 1.589650 Train acc: 0.450000\n",
      "Epoch: 46/2000 Iteration: 140 Train loss: 1.600924 Train acc: 0.340000\n",
      "Epoch: 48/2000 Iteration: 145 Train loss: 1.593609 Train acc: 0.430000\n",
      "Epoch: 49/2000 Iteration: 150 Train loss: 1.583955 Train acc: 0.360000\n",
      "Epoch: 49/2000 Iteration: 150 Validation loss: 1.592088 Validation acc: 0.490000\n",
      "Epoch: 51/2000 Iteration: 155 Train loss: 1.599404 Train acc: 0.260000\n",
      "Epoch: 53/2000 Iteration: 160 Train loss: 1.593040 Train acc: 0.440000\n",
      "Epoch: 54/2000 Iteration: 165 Train loss: 1.584069 Train acc: 0.370000\n",
      "Epoch: 56/2000 Iteration: 170 Train loss: 1.590315 Train acc: 0.320000\n",
      "Epoch: 58/2000 Iteration: 175 Train loss: 1.583765 Train acc: 0.480000\n",
      "Epoch: 58/2000 Iteration: 175 Validation loss: 1.588815 Validation acc: 0.490000\n",
      "Epoch: 59/2000 Iteration: 180 Train loss: 1.579834 Train acc: 0.330000\n",
      "Epoch: 61/2000 Iteration: 185 Train loss: 1.595648 Train acc: 0.350000\n",
      "Epoch: 63/2000 Iteration: 190 Train loss: 1.580025 Train acc: 0.470000\n",
      "Epoch: 64/2000 Iteration: 195 Train loss: 1.578032 Train acc: 0.370000\n",
      "Epoch: 66/2000 Iteration: 200 Train loss: 1.588677 Train acc: 0.290000\n",
      "Epoch: 66/2000 Iteration: 200 Validation loss: 1.585333 Validation acc: 0.490000\n",
      "Epoch: 68/2000 Iteration: 205 Train loss: 1.579447 Train acc: 0.500000\n",
      "Epoch: 69/2000 Iteration: 210 Train loss: 1.577616 Train acc: 0.390000\n",
      "Epoch: 71/2000 Iteration: 215 Train loss: 1.591335 Train acc: 0.290000\n",
      "Epoch: 73/2000 Iteration: 220 Train loss: 1.580580 Train acc: 0.490000\n",
      "Epoch: 74/2000 Iteration: 225 Train loss: 1.569630 Train acc: 0.390000\n",
      "Epoch: 74/2000 Iteration: 225 Validation loss: 1.581587 Validation acc: 0.490000\n",
      "Epoch: 76/2000 Iteration: 230 Train loss: 1.582351 Train acc: 0.320000\n",
      "Epoch: 78/2000 Iteration: 235 Train loss: 1.577417 Train acc: 0.510000\n",
      "Epoch: 79/2000 Iteration: 240 Train loss: 1.567005 Train acc: 0.470000\n",
      "Epoch: 81/2000 Iteration: 245 Train loss: 1.572793 Train acc: 0.400000\n",
      "Epoch: 83/2000 Iteration: 250 Train loss: 1.582613 Train acc: 0.470000\n",
      "Epoch: 83/2000 Iteration: 250 Validation loss: 1.577735 Validation acc: 0.490000\n",
      "Epoch: 84/2000 Iteration: 255 Train loss: 1.567175 Train acc: 0.360000\n",
      "Epoch: 86/2000 Iteration: 260 Train loss: 1.578024 Train acc: 0.260000\n",
      "Epoch: 88/2000 Iteration: 265 Train loss: 1.567559 Train acc: 0.480000\n",
      "Epoch: 89/2000 Iteration: 270 Train loss: 1.556367 Train acc: 0.450000\n",
      "Epoch: 91/2000 Iteration: 275 Train loss: 1.566455 Train acc: 0.340000\n",
      "Epoch: 91/2000 Iteration: 275 Validation loss: 1.573709 Validation acc: 0.490000\n",
      "Epoch: 93/2000 Iteration: 280 Train loss: 1.573493 Train acc: 0.510000\n",
      "Epoch: 94/2000 Iteration: 285 Train loss: 1.560060 Train acc: 0.440000\n",
      "Epoch: 96/2000 Iteration: 290 Train loss: 1.587805 Train acc: 0.230000\n",
      "Epoch: 98/2000 Iteration: 295 Train loss: 1.560549 Train acc: 0.570000\n",
      "Epoch: 99/2000 Iteration: 300 Train loss: 1.553706 Train acc: 0.470000\n",
      "Epoch: 99/2000 Iteration: 300 Validation loss: 1.569573 Validation acc: 0.490000\n",
      "Epoch: 101/2000 Iteration: 305 Train loss: 1.566616 Train acc: 0.330000\n",
      "Epoch: 103/2000 Iteration: 310 Train loss: 1.559431 Train acc: 0.460000\n",
      "Epoch: 104/2000 Iteration: 315 Train loss: 1.557912 Train acc: 0.380000\n",
      "Epoch: 106/2000 Iteration: 320 Train loss: 1.573295 Train acc: 0.430000\n",
      "Epoch: 108/2000 Iteration: 325 Train loss: 1.560881 Train acc: 0.460000\n",
      "Epoch: 108/2000 Iteration: 325 Validation loss: 1.565176 Validation acc: 0.490000\n",
      "Epoch: 109/2000 Iteration: 330 Train loss: 1.542625 Train acc: 0.420000\n",
      "Epoch: 111/2000 Iteration: 335 Train loss: 1.557943 Train acc: 0.370000\n",
      "Epoch: 113/2000 Iteration: 340 Train loss: 1.553771 Train acc: 0.540000\n",
      "Epoch: 114/2000 Iteration: 345 Train loss: 1.549786 Train acc: 0.350000\n",
      "Epoch: 116/2000 Iteration: 350 Train loss: 1.570099 Train acc: 0.280000\n",
      "Epoch: 116/2000 Iteration: 350 Validation loss: 1.560572 Validation acc: 0.490000\n",
      "Epoch: 118/2000 Iteration: 355 Train loss: 1.550299 Train acc: 0.510000\n",
      "Epoch: 119/2000 Iteration: 360 Train loss: 1.545178 Train acc: 0.320000\n",
      "Epoch: 121/2000 Iteration: 365 Train loss: 1.559677 Train acc: 0.370000\n",
      "Epoch: 123/2000 Iteration: 370 Train loss: 1.555270 Train acc: 0.470000\n",
      "Epoch: 124/2000 Iteration: 375 Train loss: 1.557464 Train acc: 0.300000\n",
      "Epoch: 124/2000 Iteration: 375 Validation loss: 1.555754 Validation acc: 0.490000\n",
      "Epoch: 126/2000 Iteration: 380 Train loss: 1.564579 Train acc: 0.290000\n",
      "Epoch: 128/2000 Iteration: 385 Train loss: 1.545516 Train acc: 0.500000\n",
      "Epoch: 129/2000 Iteration: 390 Train loss: 1.542434 Train acc: 0.370000\n",
      "Epoch: 131/2000 Iteration: 395 Train loss: 1.576401 Train acc: 0.300000\n",
      "Epoch: 133/2000 Iteration: 400 Train loss: 1.544969 Train acc: 0.390000\n",
      "Epoch: 133/2000 Iteration: 400 Validation loss: 1.550736 Validation acc: 0.490000\n",
      "Epoch: 134/2000 Iteration: 405 Train loss: 1.519884 Train acc: 0.380000\n",
      "Epoch: 136/2000 Iteration: 410 Train loss: 1.552352 Train acc: 0.340000\n",
      "Epoch: 138/2000 Iteration: 415 Train loss: 1.533452 Train acc: 0.460000\n",
      "Epoch: 139/2000 Iteration: 420 Train loss: 1.506002 Train acc: 0.460000\n",
      "Epoch: 141/2000 Iteration: 425 Train loss: 1.564692 Train acc: 0.330000\n",
      "Epoch: 141/2000 Iteration: 425 Validation loss: 1.545437 Validation acc: 0.490000\n",
      "Epoch: 143/2000 Iteration: 430 Train loss: 1.549984 Train acc: 0.440000\n",
      "Epoch: 144/2000 Iteration: 435 Train loss: 1.528855 Train acc: 0.390000\n",
      "Epoch: 146/2000 Iteration: 440 Train loss: 1.569052 Train acc: 0.360000\n",
      "Epoch: 148/2000 Iteration: 445 Train loss: 1.544326 Train acc: 0.500000\n",
      "Epoch: 149/2000 Iteration: 450 Train loss: 1.521853 Train acc: 0.380000\n",
      "Epoch: 149/2000 Iteration: 450 Validation loss: 1.540144 Validation acc: 0.490000\n",
      "Epoch: 151/2000 Iteration: 455 Train loss: 1.544647 Train acc: 0.330000\n",
      "Epoch: 153/2000 Iteration: 460 Train loss: 1.526370 Train acc: 0.460000\n",
      "Epoch: 154/2000 Iteration: 465 Train loss: 1.540084 Train acc: 0.320000\n",
      "Epoch: 156/2000 Iteration: 470 Train loss: 1.539953 Train acc: 0.260000\n",
      "Epoch: 158/2000 Iteration: 475 Train loss: 1.518237 Train acc: 0.500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 158/2000 Iteration: 475 Validation loss: 1.534724 Validation acc: 0.490000\n",
      "Epoch: 159/2000 Iteration: 480 Train loss: 1.515184 Train acc: 0.390000\n",
      "Epoch: 161/2000 Iteration: 485 Train loss: 1.538724 Train acc: 0.370000\n",
      "Epoch: 163/2000 Iteration: 490 Train loss: 1.514103 Train acc: 0.480000\n",
      "Epoch: 164/2000 Iteration: 495 Train loss: 1.514095 Train acc: 0.440000\n",
      "Epoch: 166/2000 Iteration: 500 Train loss: 1.558004 Train acc: 0.280000\n",
      "Epoch: 166/2000 Iteration: 500 Validation loss: 1.528930 Validation acc: 0.490000\n",
      "Epoch: 168/2000 Iteration: 505 Train loss: 1.521913 Train acc: 0.470000\n",
      "Epoch: 169/2000 Iteration: 510 Train loss: 1.508233 Train acc: 0.330000\n",
      "Epoch: 171/2000 Iteration: 515 Train loss: 1.544396 Train acc: 0.330000\n",
      "Epoch: 173/2000 Iteration: 520 Train loss: 1.517615 Train acc: 0.460000\n",
      "Epoch: 174/2000 Iteration: 525 Train loss: 1.491978 Train acc: 0.350000\n",
      "Epoch: 174/2000 Iteration: 525 Validation loss: 1.523246 Validation acc: 0.490000\n",
      "Epoch: 176/2000 Iteration: 530 Train loss: 1.566871 Train acc: 0.320000\n",
      "Epoch: 178/2000 Iteration: 535 Train loss: 1.514171 Train acc: 0.470000\n",
      "Epoch: 179/2000 Iteration: 540 Train loss: 1.521863 Train acc: 0.360000\n",
      "Epoch: 181/2000 Iteration: 545 Train loss: 1.520733 Train acc: 0.360000\n",
      "Epoch: 183/2000 Iteration: 550 Train loss: 1.506745 Train acc: 0.520000\n",
      "Epoch: 183/2000 Iteration: 550 Validation loss: 1.517324 Validation acc: 0.490000\n",
      "Epoch: 184/2000 Iteration: 555 Train loss: 1.484538 Train acc: 0.320000\n",
      "Epoch: 186/2000 Iteration: 560 Train loss: 1.529606 Train acc: 0.310000\n",
      "Epoch: 188/2000 Iteration: 565 Train loss: 1.505595 Train acc: 0.490000\n",
      "Epoch: 189/2000 Iteration: 570 Train loss: 1.424990 Train acc: 0.450000\n",
      "Epoch: 191/2000 Iteration: 575 Train loss: 1.533674 Train acc: 0.340000\n",
      "Epoch: 191/2000 Iteration: 575 Validation loss: 1.511230 Validation acc: 0.490000\n",
      "Epoch: 193/2000 Iteration: 580 Train loss: 1.481999 Train acc: 0.530000\n",
      "Epoch: 194/2000 Iteration: 585 Train loss: 1.511992 Train acc: 0.330000\n",
      "Epoch: 196/2000 Iteration: 590 Train loss: 1.571701 Train acc: 0.300000\n",
      "Epoch: 198/2000 Iteration: 595 Train loss: 1.490016 Train acc: 0.500000\n",
      "Epoch: 199/2000 Iteration: 600 Train loss: 1.468782 Train acc: 0.370000\n",
      "Epoch: 199/2000 Iteration: 600 Validation loss: 1.505162 Validation acc: 0.490000\n",
      "Epoch: 201/2000 Iteration: 605 Train loss: 1.529323 Train acc: 0.340000\n",
      "Epoch: 203/2000 Iteration: 610 Train loss: 1.499648 Train acc: 0.530000\n",
      "Epoch: 204/2000 Iteration: 615 Train loss: 1.471469 Train acc: 0.360000\n",
      "Epoch: 206/2000 Iteration: 620 Train loss: 1.552602 Train acc: 0.320000\n",
      "Epoch: 208/2000 Iteration: 625 Train loss: 1.486452 Train acc: 0.530000\n",
      "Epoch: 208/2000 Iteration: 625 Validation loss: 1.499154 Validation acc: 0.490000\n",
      "Epoch: 209/2000 Iteration: 630 Train loss: 1.484308 Train acc: 0.360000\n",
      "Epoch: 211/2000 Iteration: 635 Train loss: 1.523608 Train acc: 0.280000\n",
      "Epoch: 213/2000 Iteration: 640 Train loss: 1.485617 Train acc: 0.550000\n",
      "Epoch: 214/2000 Iteration: 645 Train loss: 1.464957 Train acc: 0.360000\n",
      "Epoch: 216/2000 Iteration: 650 Train loss: 1.515893 Train acc: 0.360000\n",
      "Epoch: 216/2000 Iteration: 650 Validation loss: 1.492769 Validation acc: 0.490000\n",
      "Epoch: 218/2000 Iteration: 655 Train loss: 1.483394 Train acc: 0.500000\n",
      "Epoch: 219/2000 Iteration: 660 Train loss: 1.467573 Train acc: 0.450000\n",
      "Epoch: 221/2000 Iteration: 665 Train loss: 1.550362 Train acc: 0.300000\n",
      "Epoch: 223/2000 Iteration: 670 Train loss: 1.489030 Train acc: 0.470000\n",
      "Epoch: 224/2000 Iteration: 675 Train loss: 1.465536 Train acc: 0.300000\n",
      "Epoch: 224/2000 Iteration: 675 Validation loss: 1.487064 Validation acc: 0.490000\n",
      "Epoch: 226/2000 Iteration: 680 Train loss: 1.476246 Train acc: 0.390000\n",
      "Epoch: 228/2000 Iteration: 685 Train loss: 1.490571 Train acc: 0.440000\n",
      "Epoch: 229/2000 Iteration: 690 Train loss: 1.445822 Train acc: 0.320000\n",
      "Epoch: 231/2000 Iteration: 695 Train loss: 1.496750 Train acc: 0.350000\n",
      "Epoch: 233/2000 Iteration: 700 Train loss: 1.477797 Train acc: 0.480000\n",
      "Epoch: 233/2000 Iteration: 700 Validation loss: 1.480803 Validation acc: 0.490000\n",
      "Epoch: 234/2000 Iteration: 705 Train loss: 1.451271 Train acc: 0.310000\n",
      "Epoch: 236/2000 Iteration: 710 Train loss: 1.516693 Train acc: 0.310000\n",
      "Epoch: 238/2000 Iteration: 715 Train loss: 1.442526 Train acc: 0.480000\n",
      "Epoch: 239/2000 Iteration: 720 Train loss: 1.460423 Train acc: 0.360000\n",
      "Epoch: 241/2000 Iteration: 725 Train loss: 1.503310 Train acc: 0.370000\n",
      "Epoch: 241/2000 Iteration: 725 Validation loss: 1.474379 Validation acc: 0.490000\n",
      "Epoch: 243/2000 Iteration: 730 Train loss: 1.479240 Train acc: 0.480000\n",
      "Epoch: 244/2000 Iteration: 735 Train loss: 1.405217 Train acc: 0.390000\n",
      "Epoch: 246/2000 Iteration: 740 Train loss: 1.467736 Train acc: 0.270000\n",
      "Epoch: 248/2000 Iteration: 745 Train loss: 1.457323 Train acc: 0.440000\n",
      "Epoch: 249/2000 Iteration: 750 Train loss: 1.415275 Train acc: 0.350000\n",
      "Epoch: 249/2000 Iteration: 750 Validation loss: 1.468304 Validation acc: 0.490000\n",
      "Epoch: 251/2000 Iteration: 755 Train loss: 1.509497 Train acc: 0.360000\n",
      "Epoch: 253/2000 Iteration: 760 Train loss: 1.446081 Train acc: 0.490000\n",
      "Epoch: 254/2000 Iteration: 765 Train loss: 1.410993 Train acc: 0.340000\n",
      "Epoch: 256/2000 Iteration: 770 Train loss: 1.529604 Train acc: 0.350000\n",
      "Epoch: 258/2000 Iteration: 775 Train loss: 1.441797 Train acc: 0.470000\n",
      "Epoch: 258/2000 Iteration: 775 Validation loss: 1.462116 Validation acc: 0.490000\n",
      "Epoch: 259/2000 Iteration: 780 Train loss: 1.407165 Train acc: 0.460000\n",
      "Epoch: 261/2000 Iteration: 785 Train loss: 1.527573 Train acc: 0.380000\n",
      "Epoch: 263/2000 Iteration: 790 Train loss: 1.425039 Train acc: 0.440000\n",
      "Epoch: 264/2000 Iteration: 795 Train loss: 1.440392 Train acc: 0.380000\n",
      "Epoch: 266/2000 Iteration: 800 Train loss: 1.472655 Train acc: 0.380000\n",
      "Epoch: 266/2000 Iteration: 800 Validation loss: 1.455745 Validation acc: 0.490000\n",
      "Epoch: 268/2000 Iteration: 805 Train loss: 1.433082 Train acc: 0.480000\n",
      "Epoch: 269/2000 Iteration: 810 Train loss: 1.461363 Train acc: 0.360000\n",
      "Epoch: 271/2000 Iteration: 815 Train loss: 1.504033 Train acc: 0.370000\n",
      "Epoch: 273/2000 Iteration: 820 Train loss: 1.452841 Train acc: 0.470000\n",
      "Epoch: 274/2000 Iteration: 825 Train loss: 1.437440 Train acc: 0.410000\n",
      "Epoch: 274/2000 Iteration: 825 Validation loss: 1.449809 Validation acc: 0.490000\n",
      "Epoch: 276/2000 Iteration: 830 Train loss: 1.544462 Train acc: 0.320000\n",
      "Epoch: 278/2000 Iteration: 835 Train loss: 1.438823 Train acc: 0.490000\n",
      "Epoch: 279/2000 Iteration: 840 Train loss: 1.503335 Train acc: 0.340000\n",
      "Epoch: 281/2000 Iteration: 845 Train loss: 1.461411 Train acc: 0.370000\n",
      "Epoch: 283/2000 Iteration: 850 Train loss: 1.451950 Train acc: 0.490000\n",
      "Epoch: 283/2000 Iteration: 850 Validation loss: 1.443738 Validation acc: 0.490000\n",
      "Epoch: 284/2000 Iteration: 855 Train loss: 1.430624 Train acc: 0.370000\n",
      "Epoch: 286/2000 Iteration: 860 Train loss: 1.512837 Train acc: 0.280000\n",
      "Epoch: 288/2000 Iteration: 865 Train loss: 1.412217 Train acc: 0.540000\n",
      "Epoch: 289/2000 Iteration: 870 Train loss: 1.391403 Train acc: 0.360000\n",
      "Epoch: 291/2000 Iteration: 875 Train loss: 1.453595 Train acc: 0.320000\n",
      "Epoch: 291/2000 Iteration: 875 Validation loss: 1.437548 Validation acc: 0.490000\n",
      "Epoch: 293/2000 Iteration: 880 Train loss: 1.423677 Train acc: 0.520000\n",
      "Epoch: 294/2000 Iteration: 885 Train loss: 1.393518 Train acc: 0.430000\n",
      "Epoch: 296/2000 Iteration: 890 Train loss: 1.523615 Train acc: 0.290000\n",
      "Epoch: 298/2000 Iteration: 895 Train loss: 1.411212 Train acc: 0.460000\n",
      "Epoch: 299/2000 Iteration: 900 Train loss: 1.346299 Train acc: 0.480000\n",
      "Epoch: 299/2000 Iteration: 900 Validation loss: 1.431928 Validation acc: 0.490000\n",
      "Epoch: 301/2000 Iteration: 905 Train loss: 1.449165 Train acc: 0.310000\n",
      "Epoch: 303/2000 Iteration: 910 Train loss: 1.449328 Train acc: 0.440000\n",
      "Epoch: 304/2000 Iteration: 915 Train loss: 1.488085 Train acc: 0.370000\n",
      "Epoch: 306/2000 Iteration: 920 Train loss: 1.463783 Train acc: 0.340000\n",
      "Epoch: 308/2000 Iteration: 925 Train loss: 1.389459 Train acc: 0.500000\n",
      "Epoch: 308/2000 Iteration: 925 Validation loss: 1.426547 Validation acc: 0.490000\n",
      "Epoch: 309/2000 Iteration: 930 Train loss: 1.396039 Train acc: 0.400000\n",
      "Epoch: 311/2000 Iteration: 935 Train loss: 1.423202 Train acc: 0.460000\n",
      "Epoch: 313/2000 Iteration: 940 Train loss: 1.421690 Train acc: 0.480000\n",
      "Epoch: 314/2000 Iteration: 945 Train loss: 1.423874 Train acc: 0.390000\n",
      "Epoch: 316/2000 Iteration: 950 Train loss: 1.512787 Train acc: 0.340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 316/2000 Iteration: 950 Validation loss: 1.421172 Validation acc: 0.490000\n",
      "Epoch: 318/2000 Iteration: 955 Train loss: 1.437831 Train acc: 0.490000\n",
      "Epoch: 319/2000 Iteration: 960 Train loss: 1.435058 Train acc: 0.400000\n",
      "Epoch: 321/2000 Iteration: 965 Train loss: 1.536153 Train acc: 0.300000\n",
      "Epoch: 323/2000 Iteration: 970 Train loss: 1.405001 Train acc: 0.480000\n",
      "Epoch: 324/2000 Iteration: 975 Train loss: 1.422584 Train acc: 0.420000\n",
      "Epoch: 324/2000 Iteration: 975 Validation loss: 1.415518 Validation acc: 0.490000\n",
      "Epoch: 326/2000 Iteration: 980 Train loss: 1.477592 Train acc: 0.350000\n",
      "Epoch: 328/2000 Iteration: 985 Train loss: 1.385655 Train acc: 0.550000\n",
      "Epoch: 329/2000 Iteration: 990 Train loss: 1.383868 Train acc: 0.420000\n",
      "Epoch: 331/2000 Iteration: 995 Train loss: 1.463278 Train acc: 0.370000\n",
      "Epoch: 333/2000 Iteration: 1000 Train loss: 1.399205 Train acc: 0.450000\n",
      "Epoch: 333/2000 Iteration: 1000 Validation loss: 1.409783 Validation acc: 0.490000\n",
      "Epoch: 334/2000 Iteration: 1005 Train loss: 1.418994 Train acc: 0.430000\n",
      "Epoch: 336/2000 Iteration: 1010 Train loss: 1.454850 Train acc: 0.350000\n",
      "Epoch: 338/2000 Iteration: 1015 Train loss: 1.391389 Train acc: 0.540000\n",
      "Epoch: 339/2000 Iteration: 1020 Train loss: 1.376765 Train acc: 0.500000\n",
      "Epoch: 341/2000 Iteration: 1025 Train loss: 1.495832 Train acc: 0.310000\n",
      "Epoch: 341/2000 Iteration: 1025 Validation loss: 1.404880 Validation acc: 0.490000\n",
      "Epoch: 343/2000 Iteration: 1030 Train loss: 1.397919 Train acc: 0.470000\n",
      "Epoch: 344/2000 Iteration: 1035 Train loss: 1.339659 Train acc: 0.520000\n",
      "Epoch: 346/2000 Iteration: 1040 Train loss: 1.480845 Train acc: 0.300000\n",
      "Epoch: 348/2000 Iteration: 1045 Train loss: 1.368120 Train acc: 0.540000\n",
      "Epoch: 349/2000 Iteration: 1050 Train loss: 1.348914 Train acc: 0.470000\n",
      "Epoch: 349/2000 Iteration: 1050 Validation loss: 1.399837 Validation acc: 0.490000\n",
      "Epoch: 351/2000 Iteration: 1055 Train loss: 1.453968 Train acc: 0.330000\n",
      "Epoch: 353/2000 Iteration: 1060 Train loss: 1.402635 Train acc: 0.470000\n",
      "Epoch: 354/2000 Iteration: 1065 Train loss: 1.365608 Train acc: 0.390000\n",
      "Epoch: 356/2000 Iteration: 1070 Train loss: 1.519555 Train acc: 0.330000\n",
      "Epoch: 358/2000 Iteration: 1075 Train loss: 1.380339 Train acc: 0.460000\n",
      "Epoch: 358/2000 Iteration: 1075 Validation loss: 1.395298 Validation acc: 0.490000\n",
      "Epoch: 359/2000 Iteration: 1080 Train loss: 1.436683 Train acc: 0.380000\n",
      "Epoch: 361/2000 Iteration: 1085 Train loss: 1.506490 Train acc: 0.340000\n",
      "Epoch: 363/2000 Iteration: 1090 Train loss: 1.388605 Train acc: 0.450000\n",
      "Epoch: 364/2000 Iteration: 1095 Train loss: 1.414644 Train acc: 0.390000\n",
      "Epoch: 366/2000 Iteration: 1100 Train loss: 1.484800 Train acc: 0.320000\n",
      "Epoch: 366/2000 Iteration: 1100 Validation loss: 1.390519 Validation acc: 0.490000\n",
      "Epoch: 368/2000 Iteration: 1105 Train loss: 1.372482 Train acc: 0.470000\n",
      "Epoch: 369/2000 Iteration: 1110 Train loss: 1.394494 Train acc: 0.400000\n",
      "Epoch: 371/2000 Iteration: 1115 Train loss: 1.441070 Train acc: 0.390000\n",
      "Epoch: 373/2000 Iteration: 1120 Train loss: 1.374735 Train acc: 0.520000\n",
      "Epoch: 374/2000 Iteration: 1125 Train loss: 1.332205 Train acc: 0.490000\n",
      "Epoch: 374/2000 Iteration: 1125 Validation loss: 1.385896 Validation acc: 0.490000\n",
      "Epoch: 376/2000 Iteration: 1130 Train loss: 1.467178 Train acc: 0.410000\n",
      "Epoch: 378/2000 Iteration: 1135 Train loss: 1.385038 Train acc: 0.480000\n",
      "Epoch: 379/2000 Iteration: 1140 Train loss: 1.421177 Train acc: 0.410000\n",
      "Epoch: 381/2000 Iteration: 1145 Train loss: 1.507587 Train acc: 0.280000\n",
      "Epoch: 383/2000 Iteration: 1150 Train loss: 1.393671 Train acc: 0.510000\n",
      "Epoch: 383/2000 Iteration: 1150 Validation loss: 1.381528 Validation acc: 0.490000\n",
      "Epoch: 384/2000 Iteration: 1155 Train loss: 1.370557 Train acc: 0.390000\n",
      "Epoch: 386/2000 Iteration: 1160 Train loss: 1.430142 Train acc: 0.370000\n",
      "Epoch: 388/2000 Iteration: 1165 Train loss: 1.389745 Train acc: 0.510000\n",
      "Epoch: 389/2000 Iteration: 1170 Train loss: 1.422657 Train acc: 0.390000\n",
      "Epoch: 391/2000 Iteration: 1175 Train loss: 1.507194 Train acc: 0.350000\n",
      "Epoch: 391/2000 Iteration: 1175 Validation loss: 1.377337 Validation acc: 0.490000\n",
      "Epoch: 393/2000 Iteration: 1180 Train loss: 1.394719 Train acc: 0.450000\n",
      "Epoch: 394/2000 Iteration: 1185 Train loss: 1.396319 Train acc: 0.380000\n",
      "Epoch: 396/2000 Iteration: 1190 Train loss: 1.505996 Train acc: 0.340000\n",
      "Epoch: 398/2000 Iteration: 1195 Train loss: 1.359472 Train acc: 0.490000\n",
      "Epoch: 399/2000 Iteration: 1200 Train loss: 1.372279 Train acc: 0.380000\n",
      "Epoch: 399/2000 Iteration: 1200 Validation loss: 1.373114 Validation acc: 0.490000\n",
      "Epoch: 401/2000 Iteration: 1205 Train loss: 1.460493 Train acc: 0.310000\n",
      "Epoch: 403/2000 Iteration: 1210 Train loss: 1.314397 Train acc: 0.530000\n",
      "Epoch: 404/2000 Iteration: 1215 Train loss: 1.409894 Train acc: 0.410000\n",
      "Epoch: 406/2000 Iteration: 1220 Train loss: 1.472457 Train acc: 0.360000\n",
      "Epoch: 408/2000 Iteration: 1225 Train loss: 1.355862 Train acc: 0.460000\n",
      "Epoch: 408/2000 Iteration: 1225 Validation loss: 1.368628 Validation acc: 0.490000\n",
      "Epoch: 409/2000 Iteration: 1230 Train loss: 1.351187 Train acc: 0.440000\n",
      "Epoch: 411/2000 Iteration: 1235 Train loss: 1.481237 Train acc: 0.360000\n",
      "Epoch: 413/2000 Iteration: 1240 Train loss: 1.377970 Train acc: 0.460000\n",
      "Epoch: 414/2000 Iteration: 1245 Train loss: 1.380154 Train acc: 0.380000\n",
      "Epoch: 416/2000 Iteration: 1250 Train loss: 1.536051 Train acc: 0.320000\n",
      "Epoch: 416/2000 Iteration: 1250 Validation loss: 1.364965 Validation acc: 0.490000\n",
      "Epoch: 418/2000 Iteration: 1255 Train loss: 1.326327 Train acc: 0.510000\n",
      "Epoch: 419/2000 Iteration: 1260 Train loss: 1.326685 Train acc: 0.460000\n",
      "Epoch: 421/2000 Iteration: 1265 Train loss: 1.440327 Train acc: 0.360000\n",
      "Epoch: 423/2000 Iteration: 1270 Train loss: 1.335144 Train acc: 0.490000\n",
      "Epoch: 424/2000 Iteration: 1275 Train loss: 1.387908 Train acc: 0.380000\n",
      "Epoch: 424/2000 Iteration: 1275 Validation loss: 1.361132 Validation acc: 0.490000\n",
      "Epoch: 426/2000 Iteration: 1280 Train loss: 1.470920 Train acc: 0.340000\n",
      "Epoch: 428/2000 Iteration: 1285 Train loss: 1.353144 Train acc: 0.480000\n",
      "Epoch: 429/2000 Iteration: 1290 Train loss: 1.403778 Train acc: 0.340000\n",
      "Epoch: 431/2000 Iteration: 1295 Train loss: 1.450177 Train acc: 0.360000\n",
      "Epoch: 433/2000 Iteration: 1300 Train loss: 1.315653 Train acc: 0.510000\n",
      "Epoch: 433/2000 Iteration: 1300 Validation loss: 1.356887 Validation acc: 0.490000\n",
      "Epoch: 434/2000 Iteration: 1305 Train loss: 1.367522 Train acc: 0.420000\n",
      "Epoch: 436/2000 Iteration: 1310 Train loss: 1.470266 Train acc: 0.340000\n",
      "Epoch: 438/2000 Iteration: 1315 Train loss: 1.322738 Train acc: 0.530000\n",
      "Epoch: 439/2000 Iteration: 1320 Train loss: 1.367013 Train acc: 0.450000\n",
      "Epoch: 441/2000 Iteration: 1325 Train loss: 1.473844 Train acc: 0.390000\n",
      "Epoch: 441/2000 Iteration: 1325 Validation loss: 1.353035 Validation acc: 0.490000\n",
      "Epoch: 443/2000 Iteration: 1330 Train loss: 1.384433 Train acc: 0.490000\n",
      "Epoch: 444/2000 Iteration: 1335 Train loss: 1.425493 Train acc: 0.380000\n",
      "Epoch: 446/2000 Iteration: 1340 Train loss: 1.420435 Train acc: 0.390000\n",
      "Epoch: 448/2000 Iteration: 1345 Train loss: 1.367035 Train acc: 0.530000\n",
      "Epoch: 449/2000 Iteration: 1350 Train loss: 1.325013 Train acc: 0.430000\n",
      "Epoch: 449/2000 Iteration: 1350 Validation loss: 1.349695 Validation acc: 0.490000\n",
      "Epoch: 451/2000 Iteration: 1355 Train loss: 1.501711 Train acc: 0.390000\n",
      "Epoch: 453/2000 Iteration: 1360 Train loss: 1.332494 Train acc: 0.520000\n",
      "Epoch: 454/2000 Iteration: 1365 Train loss: 1.368072 Train acc: 0.420000\n",
      "Epoch: 456/2000 Iteration: 1370 Train loss: 1.403471 Train acc: 0.380000\n",
      "Epoch: 458/2000 Iteration: 1375 Train loss: 1.351282 Train acc: 0.510000\n",
      "Epoch: 458/2000 Iteration: 1375 Validation loss: 1.346247 Validation acc: 0.490000\n",
      "Epoch: 459/2000 Iteration: 1380 Train loss: 1.389637 Train acc: 0.320000\n",
      "Epoch: 461/2000 Iteration: 1385 Train loss: 1.456400 Train acc: 0.310000\n",
      "Epoch: 463/2000 Iteration: 1390 Train loss: 1.337690 Train acc: 0.480000\n",
      "Epoch: 464/2000 Iteration: 1395 Train loss: 1.390632 Train acc: 0.380000\n",
      "Epoch: 466/2000 Iteration: 1400 Train loss: 1.444574 Train acc: 0.360000\n",
      "Epoch: 466/2000 Iteration: 1400 Validation loss: 1.343034 Validation acc: 0.490000\n",
      "Epoch: 468/2000 Iteration: 1405 Train loss: 1.299629 Train acc: 0.540000\n",
      "Epoch: 469/2000 Iteration: 1410 Train loss: 1.336348 Train acc: 0.450000\n",
      "Epoch: 471/2000 Iteration: 1415 Train loss: 1.455183 Train acc: 0.340000\n",
      "Epoch: 473/2000 Iteration: 1420 Train loss: 1.344886 Train acc: 0.500000\n",
      "Epoch: 474/2000 Iteration: 1425 Train loss: 1.317530 Train acc: 0.490000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 474/2000 Iteration: 1425 Validation loss: 1.339332 Validation acc: 0.490000\n",
      "Epoch: 476/2000 Iteration: 1430 Train loss: 1.446321 Train acc: 0.390000\n",
      "Epoch: 478/2000 Iteration: 1435 Train loss: 1.336973 Train acc: 0.520000\n",
      "Epoch: 479/2000 Iteration: 1440 Train loss: 1.316692 Train acc: 0.470000\n",
      "Epoch: 481/2000 Iteration: 1445 Train loss: 1.494753 Train acc: 0.300000\n",
      "Epoch: 483/2000 Iteration: 1450 Train loss: 1.321001 Train acc: 0.490000\n",
      "Epoch: 483/2000 Iteration: 1450 Validation loss: 1.335694 Validation acc: 0.490000\n",
      "Epoch: 484/2000 Iteration: 1455 Train loss: 1.318386 Train acc: 0.450000\n",
      "Epoch: 486/2000 Iteration: 1460 Train loss: 1.491572 Train acc: 0.390000\n",
      "Epoch: 488/2000 Iteration: 1465 Train loss: 1.348274 Train acc: 0.460000\n",
      "Epoch: 489/2000 Iteration: 1470 Train loss: 1.369967 Train acc: 0.430000\n",
      "Epoch: 491/2000 Iteration: 1475 Train loss: 1.433221 Train acc: 0.350000\n",
      "Epoch: 491/2000 Iteration: 1475 Validation loss: 1.332370 Validation acc: 0.490000\n",
      "Epoch: 493/2000 Iteration: 1480 Train loss: 1.302215 Train acc: 0.540000\n",
      "Epoch: 494/2000 Iteration: 1485 Train loss: 1.369063 Train acc: 0.410000\n",
      "Epoch: 496/2000 Iteration: 1490 Train loss: 1.445492 Train acc: 0.360000\n",
      "Epoch: 498/2000 Iteration: 1495 Train loss: 1.301103 Train acc: 0.570000\n",
      "Epoch: 499/2000 Iteration: 1500 Train loss: 1.328156 Train acc: 0.430000\n",
      "Epoch: 499/2000 Iteration: 1500 Validation loss: 1.328963 Validation acc: 0.490000\n",
      "Epoch: 501/2000 Iteration: 1505 Train loss: 1.447696 Train acc: 0.330000\n",
      "Epoch: 503/2000 Iteration: 1510 Train loss: 1.278250 Train acc: 0.570000\n",
      "Epoch: 504/2000 Iteration: 1515 Train loss: 1.394188 Train acc: 0.390000\n",
      "Epoch: 506/2000 Iteration: 1520 Train loss: 1.436116 Train acc: 0.330000\n",
      "Epoch: 508/2000 Iteration: 1525 Train loss: 1.324671 Train acc: 0.440000\n",
      "Epoch: 508/2000 Iteration: 1525 Validation loss: 1.326228 Validation acc: 0.490000\n",
      "Epoch: 509/2000 Iteration: 1530 Train loss: 1.379579 Train acc: 0.370000\n",
      "Epoch: 511/2000 Iteration: 1535 Train loss: 1.451305 Train acc: 0.320000\n",
      "Epoch: 513/2000 Iteration: 1540 Train loss: 1.310381 Train acc: 0.490000\n",
      "Epoch: 514/2000 Iteration: 1545 Train loss: 1.368725 Train acc: 0.450000\n",
      "Epoch: 516/2000 Iteration: 1550 Train loss: 1.404345 Train acc: 0.340000\n",
      "Epoch: 516/2000 Iteration: 1550 Validation loss: 1.323747 Validation acc: 0.490000\n",
      "Epoch: 518/2000 Iteration: 1555 Train loss: 1.282981 Train acc: 0.550000\n",
      "Epoch: 519/2000 Iteration: 1560 Train loss: 1.437261 Train acc: 0.460000\n",
      "Epoch: 521/2000 Iteration: 1565 Train loss: 1.462529 Train acc: 0.360000\n",
      "Epoch: 523/2000 Iteration: 1570 Train loss: 1.301615 Train acc: 0.450000\n",
      "Epoch: 524/2000 Iteration: 1575 Train loss: 1.327450 Train acc: 0.440000\n",
      "Epoch: 524/2000 Iteration: 1575 Validation loss: 1.321496 Validation acc: 0.490000\n",
      "Epoch: 526/2000 Iteration: 1580 Train loss: 1.432218 Train acc: 0.400000\n",
      "Epoch: 528/2000 Iteration: 1585 Train loss: 1.313420 Train acc: 0.450000\n",
      "Epoch: 529/2000 Iteration: 1590 Train loss: 1.355968 Train acc: 0.380000\n",
      "Epoch: 531/2000 Iteration: 1595 Train loss: 1.438133 Train acc: 0.380000\n",
      "Epoch: 533/2000 Iteration: 1600 Train loss: 1.376546 Train acc: 0.450000\n",
      "Epoch: 533/2000 Iteration: 1600 Validation loss: 1.318652 Validation acc: 0.490000\n",
      "Epoch: 534/2000 Iteration: 1605 Train loss: 1.393210 Train acc: 0.420000\n",
      "Epoch: 536/2000 Iteration: 1610 Train loss: 1.494098 Train acc: 0.310000\n",
      "Epoch: 538/2000 Iteration: 1615 Train loss: 1.295881 Train acc: 0.520000\n",
      "Epoch: 539/2000 Iteration: 1620 Train loss: 1.341334 Train acc: 0.450000\n",
      "Epoch: 541/2000 Iteration: 1625 Train loss: 1.402882 Train acc: 0.400000\n",
      "Epoch: 541/2000 Iteration: 1625 Validation loss: 1.316249 Validation acc: 0.490000\n",
      "Epoch: 543/2000 Iteration: 1630 Train loss: 1.298884 Train acc: 0.500000\n",
      "Epoch: 544/2000 Iteration: 1635 Train loss: 1.337206 Train acc: 0.420000\n",
      "Epoch: 546/2000 Iteration: 1640 Train loss: 1.494343 Train acc: 0.290000\n",
      "Epoch: 548/2000 Iteration: 1645 Train loss: 1.309642 Train acc: 0.480000\n",
      "Epoch: 549/2000 Iteration: 1650 Train loss: 1.300771 Train acc: 0.430000\n",
      "Epoch: 549/2000 Iteration: 1650 Validation loss: 1.313691 Validation acc: 0.490000\n",
      "Epoch: 551/2000 Iteration: 1655 Train loss: 1.404601 Train acc: 0.340000\n",
      "Epoch: 553/2000 Iteration: 1660 Train loss: 1.329243 Train acc: 0.470000\n",
      "Epoch: 554/2000 Iteration: 1665 Train loss: 1.400517 Train acc: 0.370000\n",
      "Epoch: 556/2000 Iteration: 1670 Train loss: 1.426344 Train acc: 0.340000\n",
      "Epoch: 558/2000 Iteration: 1675 Train loss: 1.290821 Train acc: 0.510000\n",
      "Epoch: 558/2000 Iteration: 1675 Validation loss: 1.311421 Validation acc: 0.490000\n",
      "Epoch: 559/2000 Iteration: 1680 Train loss: 1.334292 Train acc: 0.380000\n",
      "Epoch: 561/2000 Iteration: 1685 Train loss: 1.431298 Train acc: 0.340000\n",
      "Epoch: 563/2000 Iteration: 1690 Train loss: 1.290834 Train acc: 0.530000\n",
      "Epoch: 564/2000 Iteration: 1695 Train loss: 1.336025 Train acc: 0.450000\n",
      "Epoch: 566/2000 Iteration: 1700 Train loss: 1.401500 Train acc: 0.330000\n",
      "Epoch: 566/2000 Iteration: 1700 Validation loss: 1.308861 Validation acc: 0.490000\n",
      "Epoch: 568/2000 Iteration: 1705 Train loss: 1.310914 Train acc: 0.530000\n",
      "Epoch: 569/2000 Iteration: 1710 Train loss: 1.345410 Train acc: 0.390000\n",
      "Epoch: 571/2000 Iteration: 1715 Train loss: 1.436249 Train acc: 0.290000\n",
      "Epoch: 573/2000 Iteration: 1720 Train loss: 1.320034 Train acc: 0.440000\n",
      "Epoch: 574/2000 Iteration: 1725 Train loss: 1.337849 Train acc: 0.430000\n",
      "Epoch: 574/2000 Iteration: 1725 Validation loss: 1.306405 Validation acc: 0.490000\n",
      "Epoch: 576/2000 Iteration: 1730 Train loss: 1.402980 Train acc: 0.340000\n",
      "Epoch: 578/2000 Iteration: 1735 Train loss: 1.293468 Train acc: 0.480000\n",
      "Epoch: 579/2000 Iteration: 1740 Train loss: 1.352053 Train acc: 0.440000\n",
      "Epoch: 581/2000 Iteration: 1745 Train loss: 1.448754 Train acc: 0.320000\n",
      "Epoch: 583/2000 Iteration: 1750 Train loss: 1.340800 Train acc: 0.450000\n",
      "Epoch: 583/2000 Iteration: 1750 Validation loss: 1.303954 Validation acc: 0.490000\n",
      "Epoch: 584/2000 Iteration: 1755 Train loss: 1.369104 Train acc: 0.400000\n",
      "Epoch: 586/2000 Iteration: 1760 Train loss: 1.454171 Train acc: 0.370000\n",
      "Epoch: 588/2000 Iteration: 1765 Train loss: 1.255563 Train acc: 0.560000\n",
      "Epoch: 589/2000 Iteration: 1770 Train loss: 1.354405 Train acc: 0.420000\n",
      "Epoch: 591/2000 Iteration: 1775 Train loss: 1.401741 Train acc: 0.350000\n",
      "Epoch: 591/2000 Iteration: 1775 Validation loss: 1.301874 Validation acc: 0.490000\n",
      "Epoch: 593/2000 Iteration: 1780 Train loss: 1.256819 Train acc: 0.520000\n",
      "Epoch: 594/2000 Iteration: 1785 Train loss: 1.348176 Train acc: 0.380000\n",
      "Epoch: 596/2000 Iteration: 1790 Train loss: 1.429399 Train acc: 0.340000\n",
      "Epoch: 598/2000 Iteration: 1795 Train loss: 1.295079 Train acc: 0.520000\n",
      "Epoch: 599/2000 Iteration: 1800 Train loss: 1.393541 Train acc: 0.320000\n",
      "Epoch: 599/2000 Iteration: 1800 Validation loss: 1.299822 Validation acc: 0.490000\n",
      "Epoch: 601/2000 Iteration: 1805 Train loss: 1.465508 Train acc: 0.390000\n",
      "Epoch: 603/2000 Iteration: 1810 Train loss: 1.265613 Train acc: 0.500000\n",
      "Epoch: 604/2000 Iteration: 1815 Train loss: 1.296345 Train acc: 0.460000\n",
      "Epoch: 606/2000 Iteration: 1820 Train loss: 1.430749 Train acc: 0.340000\n",
      "Epoch: 608/2000 Iteration: 1825 Train loss: 1.270109 Train acc: 0.480000\n",
      "Epoch: 608/2000 Iteration: 1825 Validation loss: 1.297739 Validation acc: 0.490000\n",
      "Epoch: 609/2000 Iteration: 1830 Train loss: 1.293436 Train acc: 0.470000\n",
      "Epoch: 611/2000 Iteration: 1835 Train loss: 1.427370 Train acc: 0.380000\n",
      "Epoch: 613/2000 Iteration: 1840 Train loss: 1.272955 Train acc: 0.450000\n",
      "Epoch: 614/2000 Iteration: 1845 Train loss: 1.315909 Train acc: 0.450000\n",
      "Epoch: 616/2000 Iteration: 1850 Train loss: 1.467643 Train acc: 0.270000\n",
      "Epoch: 616/2000 Iteration: 1850 Validation loss: 1.295467 Validation acc: 0.490000\n",
      "Epoch: 618/2000 Iteration: 1855 Train loss: 1.331617 Train acc: 0.470000\n",
      "Epoch: 619/2000 Iteration: 1860 Train loss: 1.316420 Train acc: 0.380000\n",
      "Epoch: 621/2000 Iteration: 1865 Train loss: 1.470536 Train acc: 0.340000\n",
      "Epoch: 623/2000 Iteration: 1870 Train loss: 1.300192 Train acc: 0.480000\n",
      "Epoch: 624/2000 Iteration: 1875 Train loss: 1.366589 Train acc: 0.400000\n",
      "Epoch: 624/2000 Iteration: 1875 Validation loss: 1.293710 Validation acc: 0.490000\n",
      "Epoch: 626/2000 Iteration: 1880 Train loss: 1.416565 Train acc: 0.390000\n",
      "Epoch: 628/2000 Iteration: 1885 Train loss: 1.312043 Train acc: 0.430000\n",
      "Epoch: 629/2000 Iteration: 1890 Train loss: 1.347380 Train acc: 0.380000\n",
      "Epoch: 631/2000 Iteration: 1895 Train loss: 1.349352 Train acc: 0.440000\n",
      "Epoch: 633/2000 Iteration: 1900 Train loss: 1.305345 Train acc: 0.460000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 633/2000 Iteration: 1900 Validation loss: 1.292067 Validation acc: 0.490000\n",
      "Epoch: 634/2000 Iteration: 1905 Train loss: 1.348620 Train acc: 0.410000\n",
      "Epoch: 636/2000 Iteration: 1910 Train loss: 1.402141 Train acc: 0.350000\n",
      "Epoch: 638/2000 Iteration: 1915 Train loss: 1.292087 Train acc: 0.500000\n",
      "Epoch: 639/2000 Iteration: 1920 Train loss: 1.333273 Train acc: 0.450000\n",
      "Epoch: 641/2000 Iteration: 1925 Train loss: 1.448124 Train acc: 0.330000\n",
      "Epoch: 641/2000 Iteration: 1925 Validation loss: 1.290272 Validation acc: 0.490000\n",
      "Epoch: 643/2000 Iteration: 1930 Train loss: 1.262250 Train acc: 0.500000\n",
      "Epoch: 644/2000 Iteration: 1935 Train loss: 1.250468 Train acc: 0.480000\n",
      "Epoch: 646/2000 Iteration: 1940 Train loss: 1.443393 Train acc: 0.290000\n",
      "Epoch: 648/2000 Iteration: 1945 Train loss: 1.358964 Train acc: 0.490000\n",
      "Epoch: 649/2000 Iteration: 1950 Train loss: 1.382288 Train acc: 0.370000\n",
      "Epoch: 649/2000 Iteration: 1950 Validation loss: 1.288641 Validation acc: 0.490000\n",
      "Epoch: 651/2000 Iteration: 1955 Train loss: 1.455201 Train acc: 0.330000\n",
      "Epoch: 653/2000 Iteration: 1960 Train loss: 1.246401 Train acc: 0.530000\n",
      "Epoch: 654/2000 Iteration: 1965 Train loss: 1.369755 Train acc: 0.360000\n",
      "Epoch: 656/2000 Iteration: 1970 Train loss: 1.427379 Train acc: 0.280000\n",
      "Epoch: 658/2000 Iteration: 1975 Train loss: 1.290096 Train acc: 0.490000\n",
      "Epoch: 658/2000 Iteration: 1975 Validation loss: 1.287253 Validation acc: 0.490000\n",
      "Epoch: 659/2000 Iteration: 1980 Train loss: 1.288481 Train acc: 0.460000\n",
      "Epoch: 661/2000 Iteration: 1985 Train loss: 1.402253 Train acc: 0.380000\n",
      "Epoch: 663/2000 Iteration: 1990 Train loss: 1.284693 Train acc: 0.440000\n",
      "Epoch: 664/2000 Iteration: 1995 Train loss: 1.321516 Train acc: 0.420000\n",
      "Epoch: 666/2000 Iteration: 2000 Train loss: 1.494195 Train acc: 0.330000\n",
      "Epoch: 666/2000 Iteration: 2000 Validation loss: 1.285756 Validation acc: 0.490000\n",
      "Epoch: 668/2000 Iteration: 2005 Train loss: 1.246087 Train acc: 0.480000\n",
      "Epoch: 669/2000 Iteration: 2010 Train loss: 1.339825 Train acc: 0.410000\n",
      "Epoch: 671/2000 Iteration: 2015 Train loss: 1.403635 Train acc: 0.390000\n",
      "Epoch: 673/2000 Iteration: 2020 Train loss: 1.297144 Train acc: 0.520000\n",
      "Epoch: 674/2000 Iteration: 2025 Train loss: 1.368870 Train acc: 0.430000\n",
      "Epoch: 674/2000 Iteration: 2025 Validation loss: 1.284165 Validation acc: 0.490000\n",
      "Epoch: 676/2000 Iteration: 2030 Train loss: 1.399433 Train acc: 0.320000\n",
      "Epoch: 678/2000 Iteration: 2035 Train loss: 1.281079 Train acc: 0.460000\n",
      "Epoch: 679/2000 Iteration: 2040 Train loss: 1.347080 Train acc: 0.380000\n",
      "Epoch: 681/2000 Iteration: 2045 Train loss: 1.411118 Train acc: 0.370000\n",
      "Epoch: 683/2000 Iteration: 2050 Train loss: 1.250737 Train acc: 0.470000\n",
      "Epoch: 683/2000 Iteration: 2050 Validation loss: 1.282546 Validation acc: 0.490000\n",
      "Epoch: 684/2000 Iteration: 2055 Train loss: 1.282197 Train acc: 0.490000\n",
      "Epoch: 686/2000 Iteration: 2060 Train loss: 1.426992 Train acc: 0.360000\n",
      "Epoch: 688/2000 Iteration: 2065 Train loss: 1.295027 Train acc: 0.480000\n",
      "Epoch: 689/2000 Iteration: 2070 Train loss: 1.380377 Train acc: 0.400000\n",
      "Epoch: 691/2000 Iteration: 2075 Train loss: 1.394383 Train acc: 0.360000\n",
      "Epoch: 691/2000 Iteration: 2075 Validation loss: 1.281238 Validation acc: 0.490000\n",
      "Epoch: 693/2000 Iteration: 2080 Train loss: 1.275001 Train acc: 0.490000\n",
      "Epoch: 694/2000 Iteration: 2085 Train loss: 1.370776 Train acc: 0.360000\n",
      "Epoch: 696/2000 Iteration: 2090 Train loss: 1.478139 Train acc: 0.320000\n",
      "Epoch: 698/2000 Iteration: 2095 Train loss: 1.274460 Train acc: 0.510000\n",
      "Epoch: 699/2000 Iteration: 2100 Train loss: 1.323538 Train acc: 0.390000\n",
      "Epoch: 699/2000 Iteration: 2100 Validation loss: 1.280015 Validation acc: 0.490000\n",
      "Epoch: 701/2000 Iteration: 2105 Train loss: 1.388951 Train acc: 0.420000\n",
      "Epoch: 703/2000 Iteration: 2110 Train loss: 1.300303 Train acc: 0.500000\n",
      "Epoch: 704/2000 Iteration: 2115 Train loss: 1.350415 Train acc: 0.370000\n",
      "Epoch: 706/2000 Iteration: 2120 Train loss: 1.423616 Train acc: 0.310000\n",
      "Epoch: 708/2000 Iteration: 2125 Train loss: 1.306844 Train acc: 0.520000\n",
      "Epoch: 708/2000 Iteration: 2125 Validation loss: 1.278919 Validation acc: 0.490000\n",
      "Epoch: 709/2000 Iteration: 2130 Train loss: 1.368250 Train acc: 0.380000\n",
      "Epoch: 711/2000 Iteration: 2135 Train loss: 1.366983 Train acc: 0.360000\n",
      "Epoch: 713/2000 Iteration: 2140 Train loss: 1.330547 Train acc: 0.410000\n",
      "Epoch: 714/2000 Iteration: 2145 Train loss: 1.265542 Train acc: 0.440000\n",
      "Epoch: 716/2000 Iteration: 2150 Train loss: 1.419088 Train acc: 0.360000\n",
      "Epoch: 716/2000 Iteration: 2150 Validation loss: 1.277614 Validation acc: 0.490000\n",
      "Epoch: 718/2000 Iteration: 2155 Train loss: 1.269525 Train acc: 0.520000\n",
      "Epoch: 719/2000 Iteration: 2160 Train loss: 1.308814 Train acc: 0.480000\n",
      "Epoch: 721/2000 Iteration: 2165 Train loss: 1.366997 Train acc: 0.400000\n",
      "Epoch: 723/2000 Iteration: 2170 Train loss: 1.273714 Train acc: 0.540000\n",
      "Epoch: 724/2000 Iteration: 2175 Train loss: 1.345355 Train acc: 0.440000\n",
      "Epoch: 724/2000 Iteration: 2175 Validation loss: 1.276146 Validation acc: 0.490000\n",
      "Epoch: 726/2000 Iteration: 2180 Train loss: 1.421517 Train acc: 0.380000\n",
      "Epoch: 728/2000 Iteration: 2185 Train loss: 1.232259 Train acc: 0.480000\n",
      "Epoch: 729/2000 Iteration: 2190 Train loss: 1.354927 Train acc: 0.370000\n",
      "Epoch: 731/2000 Iteration: 2195 Train loss: 1.429269 Train acc: 0.300000\n",
      "Epoch: 733/2000 Iteration: 2200 Train loss: 1.246533 Train acc: 0.530000\n",
      "Epoch: 733/2000 Iteration: 2200 Validation loss: 1.274778 Validation acc: 0.490000\n",
      "Epoch: 734/2000 Iteration: 2205 Train loss: 1.316413 Train acc: 0.410000\n",
      "Epoch: 736/2000 Iteration: 2210 Train loss: 1.406990 Train acc: 0.350000\n",
      "Epoch: 738/2000 Iteration: 2215 Train loss: 1.241536 Train acc: 0.530000\n",
      "Epoch: 739/2000 Iteration: 2220 Train loss: 1.338035 Train acc: 0.440000\n",
      "Epoch: 741/2000 Iteration: 2225 Train loss: 1.451782 Train acc: 0.270000\n",
      "Epoch: 741/2000 Iteration: 2225 Validation loss: 1.273479 Validation acc: 0.490000\n",
      "Epoch: 743/2000 Iteration: 2230 Train loss: 1.262803 Train acc: 0.530000\n",
      "Epoch: 744/2000 Iteration: 2235 Train loss: 1.347236 Train acc: 0.410000\n",
      "Epoch: 746/2000 Iteration: 2240 Train loss: 1.469883 Train acc: 0.330000\n",
      "Epoch: 748/2000 Iteration: 2245 Train loss: 1.283324 Train acc: 0.450000\n",
      "Epoch: 749/2000 Iteration: 2250 Train loss: 1.394515 Train acc: 0.310000\n",
      "Epoch: 749/2000 Iteration: 2250 Validation loss: 1.272493 Validation acc: 0.490000\n",
      "Epoch: 751/2000 Iteration: 2255 Train loss: 1.405642 Train acc: 0.350000\n",
      "Epoch: 753/2000 Iteration: 2260 Train loss: 1.214635 Train acc: 0.520000\n",
      "Epoch: 754/2000 Iteration: 2265 Train loss: 1.325592 Train acc: 0.430000\n",
      "Epoch: 756/2000 Iteration: 2270 Train loss: 1.468793 Train acc: 0.330000\n",
      "Epoch: 758/2000 Iteration: 2275 Train loss: 1.238327 Train acc: 0.510000\n",
      "Epoch: 758/2000 Iteration: 2275 Validation loss: 1.271466 Validation acc: 0.490000\n",
      "Epoch: 759/2000 Iteration: 2280 Train loss: 1.317245 Train acc: 0.440000\n",
      "Epoch: 761/2000 Iteration: 2285 Train loss: 1.351076 Train acc: 0.410000\n",
      "Epoch: 763/2000 Iteration: 2290 Train loss: 1.273775 Train acc: 0.440000\n",
      "Epoch: 764/2000 Iteration: 2295 Train loss: 1.310623 Train acc: 0.440000\n",
      "Epoch: 766/2000 Iteration: 2300 Train loss: 1.417543 Train acc: 0.310000\n",
      "Epoch: 766/2000 Iteration: 2300 Validation loss: 1.270528 Validation acc: 0.490000\n",
      "Epoch: 768/2000 Iteration: 2305 Train loss: 1.245746 Train acc: 0.510000\n",
      "Epoch: 769/2000 Iteration: 2310 Train loss: 1.311179 Train acc: 0.440000\n",
      "Epoch: 771/2000 Iteration: 2315 Train loss: 1.449066 Train acc: 0.350000\n",
      "Epoch: 773/2000 Iteration: 2320 Train loss: 1.265902 Train acc: 0.470000\n",
      "Epoch: 774/2000 Iteration: 2325 Train loss: 1.284158 Train acc: 0.440000\n",
      "Epoch: 774/2000 Iteration: 2325 Validation loss: 1.269453 Validation acc: 0.490000\n",
      "Epoch: 776/2000 Iteration: 2330 Train loss: 1.430482 Train acc: 0.320000\n",
      "Epoch: 778/2000 Iteration: 2335 Train loss: 1.236395 Train acc: 0.470000\n",
      "Epoch: 779/2000 Iteration: 2340 Train loss: 1.337853 Train acc: 0.320000\n",
      "Epoch: 781/2000 Iteration: 2345 Train loss: 1.499769 Train acc: 0.270000\n",
      "Epoch: 783/2000 Iteration: 2350 Train loss: 1.257873 Train acc: 0.500000\n",
      "Epoch: 783/2000 Iteration: 2350 Validation loss: 1.268164 Validation acc: 0.490000\n",
      "Epoch: 784/2000 Iteration: 2355 Train loss: 1.368830 Train acc: 0.360000\n",
      "Epoch: 786/2000 Iteration: 2360 Train loss: 1.383667 Train acc: 0.360000\n",
      "Epoch: 788/2000 Iteration: 2365 Train loss: 1.253434 Train acc: 0.470000\n",
      "Epoch: 789/2000 Iteration: 2370 Train loss: 1.249248 Train acc: 0.460000\n",
      "Epoch: 791/2000 Iteration: 2375 Train loss: 1.373438 Train acc: 0.360000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 791/2000 Iteration: 2375 Validation loss: 1.266842 Validation acc: 0.490000\n",
      "Epoch: 793/2000 Iteration: 2380 Train loss: 1.249048 Train acc: 0.460000\n",
      "Epoch: 794/2000 Iteration: 2385 Train loss: 1.365304 Train acc: 0.370000\n",
      "Epoch: 796/2000 Iteration: 2390 Train loss: 1.411805 Train acc: 0.290000\n",
      "Epoch: 798/2000 Iteration: 2395 Train loss: 1.275002 Train acc: 0.470000\n",
      "Epoch: 799/2000 Iteration: 2400 Train loss: 1.299722 Train acc: 0.430000\n",
      "Epoch: 799/2000 Iteration: 2400 Validation loss: 1.265896 Validation acc: 0.490000\n",
      "Epoch: 801/2000 Iteration: 2405 Train loss: 1.360845 Train acc: 0.360000\n",
      "Epoch: 803/2000 Iteration: 2410 Train loss: 1.252161 Train acc: 0.490000\n",
      "Epoch: 804/2000 Iteration: 2415 Train loss: 1.379709 Train acc: 0.390000\n",
      "Epoch: 806/2000 Iteration: 2420 Train loss: 1.487442 Train acc: 0.300000\n",
      "Epoch: 808/2000 Iteration: 2425 Train loss: 1.255950 Train acc: 0.520000\n",
      "Epoch: 808/2000 Iteration: 2425 Validation loss: 1.265022 Validation acc: 0.490000\n",
      "Epoch: 809/2000 Iteration: 2430 Train loss: 1.352294 Train acc: 0.420000\n",
      "Epoch: 811/2000 Iteration: 2435 Train loss: 1.423105 Train acc: 0.340000\n",
      "Epoch: 813/2000 Iteration: 2440 Train loss: 1.256306 Train acc: 0.480000\n",
      "Epoch: 814/2000 Iteration: 2445 Train loss: 1.331395 Train acc: 0.370000\n",
      "Epoch: 816/2000 Iteration: 2450 Train loss: 1.369282 Train acc: 0.380000\n",
      "Epoch: 816/2000 Iteration: 2450 Validation loss: 1.264227 Validation acc: 0.490000\n",
      "Epoch: 818/2000 Iteration: 2455 Train loss: 1.209894 Train acc: 0.530000\n",
      "Epoch: 819/2000 Iteration: 2460 Train loss: 1.292970 Train acc: 0.470000\n",
      "Epoch: 821/2000 Iteration: 2465 Train loss: 1.399585 Train acc: 0.330000\n",
      "Epoch: 823/2000 Iteration: 2470 Train loss: 1.267176 Train acc: 0.460000\n",
      "Epoch: 824/2000 Iteration: 2475 Train loss: 1.232503 Train acc: 0.490000\n",
      "Epoch: 824/2000 Iteration: 2475 Validation loss: 1.263257 Validation acc: 0.490000\n",
      "Epoch: 826/2000 Iteration: 2480 Train loss: 1.405235 Train acc: 0.320000\n",
      "Epoch: 828/2000 Iteration: 2485 Train loss: 1.195237 Train acc: 0.530000\n",
      "Epoch: 829/2000 Iteration: 2490 Train loss: 1.340471 Train acc: 0.360000\n",
      "Epoch: 831/2000 Iteration: 2495 Train loss: 1.433912 Train acc: 0.340000\n",
      "Epoch: 833/2000 Iteration: 2500 Train loss: 1.249854 Train acc: 0.530000\n",
      "Epoch: 833/2000 Iteration: 2500 Validation loss: 1.262324 Validation acc: 0.490000\n",
      "Epoch: 834/2000 Iteration: 2505 Train loss: 1.261704 Train acc: 0.460000\n",
      "Epoch: 836/2000 Iteration: 2510 Train loss: 1.452768 Train acc: 0.300000\n",
      "Epoch: 838/2000 Iteration: 2515 Train loss: 1.234465 Train acc: 0.500000\n",
      "Epoch: 839/2000 Iteration: 2520 Train loss: 1.373936 Train acc: 0.390000\n",
      "Epoch: 841/2000 Iteration: 2525 Train loss: 1.416952 Train acc: 0.300000\n",
      "Epoch: 841/2000 Iteration: 2525 Validation loss: 1.261653 Validation acc: 0.490000\n",
      "Epoch: 843/2000 Iteration: 2530 Train loss: 1.229904 Train acc: 0.490000\n",
      "Epoch: 844/2000 Iteration: 2535 Train loss: 1.262885 Train acc: 0.380000\n",
      "Epoch: 846/2000 Iteration: 2540 Train loss: 1.425378 Train acc: 0.370000\n",
      "Epoch: 848/2000 Iteration: 2545 Train loss: 1.281028 Train acc: 0.430000\n",
      "Epoch: 849/2000 Iteration: 2550 Train loss: 1.349067 Train acc: 0.440000\n",
      "Epoch: 849/2000 Iteration: 2550 Validation loss: 1.260808 Validation acc: 0.490000\n",
      "Epoch: 851/2000 Iteration: 2555 Train loss: 1.441232 Train acc: 0.370000\n",
      "Epoch: 853/2000 Iteration: 2560 Train loss: 1.245617 Train acc: 0.470000\n",
      "Epoch: 854/2000 Iteration: 2565 Train loss: 1.354354 Train acc: 0.400000\n",
      "Epoch: 856/2000 Iteration: 2570 Train loss: 1.487850 Train acc: 0.290000\n",
      "Epoch: 858/2000 Iteration: 2575 Train loss: 1.239868 Train acc: 0.510000\n",
      "Epoch: 858/2000 Iteration: 2575 Validation loss: 1.260124 Validation acc: 0.490000\n",
      "Epoch: 859/2000 Iteration: 2580 Train loss: 1.336109 Train acc: 0.370000\n",
      "Epoch: 861/2000 Iteration: 2585 Train loss: 1.428301 Train acc: 0.340000\n",
      "Epoch: 863/2000 Iteration: 2590 Train loss: 1.265037 Train acc: 0.480000\n",
      "Epoch: 864/2000 Iteration: 2595 Train loss: 1.298230 Train acc: 0.400000\n",
      "Epoch: 866/2000 Iteration: 2600 Train loss: 1.432895 Train acc: 0.360000\n",
      "Epoch: 866/2000 Iteration: 2600 Validation loss: 1.259444 Validation acc: 0.490000\n",
      "Epoch: 868/2000 Iteration: 2605 Train loss: 1.260380 Train acc: 0.490000\n",
      "Epoch: 869/2000 Iteration: 2610 Train loss: 1.291719 Train acc: 0.490000\n",
      "Epoch: 871/2000 Iteration: 2615 Train loss: 1.444409 Train acc: 0.330000\n",
      "Epoch: 873/2000 Iteration: 2620 Train loss: 1.233098 Train acc: 0.480000\n",
      "Epoch: 874/2000 Iteration: 2625 Train loss: 1.313385 Train acc: 0.420000\n",
      "Epoch: 874/2000 Iteration: 2625 Validation loss: 1.258768 Validation acc: 0.490000\n",
      "Epoch: 876/2000 Iteration: 2630 Train loss: 1.406306 Train acc: 0.360000\n",
      "Epoch: 878/2000 Iteration: 2635 Train loss: 1.235315 Train acc: 0.530000\n",
      "Epoch: 879/2000 Iteration: 2640 Train loss: 1.300489 Train acc: 0.430000\n",
      "Epoch: 881/2000 Iteration: 2645 Train loss: 1.463843 Train acc: 0.350000\n",
      "Epoch: 883/2000 Iteration: 2650 Train loss: 1.196465 Train acc: 0.520000\n",
      "Epoch: 883/2000 Iteration: 2650 Validation loss: 1.257979 Validation acc: 0.490000\n",
      "Epoch: 884/2000 Iteration: 2655 Train loss: 1.357654 Train acc: 0.390000\n",
      "Epoch: 886/2000 Iteration: 2660 Train loss: 1.428723 Train acc: 0.300000\n",
      "Epoch: 888/2000 Iteration: 2665 Train loss: 1.255119 Train acc: 0.530000\n",
      "Epoch: 889/2000 Iteration: 2670 Train loss: 1.337110 Train acc: 0.380000\n",
      "Epoch: 891/2000 Iteration: 2675 Train loss: 1.454213 Train acc: 0.290000\n",
      "Epoch: 891/2000 Iteration: 2675 Validation loss: 1.257335 Validation acc: 0.490000\n",
      "Epoch: 893/2000 Iteration: 2680 Train loss: 1.252173 Train acc: 0.480000\n",
      "Epoch: 894/2000 Iteration: 2685 Train loss: 1.316800 Train acc: 0.420000\n",
      "Epoch: 896/2000 Iteration: 2690 Train loss: 1.389717 Train acc: 0.380000\n",
      "Epoch: 898/2000 Iteration: 2695 Train loss: 1.229445 Train acc: 0.440000\n",
      "Epoch: 899/2000 Iteration: 2700 Train loss: 1.348796 Train acc: 0.360000\n",
      "Epoch: 899/2000 Iteration: 2700 Validation loss: 1.256600 Validation acc: 0.490000\n",
      "Epoch: 901/2000 Iteration: 2705 Train loss: 1.416063 Train acc: 0.310000\n",
      "Epoch: 903/2000 Iteration: 2710 Train loss: 1.234220 Train acc: 0.520000\n",
      "Epoch: 904/2000 Iteration: 2715 Train loss: 1.329061 Train acc: 0.400000\n",
      "Epoch: 906/2000 Iteration: 2720 Train loss: 1.400843 Train acc: 0.360000\n",
      "Epoch: 908/2000 Iteration: 2725 Train loss: 1.264937 Train acc: 0.490000\n",
      "Epoch: 908/2000 Iteration: 2725 Validation loss: 1.255815 Validation acc: 0.490000\n",
      "Epoch: 909/2000 Iteration: 2730 Train loss: 1.334524 Train acc: 0.390000\n",
      "Epoch: 911/2000 Iteration: 2735 Train loss: 1.471177 Train acc: 0.330000\n",
      "Epoch: 913/2000 Iteration: 2740 Train loss: 1.235365 Train acc: 0.450000\n",
      "Epoch: 914/2000 Iteration: 2745 Train loss: 1.343284 Train acc: 0.450000\n",
      "Epoch: 916/2000 Iteration: 2750 Train loss: 1.412506 Train acc: 0.360000\n",
      "Epoch: 916/2000 Iteration: 2750 Validation loss: 1.255162 Validation acc: 0.490000\n",
      "Epoch: 918/2000 Iteration: 2755 Train loss: 1.234179 Train acc: 0.530000\n",
      "Epoch: 919/2000 Iteration: 2760 Train loss: 1.367472 Train acc: 0.390000\n",
      "Epoch: 921/2000 Iteration: 2765 Train loss: 1.431801 Train acc: 0.350000\n",
      "Epoch: 923/2000 Iteration: 2770 Train loss: 1.230236 Train acc: 0.500000\n",
      "Epoch: 924/2000 Iteration: 2775 Train loss: 1.322017 Train acc: 0.400000\n",
      "Epoch: 924/2000 Iteration: 2775 Validation loss: 1.254565 Validation acc: 0.490000\n",
      "Epoch: 926/2000 Iteration: 2780 Train loss: 1.377056 Train acc: 0.300000\n",
      "Epoch: 928/2000 Iteration: 2785 Train loss: 1.184572 Train acc: 0.540000\n",
      "Epoch: 929/2000 Iteration: 2790 Train loss: 1.288966 Train acc: 0.440000\n",
      "Epoch: 931/2000 Iteration: 2795 Train loss: 1.387445 Train acc: 0.360000\n",
      "Epoch: 933/2000 Iteration: 2800 Train loss: 1.202477 Train acc: 0.560000\n",
      "Epoch: 933/2000 Iteration: 2800 Validation loss: 1.253812 Validation acc: 0.490000\n",
      "Epoch: 934/2000 Iteration: 2805 Train loss: 1.352628 Train acc: 0.400000\n",
      "Epoch: 936/2000 Iteration: 2810 Train loss: 1.410435 Train acc: 0.360000\n",
      "Epoch: 938/2000 Iteration: 2815 Train loss: 1.227994 Train acc: 0.490000\n",
      "Epoch: 939/2000 Iteration: 2820 Train loss: 1.285991 Train acc: 0.400000\n",
      "Epoch: 941/2000 Iteration: 2825 Train loss: 1.496810 Train acc: 0.270000\n",
      "Epoch: 941/2000 Iteration: 2825 Validation loss: 1.253072 Validation acc: 0.490000\n",
      "Epoch: 943/2000 Iteration: 2830 Train loss: 1.213354 Train acc: 0.540000\n",
      "Epoch: 944/2000 Iteration: 2835 Train loss: 1.338716 Train acc: 0.340000\n",
      "Epoch: 946/2000 Iteration: 2840 Train loss: 1.408039 Train acc: 0.360000\n",
      "Epoch: 948/2000 Iteration: 2845 Train loss: 1.268536 Train acc: 0.500000\n",
      "Epoch: 949/2000 Iteration: 2850 Train loss: 1.325832 Train acc: 0.430000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 949/2000 Iteration: 2850 Validation loss: 1.252354 Validation acc: 0.490000\n",
      "Epoch: 951/2000 Iteration: 2855 Train loss: 1.441700 Train acc: 0.270000\n",
      "Epoch: 953/2000 Iteration: 2860 Train loss: 1.252192 Train acc: 0.520000\n",
      "Epoch: 954/2000 Iteration: 2865 Train loss: 1.364917 Train acc: 0.390000\n",
      "Epoch: 956/2000 Iteration: 2870 Train loss: 1.427647 Train acc: 0.290000\n",
      "Epoch: 958/2000 Iteration: 2875 Train loss: 1.221632 Train acc: 0.540000\n",
      "Epoch: 958/2000 Iteration: 2875 Validation loss: 1.251829 Validation acc: 0.490000\n",
      "Epoch: 959/2000 Iteration: 2880 Train loss: 1.372906 Train acc: 0.370000\n",
      "Epoch: 961/2000 Iteration: 2885 Train loss: 1.387040 Train acc: 0.320000\n",
      "Epoch: 963/2000 Iteration: 2890 Train loss: 1.246063 Train acc: 0.470000\n",
      "Epoch: 964/2000 Iteration: 2895 Train loss: 1.251747 Train acc: 0.470000\n",
      "Epoch: 966/2000 Iteration: 2900 Train loss: 1.384251 Train acc: 0.310000\n",
      "Epoch: 966/2000 Iteration: 2900 Validation loss: 1.251220 Validation acc: 0.490000\n",
      "Epoch: 968/2000 Iteration: 2905 Train loss: 1.288612 Train acc: 0.430000\n",
      "Epoch: 969/2000 Iteration: 2910 Train loss: 1.215659 Train acc: 0.440000\n",
      "Epoch: 971/2000 Iteration: 2915 Train loss: 1.350590 Train acc: 0.420000\n",
      "Epoch: 973/2000 Iteration: 2920 Train loss: 1.235394 Train acc: 0.480000\n",
      "Epoch: 974/2000 Iteration: 2925 Train loss: 1.250918 Train acc: 0.430000\n",
      "Epoch: 974/2000 Iteration: 2925 Validation loss: 1.250757 Validation acc: 0.490000\n",
      "Epoch: 976/2000 Iteration: 2930 Train loss: 1.450045 Train acc: 0.340000\n",
      "Epoch: 978/2000 Iteration: 2935 Train loss: 1.215446 Train acc: 0.520000\n",
      "Epoch: 979/2000 Iteration: 2940 Train loss: 1.327269 Train acc: 0.400000\n",
      "Epoch: 981/2000 Iteration: 2945 Train loss: 1.401360 Train acc: 0.340000\n",
      "Epoch: 983/2000 Iteration: 2950 Train loss: 1.246700 Train acc: 0.540000\n",
      "Epoch: 983/2000 Iteration: 2950 Validation loss: 1.250139 Validation acc: 0.490000\n",
      "Epoch: 984/2000 Iteration: 2955 Train loss: 1.274365 Train acc: 0.410000\n",
      "Epoch: 986/2000 Iteration: 2960 Train loss: 1.444296 Train acc: 0.310000\n",
      "Epoch: 988/2000 Iteration: 2965 Train loss: 1.267204 Train acc: 0.440000\n",
      "Epoch: 989/2000 Iteration: 2970 Train loss: 1.259660 Train acc: 0.450000\n",
      "Epoch: 991/2000 Iteration: 2975 Train loss: 1.406735 Train acc: 0.350000\n",
      "Epoch: 991/2000 Iteration: 2975 Validation loss: 1.249687 Validation acc: 0.490000\n",
      "Epoch: 993/2000 Iteration: 2980 Train loss: 1.218208 Train acc: 0.560000\n",
      "Epoch: 994/2000 Iteration: 2985 Train loss: 1.319855 Train acc: 0.420000\n",
      "Epoch: 996/2000 Iteration: 2990 Train loss: 1.454883 Train acc: 0.290000\n",
      "Epoch: 998/2000 Iteration: 2995 Train loss: 1.252546 Train acc: 0.500000\n",
      "Epoch: 999/2000 Iteration: 3000 Train loss: 1.284886 Train acc: 0.410000\n",
      "Epoch: 999/2000 Iteration: 3000 Validation loss: 1.249012 Validation acc: 0.490000\n",
      "Epoch: 1001/2000 Iteration: 3005 Train loss: 1.297389 Train acc: 0.370000\n",
      "Epoch: 1003/2000 Iteration: 3010 Train loss: 1.216882 Train acc: 0.500000\n",
      "Epoch: 1004/2000 Iteration: 3015 Train loss: 1.312657 Train acc: 0.370000\n",
      "Epoch: 1006/2000 Iteration: 3020 Train loss: 1.439473 Train acc: 0.320000\n",
      "Epoch: 1008/2000 Iteration: 3025 Train loss: 1.222484 Train acc: 0.480000\n",
      "Epoch: 1008/2000 Iteration: 3025 Validation loss: 1.248458 Validation acc: 0.490000\n",
      "Epoch: 1009/2000 Iteration: 3030 Train loss: 1.319698 Train acc: 0.410000\n",
      "Epoch: 1011/2000 Iteration: 3035 Train loss: 1.458940 Train acc: 0.330000\n",
      "Epoch: 1013/2000 Iteration: 3040 Train loss: 1.276406 Train acc: 0.380000\n",
      "Epoch: 1014/2000 Iteration: 3045 Train loss: 1.332413 Train acc: 0.420000\n",
      "Epoch: 1016/2000 Iteration: 3050 Train loss: 1.394294 Train acc: 0.310000\n",
      "Epoch: 1016/2000 Iteration: 3050 Validation loss: 1.247853 Validation acc: 0.490000\n",
      "Epoch: 1018/2000 Iteration: 3055 Train loss: 1.252961 Train acc: 0.490000\n",
      "Epoch: 1019/2000 Iteration: 3060 Train loss: 1.294379 Train acc: 0.400000\n",
      "Epoch: 1021/2000 Iteration: 3065 Train loss: 1.396497 Train acc: 0.370000\n",
      "Epoch: 1023/2000 Iteration: 3070 Train loss: 1.214550 Train acc: 0.500000\n",
      "Epoch: 1024/2000 Iteration: 3075 Train loss: 1.280909 Train acc: 0.430000\n",
      "Epoch: 1024/2000 Iteration: 3075 Validation loss: 1.247311 Validation acc: 0.490000\n",
      "Epoch: 1026/2000 Iteration: 3080 Train loss: 1.414901 Train acc: 0.370000\n",
      "Epoch: 1028/2000 Iteration: 3085 Train loss: 1.228602 Train acc: 0.550000\n",
      "Epoch: 1029/2000 Iteration: 3090 Train loss: 1.300161 Train acc: 0.450000\n",
      "Epoch: 1031/2000 Iteration: 3095 Train loss: 1.359876 Train acc: 0.420000\n",
      "Epoch: 1033/2000 Iteration: 3100 Train loss: 1.226541 Train acc: 0.450000\n",
      "Epoch: 1033/2000 Iteration: 3100 Validation loss: 1.246730 Validation acc: 0.490000\n",
      "Epoch: 1034/2000 Iteration: 3105 Train loss: 1.292377 Train acc: 0.440000\n",
      "Epoch: 1036/2000 Iteration: 3110 Train loss: 1.424162 Train acc: 0.320000\n",
      "Epoch: 1038/2000 Iteration: 3115 Train loss: 1.254576 Train acc: 0.480000\n",
      "Epoch: 1039/2000 Iteration: 3120 Train loss: 1.304656 Train acc: 0.460000\n",
      "Epoch: 1041/2000 Iteration: 3125 Train loss: 1.374729 Train acc: 0.320000\n",
      "Epoch: 1041/2000 Iteration: 3125 Validation loss: 1.246227 Validation acc: 0.490000\n",
      "Epoch: 1043/2000 Iteration: 3130 Train loss: 1.279349 Train acc: 0.460000\n",
      "Epoch: 1044/2000 Iteration: 3135 Train loss: 1.348983 Train acc: 0.380000\n",
      "Epoch: 1046/2000 Iteration: 3140 Train loss: 1.375286 Train acc: 0.400000\n",
      "Epoch: 1048/2000 Iteration: 3145 Train loss: 1.268764 Train acc: 0.470000\n",
      "Epoch: 1049/2000 Iteration: 3150 Train loss: 1.287979 Train acc: 0.460000\n",
      "Epoch: 1049/2000 Iteration: 3150 Validation loss: 1.245793 Validation acc: 0.490000\n",
      "Epoch: 1051/2000 Iteration: 3155 Train loss: 1.443505 Train acc: 0.380000\n",
      "Epoch: 1053/2000 Iteration: 3160 Train loss: 1.217961 Train acc: 0.470000\n",
      "Epoch: 1054/2000 Iteration: 3165 Train loss: 1.347700 Train acc: 0.390000\n",
      "Epoch: 1056/2000 Iteration: 3170 Train loss: 1.422101 Train acc: 0.370000\n",
      "Epoch: 1058/2000 Iteration: 3175 Train loss: 1.202410 Train acc: 0.490000\n",
      "Epoch: 1058/2000 Iteration: 3175 Validation loss: 1.245237 Validation acc: 0.490000\n",
      "Epoch: 1059/2000 Iteration: 3180 Train loss: 1.302960 Train acc: 0.410000\n",
      "Epoch: 1061/2000 Iteration: 3185 Train loss: 1.358703 Train acc: 0.370000\n",
      "Epoch: 1063/2000 Iteration: 3190 Train loss: 1.245692 Train acc: 0.500000\n",
      "Epoch: 1064/2000 Iteration: 3195 Train loss: 1.306956 Train acc: 0.390000\n",
      "Epoch: 1066/2000 Iteration: 3200 Train loss: 1.422505 Train acc: 0.340000\n",
      "Epoch: 1066/2000 Iteration: 3200 Validation loss: 1.244818 Validation acc: 0.490000\n",
      "Epoch: 1068/2000 Iteration: 3205 Train loss: 1.252844 Train acc: 0.470000\n",
      "Epoch: 1069/2000 Iteration: 3210 Train loss: 1.304196 Train acc: 0.410000\n",
      "Epoch: 1071/2000 Iteration: 3215 Train loss: 1.371419 Train acc: 0.380000\n",
      "Epoch: 1073/2000 Iteration: 3220 Train loss: 1.197583 Train acc: 0.540000\n",
      "Epoch: 1074/2000 Iteration: 3225 Train loss: 1.313893 Train acc: 0.420000\n",
      "Epoch: 1074/2000 Iteration: 3225 Validation loss: 1.244486 Validation acc: 0.490000\n",
      "Epoch: 1076/2000 Iteration: 3230 Train loss: 1.426373 Train acc: 0.330000\n",
      "Epoch: 1078/2000 Iteration: 3235 Train loss: 1.193995 Train acc: 0.500000\n",
      "Epoch: 1079/2000 Iteration: 3240 Train loss: 1.320670 Train acc: 0.390000\n",
      "Epoch: 1081/2000 Iteration: 3245 Train loss: 1.351471 Train acc: 0.360000\n",
      "Epoch: 1083/2000 Iteration: 3250 Train loss: 1.243363 Train acc: 0.500000\n",
      "Epoch: 1083/2000 Iteration: 3250 Validation loss: 1.244125 Validation acc: 0.490000\n",
      "Epoch: 1084/2000 Iteration: 3255 Train loss: 1.335808 Train acc: 0.400000\n",
      "Epoch: 1086/2000 Iteration: 3260 Train loss: 1.438603 Train acc: 0.310000\n",
      "Epoch: 1088/2000 Iteration: 3265 Train loss: 1.234084 Train acc: 0.530000\n",
      "Epoch: 1089/2000 Iteration: 3270 Train loss: 1.310628 Train acc: 0.380000\n",
      "Epoch: 1091/2000 Iteration: 3275 Train loss: 1.392079 Train acc: 0.290000\n",
      "Epoch: 1091/2000 Iteration: 3275 Validation loss: 1.243783 Validation acc: 0.490000\n",
      "Epoch: 1093/2000 Iteration: 3280 Train loss: 1.243722 Train acc: 0.450000\n",
      "Epoch: 1094/2000 Iteration: 3285 Train loss: 1.316911 Train acc: 0.450000\n",
      "Epoch: 1096/2000 Iteration: 3290 Train loss: 1.411999 Train acc: 0.330000\n",
      "Epoch: 1098/2000 Iteration: 3295 Train loss: 1.222258 Train acc: 0.500000\n",
      "Epoch: 1099/2000 Iteration: 3300 Train loss: 1.284730 Train acc: 0.440000\n",
      "Epoch: 1099/2000 Iteration: 3300 Validation loss: 1.243385 Validation acc: 0.490000\n",
      "Epoch: 1101/2000 Iteration: 3305 Train loss: 1.459784 Train acc: 0.330000\n",
      "Epoch: 1103/2000 Iteration: 3310 Train loss: 1.231581 Train acc: 0.540000\n",
      "Epoch: 1104/2000 Iteration: 3315 Train loss: 1.245267 Train acc: 0.430000\n",
      "Epoch: 1106/2000 Iteration: 3320 Train loss: 1.398024 Train acc: 0.340000\n",
      "Epoch: 1108/2000 Iteration: 3325 Train loss: 1.188785 Train acc: 0.500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1108/2000 Iteration: 3325 Validation loss: 1.242974 Validation acc: 0.490000\n",
      "Epoch: 1109/2000 Iteration: 3330 Train loss: 1.313432 Train acc: 0.480000\n",
      "Epoch: 1111/2000 Iteration: 3335 Train loss: 1.429308 Train acc: 0.360000\n",
      "Epoch: 1113/2000 Iteration: 3340 Train loss: 1.200863 Train acc: 0.550000\n",
      "Epoch: 1114/2000 Iteration: 3345 Train loss: 1.255726 Train acc: 0.370000\n",
      "Epoch: 1116/2000 Iteration: 3350 Train loss: 1.456730 Train acc: 0.270000\n",
      "Epoch: 1116/2000 Iteration: 3350 Validation loss: 1.242543 Validation acc: 0.490000\n",
      "Epoch: 1118/2000 Iteration: 3355 Train loss: 1.178519 Train acc: 0.470000\n",
      "Epoch: 1119/2000 Iteration: 3360 Train loss: 1.293368 Train acc: 0.390000\n",
      "Epoch: 1121/2000 Iteration: 3365 Train loss: 1.412505 Train acc: 0.320000\n",
      "Epoch: 1123/2000 Iteration: 3370 Train loss: 1.255594 Train acc: 0.490000\n",
      "Epoch: 1124/2000 Iteration: 3375 Train loss: 1.318177 Train acc: 0.390000\n",
      "Epoch: 1124/2000 Iteration: 3375 Validation loss: 1.242070 Validation acc: 0.490000\n",
      "Epoch: 1126/2000 Iteration: 3380 Train loss: 1.378994 Train acc: 0.340000\n",
      "Epoch: 1128/2000 Iteration: 3385 Train loss: 1.215614 Train acc: 0.510000\n",
      "Epoch: 1129/2000 Iteration: 3390 Train loss: 1.366360 Train acc: 0.370000\n",
      "Epoch: 1131/2000 Iteration: 3395 Train loss: 1.433052 Train acc: 0.340000\n",
      "Epoch: 1133/2000 Iteration: 3400 Train loss: 1.217370 Train acc: 0.510000\n",
      "Epoch: 1133/2000 Iteration: 3400 Validation loss: 1.241722 Validation acc: 0.490000\n",
      "Epoch: 1134/2000 Iteration: 3405 Train loss: 1.339844 Train acc: 0.400000\n",
      "Epoch: 1136/2000 Iteration: 3410 Train loss: 1.395962 Train acc: 0.390000\n",
      "Epoch: 1138/2000 Iteration: 3415 Train loss: 1.232478 Train acc: 0.490000\n",
      "Epoch: 1139/2000 Iteration: 3420 Train loss: 1.283272 Train acc: 0.460000\n",
      "Epoch: 1141/2000 Iteration: 3425 Train loss: 1.401434 Train acc: 0.360000\n",
      "Epoch: 1141/2000 Iteration: 3425 Validation loss: 1.241455 Validation acc: 0.490000\n",
      "Epoch: 1143/2000 Iteration: 3430 Train loss: 1.197452 Train acc: 0.570000\n",
      "Epoch: 1144/2000 Iteration: 3435 Train loss: 1.295423 Train acc: 0.420000\n",
      "Epoch: 1146/2000 Iteration: 3440 Train loss: 1.365785 Train acc: 0.350000\n",
      "Epoch: 1148/2000 Iteration: 3445 Train loss: 1.227417 Train acc: 0.470000\n",
      "Epoch: 1149/2000 Iteration: 3450 Train loss: 1.272900 Train acc: 0.420000\n",
      "Epoch: 1149/2000 Iteration: 3450 Validation loss: 1.240991 Validation acc: 0.490000\n",
      "Epoch: 1151/2000 Iteration: 3455 Train loss: 1.425440 Train acc: 0.350000\n",
      "Epoch: 1153/2000 Iteration: 3460 Train loss: 1.190793 Train acc: 0.580000\n",
      "Epoch: 1154/2000 Iteration: 3465 Train loss: 1.274992 Train acc: 0.470000\n",
      "Epoch: 1156/2000 Iteration: 3470 Train loss: 1.388052 Train acc: 0.380000\n",
      "Epoch: 1158/2000 Iteration: 3475 Train loss: 1.225934 Train acc: 0.490000\n",
      "Epoch: 1158/2000 Iteration: 3475 Validation loss: 1.240498 Validation acc: 0.490000\n",
      "Epoch: 1159/2000 Iteration: 3480 Train loss: 1.273310 Train acc: 0.450000\n",
      "Epoch: 1161/2000 Iteration: 3485 Train loss: 1.380544 Train acc: 0.340000\n",
      "Epoch: 1163/2000 Iteration: 3490 Train loss: 1.224573 Train acc: 0.500000\n",
      "Epoch: 1164/2000 Iteration: 3495 Train loss: 1.311516 Train acc: 0.410000\n",
      "Epoch: 1166/2000 Iteration: 3500 Train loss: 1.337981 Train acc: 0.370000\n",
      "Epoch: 1166/2000 Iteration: 3500 Validation loss: 1.240057 Validation acc: 0.490000\n",
      "Epoch: 1168/2000 Iteration: 3505 Train loss: 1.253204 Train acc: 0.490000\n",
      "Epoch: 1169/2000 Iteration: 3510 Train loss: 1.271876 Train acc: 0.450000\n",
      "Epoch: 1171/2000 Iteration: 3515 Train loss: 1.343459 Train acc: 0.330000\n",
      "Epoch: 1173/2000 Iteration: 3520 Train loss: 1.231800 Train acc: 0.520000\n",
      "Epoch: 1174/2000 Iteration: 3525 Train loss: 1.285048 Train acc: 0.380000\n",
      "Epoch: 1174/2000 Iteration: 3525 Validation loss: 1.239650 Validation acc: 0.490000\n",
      "Epoch: 1176/2000 Iteration: 3530 Train loss: 1.367624 Train acc: 0.350000\n",
      "Epoch: 1178/2000 Iteration: 3535 Train loss: 1.189943 Train acc: 0.520000\n",
      "Epoch: 1179/2000 Iteration: 3540 Train loss: 1.312490 Train acc: 0.460000\n",
      "Epoch: 1181/2000 Iteration: 3545 Train loss: 1.426619 Train acc: 0.390000\n",
      "Epoch: 1183/2000 Iteration: 3550 Train loss: 1.227843 Train acc: 0.490000\n",
      "Epoch: 1183/2000 Iteration: 3550 Validation loss: 1.239237 Validation acc: 0.490000\n",
      "Epoch: 1184/2000 Iteration: 3555 Train loss: 1.332588 Train acc: 0.330000\n",
      "Epoch: 1186/2000 Iteration: 3560 Train loss: 1.393438 Train acc: 0.360000\n",
      "Epoch: 1188/2000 Iteration: 3565 Train loss: 1.219947 Train acc: 0.460000\n",
      "Epoch: 1189/2000 Iteration: 3570 Train loss: 1.275884 Train acc: 0.440000\n",
      "Epoch: 1191/2000 Iteration: 3575 Train loss: 1.361581 Train acc: 0.380000\n",
      "Epoch: 1191/2000 Iteration: 3575 Validation loss: 1.238907 Validation acc: 0.490000\n",
      "Epoch: 1193/2000 Iteration: 3580 Train loss: 1.222198 Train acc: 0.500000\n",
      "Epoch: 1194/2000 Iteration: 3585 Train loss: 1.268676 Train acc: 0.440000\n",
      "Epoch: 1196/2000 Iteration: 3590 Train loss: 1.423757 Train acc: 0.330000\n",
      "Epoch: 1198/2000 Iteration: 3595 Train loss: 1.250699 Train acc: 0.510000\n",
      "Epoch: 1199/2000 Iteration: 3600 Train loss: 1.327852 Train acc: 0.410000\n",
      "Epoch: 1199/2000 Iteration: 3600 Validation loss: 1.238616 Validation acc: 0.490000\n",
      "Epoch: 1201/2000 Iteration: 3605 Train loss: 1.377189 Train acc: 0.350000\n",
      "Epoch: 1203/2000 Iteration: 3610 Train loss: 1.214155 Train acc: 0.510000\n",
      "Epoch: 1204/2000 Iteration: 3615 Train loss: 1.347816 Train acc: 0.350000\n",
      "Epoch: 1206/2000 Iteration: 3620 Train loss: 1.356453 Train acc: 0.390000\n",
      "Epoch: 1208/2000 Iteration: 3625 Train loss: 1.210579 Train acc: 0.480000\n",
      "Epoch: 1208/2000 Iteration: 3625 Validation loss: 1.238270 Validation acc: 0.490000\n",
      "Epoch: 1209/2000 Iteration: 3630 Train loss: 1.330942 Train acc: 0.440000\n",
      "Epoch: 1211/2000 Iteration: 3635 Train loss: 1.354579 Train acc: 0.380000\n",
      "Epoch: 1213/2000 Iteration: 3640 Train loss: 1.214248 Train acc: 0.520000\n",
      "Epoch: 1214/2000 Iteration: 3645 Train loss: 1.294192 Train acc: 0.450000\n",
      "Epoch: 1216/2000 Iteration: 3650 Train loss: 1.423887 Train acc: 0.360000\n",
      "Epoch: 1216/2000 Iteration: 3650 Validation loss: 1.237841 Validation acc: 0.490000\n",
      "Epoch: 1218/2000 Iteration: 3655 Train loss: 1.182914 Train acc: 0.530000\n",
      "Epoch: 1219/2000 Iteration: 3660 Train loss: 1.303200 Train acc: 0.460000\n",
      "Epoch: 1221/2000 Iteration: 3665 Train loss: 1.399198 Train acc: 0.390000\n",
      "Epoch: 1223/2000 Iteration: 3670 Train loss: 1.232402 Train acc: 0.500000\n",
      "Epoch: 1224/2000 Iteration: 3675 Train loss: 1.238967 Train acc: 0.470000\n",
      "Epoch: 1224/2000 Iteration: 3675 Validation loss: 1.237373 Validation acc: 0.490000\n",
      "Epoch: 1226/2000 Iteration: 3680 Train loss: 1.372774 Train acc: 0.390000\n",
      "Epoch: 1228/2000 Iteration: 3685 Train loss: 1.234468 Train acc: 0.500000\n",
      "Epoch: 1229/2000 Iteration: 3690 Train loss: 1.285313 Train acc: 0.460000\n",
      "Epoch: 1231/2000 Iteration: 3695 Train loss: 1.403183 Train acc: 0.310000\n",
      "Epoch: 1233/2000 Iteration: 3700 Train loss: 1.197955 Train acc: 0.540000\n",
      "Epoch: 1233/2000 Iteration: 3700 Validation loss: 1.236973 Validation acc: 0.490000\n",
      "Epoch: 1234/2000 Iteration: 3705 Train loss: 1.301142 Train acc: 0.440000\n",
      "Epoch: 1236/2000 Iteration: 3710 Train loss: 1.353809 Train acc: 0.380000\n",
      "Epoch: 1238/2000 Iteration: 3715 Train loss: 1.213065 Train acc: 0.480000\n",
      "Epoch: 1239/2000 Iteration: 3720 Train loss: 1.283607 Train acc: 0.430000\n",
      "Epoch: 1241/2000 Iteration: 3725 Train loss: 1.407514 Train acc: 0.320000\n",
      "Epoch: 1241/2000 Iteration: 3725 Validation loss: 1.236600 Validation acc: 0.490000\n",
      "Epoch: 1243/2000 Iteration: 3730 Train loss: 1.214782 Train acc: 0.480000\n",
      "Epoch: 1244/2000 Iteration: 3735 Train loss: 1.271323 Train acc: 0.420000\n",
      "Epoch: 1246/2000 Iteration: 3740 Train loss: 1.395567 Train acc: 0.330000\n",
      "Epoch: 1248/2000 Iteration: 3745 Train loss: 1.247990 Train acc: 0.500000\n",
      "Epoch: 1249/2000 Iteration: 3750 Train loss: 1.289801 Train acc: 0.400000\n",
      "Epoch: 1249/2000 Iteration: 3750 Validation loss: 1.236309 Validation acc: 0.490000\n",
      "Epoch: 1251/2000 Iteration: 3755 Train loss: 1.335816 Train acc: 0.380000\n",
      "Epoch: 1253/2000 Iteration: 3760 Train loss: 1.179826 Train acc: 0.540000\n",
      "Epoch: 1254/2000 Iteration: 3765 Train loss: 1.366632 Train acc: 0.350000\n",
      "Epoch: 1256/2000 Iteration: 3770 Train loss: 1.352617 Train acc: 0.350000\n",
      "Epoch: 1258/2000 Iteration: 3775 Train loss: 1.240626 Train acc: 0.450000\n",
      "Epoch: 1258/2000 Iteration: 3775 Validation loss: 1.236055 Validation acc: 0.490000\n",
      "Epoch: 1259/2000 Iteration: 3780 Train loss: 1.241952 Train acc: 0.450000\n",
      "Epoch: 1261/2000 Iteration: 3785 Train loss: 1.404451 Train acc: 0.330000\n",
      "Epoch: 1263/2000 Iteration: 3790 Train loss: 1.188820 Train acc: 0.530000\n",
      "Epoch: 1264/2000 Iteration: 3795 Train loss: 1.311446 Train acc: 0.410000\n",
      "Epoch: 1266/2000 Iteration: 3800 Train loss: 1.379301 Train acc: 0.380000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1266/2000 Iteration: 3800 Validation loss: 1.235743 Validation acc: 0.490000\n",
      "Epoch: 1268/2000 Iteration: 3805 Train loss: 1.226785 Train acc: 0.500000\n",
      "Epoch: 1269/2000 Iteration: 3810 Train loss: 1.284577 Train acc: 0.440000\n",
      "Epoch: 1271/2000 Iteration: 3815 Train loss: 1.352014 Train acc: 0.350000\n",
      "Epoch: 1273/2000 Iteration: 3820 Train loss: 1.240392 Train acc: 0.530000\n",
      "Epoch: 1274/2000 Iteration: 3825 Train loss: 1.236054 Train acc: 0.410000\n",
      "Epoch: 1274/2000 Iteration: 3825 Validation loss: 1.235417 Validation acc: 0.490000\n",
      "Epoch: 1276/2000 Iteration: 3830 Train loss: 1.341669 Train acc: 0.370000\n",
      "Epoch: 1278/2000 Iteration: 3835 Train loss: 1.215127 Train acc: 0.540000\n",
      "Epoch: 1279/2000 Iteration: 3840 Train loss: 1.267513 Train acc: 0.430000\n",
      "Epoch: 1281/2000 Iteration: 3845 Train loss: 1.448398 Train acc: 0.340000\n",
      "Epoch: 1283/2000 Iteration: 3850 Train loss: 1.177090 Train acc: 0.590000\n",
      "Epoch: 1283/2000 Iteration: 3850 Validation loss: 1.235026 Validation acc: 0.490000\n",
      "Epoch: 1284/2000 Iteration: 3855 Train loss: 1.329301 Train acc: 0.410000\n",
      "Epoch: 1286/2000 Iteration: 3860 Train loss: 1.344340 Train acc: 0.340000\n",
      "Epoch: 1288/2000 Iteration: 3865 Train loss: 1.233782 Train acc: 0.500000\n",
      "Epoch: 1289/2000 Iteration: 3870 Train loss: 1.287448 Train acc: 0.430000\n",
      "Epoch: 1291/2000 Iteration: 3875 Train loss: 1.428495 Train acc: 0.340000\n",
      "Epoch: 1291/2000 Iteration: 3875 Validation loss: 1.234569 Validation acc: 0.490000\n",
      "Epoch: 1293/2000 Iteration: 3880 Train loss: 1.181810 Train acc: 0.530000\n",
      "Epoch: 1294/2000 Iteration: 3885 Train loss: 1.300345 Train acc: 0.460000\n",
      "Epoch: 1296/2000 Iteration: 3890 Train loss: 1.421255 Train acc: 0.350000\n",
      "Epoch: 1298/2000 Iteration: 3895 Train loss: 1.185399 Train acc: 0.500000\n",
      "Epoch: 1299/2000 Iteration: 3900 Train loss: 1.250949 Train acc: 0.450000\n",
      "Epoch: 1299/2000 Iteration: 3900 Validation loss: 1.234187 Validation acc: 0.490000\n",
      "Epoch: 1301/2000 Iteration: 3905 Train loss: 1.452855 Train acc: 0.370000\n",
      "Epoch: 1303/2000 Iteration: 3910 Train loss: 1.233667 Train acc: 0.490000\n",
      "Epoch: 1304/2000 Iteration: 3915 Train loss: 1.280376 Train acc: 0.430000\n",
      "Epoch: 1306/2000 Iteration: 3920 Train loss: 1.342410 Train acc: 0.400000\n",
      "Epoch: 1308/2000 Iteration: 3925 Train loss: 1.207265 Train acc: 0.520000\n",
      "Epoch: 1308/2000 Iteration: 3925 Validation loss: 1.233994 Validation acc: 0.490000\n",
      "Epoch: 1309/2000 Iteration: 3930 Train loss: 1.277670 Train acc: 0.460000\n",
      "Epoch: 1311/2000 Iteration: 3935 Train loss: 1.391273 Train acc: 0.390000\n",
      "Epoch: 1313/2000 Iteration: 3940 Train loss: 1.203961 Train acc: 0.490000\n",
      "Epoch: 1314/2000 Iteration: 3945 Train loss: 1.253049 Train acc: 0.450000\n",
      "Epoch: 1316/2000 Iteration: 3950 Train loss: 1.414764 Train acc: 0.360000\n",
      "Epoch: 1316/2000 Iteration: 3950 Validation loss: 1.233865 Validation acc: 0.490000\n",
      "Epoch: 1318/2000 Iteration: 3955 Train loss: 1.212758 Train acc: 0.510000\n",
      "Epoch: 1319/2000 Iteration: 3960 Train loss: 1.277409 Train acc: 0.410000\n",
      "Epoch: 1321/2000 Iteration: 3965 Train loss: 1.384137 Train acc: 0.350000\n",
      "Epoch: 1323/2000 Iteration: 3970 Train loss: 1.216330 Train acc: 0.510000\n",
      "Epoch: 1324/2000 Iteration: 3975 Train loss: 1.294395 Train acc: 0.410000\n",
      "Epoch: 1324/2000 Iteration: 3975 Validation loss: 1.233627 Validation acc: 0.490000\n",
      "Epoch: 1326/2000 Iteration: 3980 Train loss: 1.374969 Train acc: 0.350000\n",
      "Epoch: 1328/2000 Iteration: 3985 Train loss: 1.232188 Train acc: 0.590000\n",
      "Epoch: 1329/2000 Iteration: 3990 Train loss: 1.314327 Train acc: 0.410000\n",
      "Epoch: 1331/2000 Iteration: 3995 Train loss: 1.314832 Train acc: 0.370000\n",
      "Epoch: 1333/2000 Iteration: 4000 Train loss: 1.193025 Train acc: 0.550000\n",
      "Epoch: 1333/2000 Iteration: 4000 Validation loss: 1.233354 Validation acc: 0.490000\n",
      "Epoch: 1334/2000 Iteration: 4005 Train loss: 1.313929 Train acc: 0.440000\n",
      "Epoch: 1336/2000 Iteration: 4010 Train loss: 1.372073 Train acc: 0.330000\n",
      "Epoch: 1338/2000 Iteration: 4015 Train loss: 1.234890 Train acc: 0.500000\n",
      "Epoch: 1339/2000 Iteration: 4020 Train loss: 1.268881 Train acc: 0.450000\n",
      "Epoch: 1341/2000 Iteration: 4025 Train loss: 1.403542 Train acc: 0.330000\n",
      "Epoch: 1341/2000 Iteration: 4025 Validation loss: 1.233166 Validation acc: 0.490000\n",
      "Epoch: 1343/2000 Iteration: 4030 Train loss: 1.167465 Train acc: 0.500000\n",
      "Epoch: 1344/2000 Iteration: 4035 Train loss: 1.286041 Train acc: 0.460000\n",
      "Epoch: 1346/2000 Iteration: 4040 Train loss: 1.366829 Train acc: 0.310000\n",
      "Epoch: 1348/2000 Iteration: 4045 Train loss: 1.223486 Train acc: 0.520000\n",
      "Epoch: 1349/2000 Iteration: 4050 Train loss: 1.300621 Train acc: 0.400000\n",
      "Epoch: 1349/2000 Iteration: 4050 Validation loss: 1.232970 Validation acc: 0.490000\n",
      "Epoch: 1351/2000 Iteration: 4055 Train loss: 1.434146 Train acc: 0.320000\n",
      "Epoch: 1353/2000 Iteration: 4060 Train loss: 1.223611 Train acc: 0.490000\n",
      "Epoch: 1354/2000 Iteration: 4065 Train loss: 1.318333 Train acc: 0.420000\n",
      "Epoch: 1356/2000 Iteration: 4070 Train loss: 1.439877 Train acc: 0.330000\n",
      "Epoch: 1358/2000 Iteration: 4075 Train loss: 1.206332 Train acc: 0.560000\n",
      "Epoch: 1358/2000 Iteration: 4075 Validation loss: 1.232814 Validation acc: 0.490000\n",
      "Epoch: 1359/2000 Iteration: 4080 Train loss: 1.241767 Train acc: 0.440000\n",
      "Epoch: 1361/2000 Iteration: 4085 Train loss: 1.386795 Train acc: 0.370000\n",
      "Epoch: 1363/2000 Iteration: 4090 Train loss: 1.194325 Train acc: 0.540000\n",
      "Epoch: 1364/2000 Iteration: 4095 Train loss: 1.287282 Train acc: 0.380000\n",
      "Epoch: 1366/2000 Iteration: 4100 Train loss: 1.401770 Train acc: 0.340000\n",
      "Epoch: 1366/2000 Iteration: 4100 Validation loss: 1.232564 Validation acc: 0.490000\n",
      "Epoch: 1368/2000 Iteration: 4105 Train loss: 1.215143 Train acc: 0.470000\n",
      "Epoch: 1369/2000 Iteration: 4110 Train loss: 1.289514 Train acc: 0.480000\n",
      "Epoch: 1371/2000 Iteration: 4115 Train loss: 1.371172 Train acc: 0.360000\n",
      "Epoch: 1373/2000 Iteration: 4120 Train loss: 1.218295 Train acc: 0.500000\n",
      "Epoch: 1374/2000 Iteration: 4125 Train loss: 1.245268 Train acc: 0.470000\n",
      "Epoch: 1374/2000 Iteration: 4125 Validation loss: 1.232354 Validation acc: 0.490000\n",
      "Epoch: 1376/2000 Iteration: 4130 Train loss: 1.385552 Train acc: 0.350000\n",
      "Epoch: 1378/2000 Iteration: 4135 Train loss: 1.197635 Train acc: 0.510000\n",
      "Epoch: 1379/2000 Iteration: 4140 Train loss: 1.274519 Train acc: 0.480000\n",
      "Epoch: 1381/2000 Iteration: 4145 Train loss: 1.349581 Train acc: 0.360000\n",
      "Epoch: 1383/2000 Iteration: 4150 Train loss: 1.171140 Train acc: 0.510000\n",
      "Epoch: 1383/2000 Iteration: 4150 Validation loss: 1.232097 Validation acc: 0.490000\n",
      "Epoch: 1384/2000 Iteration: 4155 Train loss: 1.279199 Train acc: 0.470000\n",
      "Epoch: 1386/2000 Iteration: 4160 Train loss: 1.354265 Train acc: 0.390000\n",
      "Epoch: 1388/2000 Iteration: 4165 Train loss: 1.194304 Train acc: 0.510000\n",
      "Epoch: 1389/2000 Iteration: 4170 Train loss: 1.281492 Train acc: 0.480000\n",
      "Epoch: 1391/2000 Iteration: 4175 Train loss: 1.407649 Train acc: 0.340000\n",
      "Epoch: 1391/2000 Iteration: 4175 Validation loss: 1.231658 Validation acc: 0.490000\n",
      "Epoch: 1393/2000 Iteration: 4180 Train loss: 1.218319 Train acc: 0.550000\n",
      "Epoch: 1394/2000 Iteration: 4185 Train loss: 1.300352 Train acc: 0.450000\n",
      "Epoch: 1396/2000 Iteration: 4190 Train loss: 1.391800 Train acc: 0.300000\n",
      "Epoch: 1398/2000 Iteration: 4195 Train loss: 1.223588 Train acc: 0.510000\n",
      "Epoch: 1399/2000 Iteration: 4200 Train loss: 1.300971 Train acc: 0.460000\n",
      "Epoch: 1399/2000 Iteration: 4200 Validation loss: 1.231367 Validation acc: 0.490000\n",
      "Epoch: 1401/2000 Iteration: 4205 Train loss: 1.384721 Train acc: 0.330000\n",
      "Epoch: 1403/2000 Iteration: 4210 Train loss: 1.201980 Train acc: 0.530000\n",
      "Epoch: 1404/2000 Iteration: 4215 Train loss: 1.257200 Train acc: 0.450000\n",
      "Epoch: 1406/2000 Iteration: 4220 Train loss: 1.360250 Train acc: 0.400000\n",
      "Epoch: 1408/2000 Iteration: 4225 Train loss: 1.202982 Train acc: 0.540000\n",
      "Epoch: 1408/2000 Iteration: 4225 Validation loss: 1.231131 Validation acc: 0.490000\n",
      "Epoch: 1409/2000 Iteration: 4230 Train loss: 1.320107 Train acc: 0.410000\n",
      "Epoch: 1411/2000 Iteration: 4235 Train loss: 1.384163 Train acc: 0.330000\n",
      "Epoch: 1413/2000 Iteration: 4240 Train loss: 1.216241 Train acc: 0.500000\n",
      "Epoch: 1414/2000 Iteration: 4245 Train loss: 1.282629 Train acc: 0.460000\n",
      "Epoch: 1416/2000 Iteration: 4250 Train loss: 1.352183 Train acc: 0.350000\n",
      "Epoch: 1416/2000 Iteration: 4250 Validation loss: 1.230914 Validation acc: 0.490000\n",
      "Epoch: 1418/2000 Iteration: 4255 Train loss: 1.233645 Train acc: 0.480000\n",
      "Epoch: 1419/2000 Iteration: 4260 Train loss: 1.269101 Train acc: 0.420000\n",
      "Epoch: 1421/2000 Iteration: 4265 Train loss: 1.431520 Train acc: 0.370000\n",
      "Epoch: 1423/2000 Iteration: 4270 Train loss: 1.156859 Train acc: 0.490000\n",
      "Epoch: 1424/2000 Iteration: 4275 Train loss: 1.284102 Train acc: 0.460000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1424/2000 Iteration: 4275 Validation loss: 1.230722 Validation acc: 0.490000\n",
      "Epoch: 1426/2000 Iteration: 4280 Train loss: 1.384740 Train acc: 0.390000\n",
      "Epoch: 1428/2000 Iteration: 4285 Train loss: 1.217107 Train acc: 0.470000\n",
      "Epoch: 1429/2000 Iteration: 4290 Train loss: 1.267331 Train acc: 0.440000\n",
      "Epoch: 1431/2000 Iteration: 4295 Train loss: 1.352226 Train acc: 0.410000\n",
      "Epoch: 1433/2000 Iteration: 4300 Train loss: 1.173378 Train acc: 0.530000\n",
      "Epoch: 1433/2000 Iteration: 4300 Validation loss: 1.230550 Validation acc: 0.490000\n",
      "Epoch: 1434/2000 Iteration: 4305 Train loss: 1.299209 Train acc: 0.410000\n",
      "Epoch: 1436/2000 Iteration: 4310 Train loss: 1.383522 Train acc: 0.340000\n",
      "Epoch: 1438/2000 Iteration: 4315 Train loss: 1.214086 Train acc: 0.530000\n",
      "Epoch: 1439/2000 Iteration: 4320 Train loss: 1.293399 Train acc: 0.440000\n",
      "Epoch: 1441/2000 Iteration: 4325 Train loss: 1.341546 Train acc: 0.350000\n",
      "Epoch: 1441/2000 Iteration: 4325 Validation loss: 1.230337 Validation acc: 0.490000\n",
      "Epoch: 1443/2000 Iteration: 4330 Train loss: 1.177709 Train acc: 0.490000\n",
      "Epoch: 1444/2000 Iteration: 4335 Train loss: 1.294049 Train acc: 0.450000\n",
      "Epoch: 1446/2000 Iteration: 4340 Train loss: 1.354355 Train acc: 0.370000\n",
      "Epoch: 1448/2000 Iteration: 4345 Train loss: 1.208251 Train acc: 0.520000\n",
      "Epoch: 1449/2000 Iteration: 4350 Train loss: 1.334229 Train acc: 0.410000\n",
      "Epoch: 1449/2000 Iteration: 4350 Validation loss: 1.230054 Validation acc: 0.490000\n",
      "Epoch: 1451/2000 Iteration: 4355 Train loss: 1.353001 Train acc: 0.370000\n",
      "Epoch: 1453/2000 Iteration: 4360 Train loss: 1.206245 Train acc: 0.530000\n",
      "Epoch: 1454/2000 Iteration: 4365 Train loss: 1.283519 Train acc: 0.440000\n",
      "Epoch: 1456/2000 Iteration: 4370 Train loss: 1.399345 Train acc: 0.410000\n",
      "Epoch: 1458/2000 Iteration: 4375 Train loss: 1.183147 Train acc: 0.530000\n",
      "Epoch: 1458/2000 Iteration: 4375 Validation loss: 1.229709 Validation acc: 0.490000\n",
      "Epoch: 1459/2000 Iteration: 4380 Train loss: 1.306507 Train acc: 0.440000\n",
      "Epoch: 1461/2000 Iteration: 4385 Train loss: 1.388365 Train acc: 0.370000\n",
      "Epoch: 1463/2000 Iteration: 4390 Train loss: 1.190694 Train acc: 0.530000\n",
      "Epoch: 1464/2000 Iteration: 4395 Train loss: 1.256578 Train acc: 0.450000\n",
      "Epoch: 1466/2000 Iteration: 4400 Train loss: 1.395800 Train acc: 0.380000\n",
      "Epoch: 1466/2000 Iteration: 4400 Validation loss: 1.229324 Validation acc: 0.490000\n",
      "Epoch: 1468/2000 Iteration: 4405 Train loss: 1.178459 Train acc: 0.520000\n",
      "Epoch: 1469/2000 Iteration: 4410 Train loss: 1.288212 Train acc: 0.440000\n",
      "Epoch: 1471/2000 Iteration: 4415 Train loss: 1.353361 Train acc: 0.360000\n",
      "Epoch: 1473/2000 Iteration: 4420 Train loss: 1.207419 Train acc: 0.510000\n",
      "Epoch: 1474/2000 Iteration: 4425 Train loss: 1.310946 Train acc: 0.430000\n",
      "Epoch: 1474/2000 Iteration: 4425 Validation loss: 1.229113 Validation acc: 0.490000\n",
      "Epoch: 1476/2000 Iteration: 4430 Train loss: 1.375862 Train acc: 0.360000\n",
      "Epoch: 1478/2000 Iteration: 4435 Train loss: 1.192098 Train acc: 0.520000\n",
      "Epoch: 1479/2000 Iteration: 4440 Train loss: 1.275499 Train acc: 0.470000\n",
      "Epoch: 1481/2000 Iteration: 4445 Train loss: 1.417340 Train acc: 0.340000\n",
      "Epoch: 1483/2000 Iteration: 4450 Train loss: 1.252120 Train acc: 0.550000\n",
      "Epoch: 1483/2000 Iteration: 4450 Validation loss: 1.228910 Validation acc: 0.490000\n",
      "Epoch: 1484/2000 Iteration: 4455 Train loss: 1.313489 Train acc: 0.450000\n",
      "Epoch: 1486/2000 Iteration: 4460 Train loss: 1.383876 Train acc: 0.360000\n",
      "Epoch: 1488/2000 Iteration: 4465 Train loss: 1.240494 Train acc: 0.540000\n",
      "Epoch: 1489/2000 Iteration: 4470 Train loss: 1.300888 Train acc: 0.410000\n",
      "Epoch: 1491/2000 Iteration: 4475 Train loss: 1.401132 Train acc: 0.270000\n",
      "Epoch: 1491/2000 Iteration: 4475 Validation loss: 1.228721 Validation acc: 0.490000\n",
      "Epoch: 1493/2000 Iteration: 4480 Train loss: 1.207766 Train acc: 0.470000\n",
      "Epoch: 1494/2000 Iteration: 4485 Train loss: 1.268217 Train acc: 0.480000\n",
      "Epoch: 1496/2000 Iteration: 4490 Train loss: 1.402811 Train acc: 0.410000\n",
      "Epoch: 1498/2000 Iteration: 4495 Train loss: 1.199149 Train acc: 0.560000\n",
      "Epoch: 1499/2000 Iteration: 4500 Train loss: 1.293388 Train acc: 0.460000\n",
      "Epoch: 1499/2000 Iteration: 4500 Validation loss: 1.228468 Validation acc: 0.490000\n",
      "Epoch: 1501/2000 Iteration: 4505 Train loss: 1.378130 Train acc: 0.390000\n",
      "Epoch: 1503/2000 Iteration: 4510 Train loss: 1.217298 Train acc: 0.510000\n",
      "Epoch: 1504/2000 Iteration: 4515 Train loss: 1.277339 Train acc: 0.450000\n",
      "Epoch: 1506/2000 Iteration: 4520 Train loss: 1.372335 Train acc: 0.350000\n",
      "Epoch: 1508/2000 Iteration: 4525 Train loss: 1.217091 Train acc: 0.490000\n",
      "Epoch: 1508/2000 Iteration: 4525 Validation loss: 1.228217 Validation acc: 0.490000\n",
      "Epoch: 1509/2000 Iteration: 4530 Train loss: 1.319010 Train acc: 0.470000\n",
      "Epoch: 1511/2000 Iteration: 4535 Train loss: 1.344601 Train acc: 0.390000\n",
      "Epoch: 1513/2000 Iteration: 4540 Train loss: 1.172746 Train acc: 0.560000\n",
      "Epoch: 1514/2000 Iteration: 4545 Train loss: 1.321476 Train acc: 0.430000\n",
      "Epoch: 1516/2000 Iteration: 4550 Train loss: 1.369244 Train acc: 0.350000\n",
      "Epoch: 1516/2000 Iteration: 4550 Validation loss: 1.228074 Validation acc: 0.490000\n",
      "Epoch: 1518/2000 Iteration: 4555 Train loss: 1.212285 Train acc: 0.520000\n",
      "Epoch: 1519/2000 Iteration: 4560 Train loss: 1.245874 Train acc: 0.480000\n",
      "Epoch: 1521/2000 Iteration: 4565 Train loss: 1.419527 Train acc: 0.350000\n",
      "Epoch: 1523/2000 Iteration: 4570 Train loss: 1.173644 Train acc: 0.510000\n",
      "Epoch: 1524/2000 Iteration: 4575 Train loss: 1.286892 Train acc: 0.500000\n",
      "Epoch: 1524/2000 Iteration: 4575 Validation loss: 1.227838 Validation acc: 0.490000\n",
      "Epoch: 1526/2000 Iteration: 4580 Train loss: 1.392385 Train acc: 0.350000\n",
      "Epoch: 1528/2000 Iteration: 4585 Train loss: 1.178822 Train acc: 0.560000\n",
      "Epoch: 1529/2000 Iteration: 4590 Train loss: 1.267676 Train acc: 0.500000\n",
      "Epoch: 1531/2000 Iteration: 4595 Train loss: 1.379614 Train acc: 0.310000\n",
      "Epoch: 1533/2000 Iteration: 4600 Train loss: 1.170953 Train acc: 0.550000\n",
      "Epoch: 1533/2000 Iteration: 4600 Validation loss: 1.227587 Validation acc: 0.490000\n",
      "Epoch: 1534/2000 Iteration: 4605 Train loss: 1.275230 Train acc: 0.410000\n",
      "Epoch: 1536/2000 Iteration: 4610 Train loss: 1.390848 Train acc: 0.340000\n",
      "Epoch: 1538/2000 Iteration: 4615 Train loss: 1.195812 Train acc: 0.550000\n",
      "Epoch: 1539/2000 Iteration: 4620 Train loss: 1.301923 Train acc: 0.440000\n",
      "Epoch: 1541/2000 Iteration: 4625 Train loss: 1.346546 Train acc: 0.360000\n",
      "Epoch: 1541/2000 Iteration: 4625 Validation loss: 1.227493 Validation acc: 0.490000\n",
      "Epoch: 1543/2000 Iteration: 4630 Train loss: 1.189864 Train acc: 0.540000\n",
      "Epoch: 1544/2000 Iteration: 4635 Train loss: 1.289213 Train acc: 0.460000\n",
      "Epoch: 1546/2000 Iteration: 4640 Train loss: 1.376340 Train acc: 0.350000\n",
      "Epoch: 1548/2000 Iteration: 4645 Train loss: 1.196546 Train acc: 0.530000\n",
      "Epoch: 1549/2000 Iteration: 4650 Train loss: 1.254031 Train acc: 0.440000\n",
      "Epoch: 1549/2000 Iteration: 4650 Validation loss: 1.227303 Validation acc: 0.490000\n",
      "Epoch: 1551/2000 Iteration: 4655 Train loss: 1.343306 Train acc: 0.380000\n",
      "Epoch: 1553/2000 Iteration: 4660 Train loss: 1.204044 Train acc: 0.520000\n",
      "Epoch: 1554/2000 Iteration: 4665 Train loss: 1.284475 Train acc: 0.460000\n",
      "Epoch: 1556/2000 Iteration: 4670 Train loss: 1.399058 Train acc: 0.360000\n",
      "Epoch: 1558/2000 Iteration: 4675 Train loss: 1.219609 Train acc: 0.530000\n",
      "Epoch: 1558/2000 Iteration: 4675 Validation loss: 1.227143 Validation acc: 0.490000\n",
      "Epoch: 1559/2000 Iteration: 4680 Train loss: 1.283732 Train acc: 0.470000\n",
      "Epoch: 1561/2000 Iteration: 4685 Train loss: 1.365489 Train acc: 0.390000\n",
      "Epoch: 1563/2000 Iteration: 4690 Train loss: 1.174934 Train acc: 0.580000\n",
      "Epoch: 1564/2000 Iteration: 4695 Train loss: 1.265895 Train acc: 0.460000\n",
      "Epoch: 1566/2000 Iteration: 4700 Train loss: 1.363551 Train acc: 0.420000\n",
      "Epoch: 1566/2000 Iteration: 4700 Validation loss: 1.226979 Validation acc: 0.490000\n",
      "Epoch: 1568/2000 Iteration: 4705 Train loss: 1.149328 Train acc: 0.530000\n",
      "Epoch: 1569/2000 Iteration: 4710 Train loss: 1.261028 Train acc: 0.450000\n",
      "Epoch: 1571/2000 Iteration: 4715 Train loss: 1.436656 Train acc: 0.340000\n",
      "Epoch: 1573/2000 Iteration: 4720 Train loss: 1.213626 Train acc: 0.480000\n",
      "Epoch: 1574/2000 Iteration: 4725 Train loss: 1.248119 Train acc: 0.460000\n",
      "Epoch: 1574/2000 Iteration: 4725 Validation loss: 1.226726 Validation acc: 0.490000\n",
      "Epoch: 1576/2000 Iteration: 4730 Train loss: 1.372777 Train acc: 0.380000\n",
      "Epoch: 1578/2000 Iteration: 4735 Train loss: 1.211085 Train acc: 0.510000\n",
      "Epoch: 1579/2000 Iteration: 4740 Train loss: 1.304157 Train acc: 0.440000\n",
      "Epoch: 1581/2000 Iteration: 4745 Train loss: 1.373463 Train acc: 0.360000\n",
      "Epoch: 1583/2000 Iteration: 4750 Train loss: 1.182997 Train acc: 0.540000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1583/2000 Iteration: 4750 Validation loss: 1.226492 Validation acc: 0.490000\n",
      "Epoch: 1584/2000 Iteration: 4755 Train loss: 1.291568 Train acc: 0.460000\n",
      "Epoch: 1586/2000 Iteration: 4760 Train loss: 1.362932 Train acc: 0.330000\n",
      "Epoch: 1588/2000 Iteration: 4765 Train loss: 1.161023 Train acc: 0.530000\n",
      "Epoch: 1589/2000 Iteration: 4770 Train loss: 1.269513 Train acc: 0.480000\n",
      "Epoch: 1591/2000 Iteration: 4775 Train loss: 1.383679 Train acc: 0.360000\n",
      "Epoch: 1591/2000 Iteration: 4775 Validation loss: 1.226259 Validation acc: 0.490000\n",
      "Epoch: 1593/2000 Iteration: 4780 Train loss: 1.170140 Train acc: 0.550000\n",
      "Epoch: 1594/2000 Iteration: 4785 Train loss: 1.283273 Train acc: 0.470000\n",
      "Epoch: 1596/2000 Iteration: 4790 Train loss: 1.404716 Train acc: 0.390000\n",
      "Epoch: 1598/2000 Iteration: 4795 Train loss: 1.185600 Train acc: 0.530000\n",
      "Epoch: 1599/2000 Iteration: 4800 Train loss: 1.271665 Train acc: 0.450000\n",
      "Epoch: 1599/2000 Iteration: 4800 Validation loss: 1.226073 Validation acc: 0.490000\n",
      "Epoch: 1601/2000 Iteration: 4805 Train loss: 1.404069 Train acc: 0.370000\n",
      "Epoch: 1603/2000 Iteration: 4810 Train loss: 1.192352 Train acc: 0.560000\n",
      "Epoch: 1604/2000 Iteration: 4815 Train loss: 1.254444 Train acc: 0.460000\n",
      "Epoch: 1606/2000 Iteration: 4820 Train loss: 1.369959 Train acc: 0.370000\n",
      "Epoch: 1608/2000 Iteration: 4825 Train loss: 1.228163 Train acc: 0.530000\n",
      "Epoch: 1608/2000 Iteration: 4825 Validation loss: 1.225911 Validation acc: 0.490000\n",
      "Epoch: 1609/2000 Iteration: 4830 Train loss: 1.218352 Train acc: 0.490000\n",
      "Epoch: 1611/2000 Iteration: 4835 Train loss: 1.384281 Train acc: 0.370000\n",
      "Epoch: 1613/2000 Iteration: 4840 Train loss: 1.180260 Train acc: 0.510000\n",
      "Epoch: 1614/2000 Iteration: 4845 Train loss: 1.318147 Train acc: 0.440000\n",
      "Epoch: 1616/2000 Iteration: 4850 Train loss: 1.383662 Train acc: 0.320000\n",
      "Epoch: 1616/2000 Iteration: 4850 Validation loss: 1.225671 Validation acc: 0.490000\n",
      "Epoch: 1618/2000 Iteration: 4855 Train loss: 1.177021 Train acc: 0.540000\n",
      "Epoch: 1619/2000 Iteration: 4860 Train loss: 1.295344 Train acc: 0.440000\n",
      "Epoch: 1621/2000 Iteration: 4865 Train loss: 1.341198 Train acc: 0.360000\n",
      "Epoch: 1623/2000 Iteration: 4870 Train loss: 1.177543 Train acc: 0.520000\n",
      "Epoch: 1624/2000 Iteration: 4875 Train loss: 1.300833 Train acc: 0.450000\n",
      "Epoch: 1624/2000 Iteration: 4875 Validation loss: 1.225445 Validation acc: 0.490000\n",
      "Epoch: 1626/2000 Iteration: 4880 Train loss: 1.422995 Train acc: 0.360000\n",
      "Epoch: 1628/2000 Iteration: 4885 Train loss: 1.180310 Train acc: 0.530000\n",
      "Epoch: 1629/2000 Iteration: 4890 Train loss: 1.239815 Train acc: 0.450000\n",
      "Epoch: 1631/2000 Iteration: 4895 Train loss: 1.365074 Train acc: 0.370000\n",
      "Epoch: 1633/2000 Iteration: 4900 Train loss: 1.154880 Train acc: 0.550000\n",
      "Epoch: 1633/2000 Iteration: 4900 Validation loss: 1.225292 Validation acc: 0.490000\n",
      "Epoch: 1634/2000 Iteration: 4905 Train loss: 1.280625 Train acc: 0.450000\n",
      "Epoch: 1636/2000 Iteration: 4910 Train loss: 1.400881 Train acc: 0.380000\n",
      "Epoch: 1638/2000 Iteration: 4915 Train loss: 1.183145 Train acc: 0.540000\n",
      "Epoch: 1639/2000 Iteration: 4920 Train loss: 1.277917 Train acc: 0.460000\n",
      "Epoch: 1641/2000 Iteration: 4925 Train loss: 1.375819 Train acc: 0.380000\n",
      "Epoch: 1641/2000 Iteration: 4925 Validation loss: 1.225056 Validation acc: 0.490000\n",
      "Epoch: 1643/2000 Iteration: 4930 Train loss: 1.203153 Train acc: 0.540000\n",
      "Epoch: 1644/2000 Iteration: 4935 Train loss: 1.180573 Train acc: 0.490000\n",
      "Epoch: 1646/2000 Iteration: 4940 Train loss: 1.418759 Train acc: 0.350000\n",
      "Epoch: 1648/2000 Iteration: 4945 Train loss: 1.193902 Train acc: 0.490000\n",
      "Epoch: 1649/2000 Iteration: 4950 Train loss: 1.306048 Train acc: 0.460000\n",
      "Epoch: 1649/2000 Iteration: 4950 Validation loss: 1.224864 Validation acc: 0.490000\n",
      "Epoch: 1651/2000 Iteration: 4955 Train loss: 1.354822 Train acc: 0.380000\n",
      "Epoch: 1653/2000 Iteration: 4960 Train loss: 1.220697 Train acc: 0.490000\n",
      "Epoch: 1654/2000 Iteration: 4965 Train loss: 1.268602 Train acc: 0.450000\n",
      "Epoch: 1656/2000 Iteration: 4970 Train loss: 1.356822 Train acc: 0.370000\n",
      "Epoch: 1658/2000 Iteration: 4975 Train loss: 1.221110 Train acc: 0.540000\n",
      "Epoch: 1658/2000 Iteration: 4975 Validation loss: 1.224689 Validation acc: 0.490000\n",
      "Epoch: 1659/2000 Iteration: 4980 Train loss: 1.244059 Train acc: 0.470000\n",
      "Epoch: 1661/2000 Iteration: 4985 Train loss: 1.345640 Train acc: 0.370000\n",
      "Epoch: 1663/2000 Iteration: 4990 Train loss: 1.217523 Train acc: 0.530000\n",
      "Epoch: 1664/2000 Iteration: 4995 Train loss: 1.249860 Train acc: 0.460000\n",
      "Epoch: 1666/2000 Iteration: 5000 Train loss: 1.363698 Train acc: 0.360000\n",
      "Epoch: 1666/2000 Iteration: 5000 Validation loss: 1.224502 Validation acc: 0.490000\n",
      "Epoch: 1668/2000 Iteration: 5005 Train loss: 1.156248 Train acc: 0.550000\n",
      "Epoch: 1669/2000 Iteration: 5010 Train loss: 1.271548 Train acc: 0.450000\n",
      "Epoch: 1671/2000 Iteration: 5015 Train loss: 1.387965 Train acc: 0.370000\n",
      "Epoch: 1673/2000 Iteration: 5020 Train loss: 1.178221 Train acc: 0.540000\n",
      "Epoch: 1674/2000 Iteration: 5025 Train loss: 1.260680 Train acc: 0.420000\n",
      "Epoch: 1674/2000 Iteration: 5025 Validation loss: 1.224336 Validation acc: 0.490000\n",
      "Epoch: 1676/2000 Iteration: 5030 Train loss: 1.372127 Train acc: 0.340000\n",
      "Epoch: 1678/2000 Iteration: 5035 Train loss: 1.188576 Train acc: 0.530000\n",
      "Epoch: 1679/2000 Iteration: 5040 Train loss: 1.245988 Train acc: 0.480000\n",
      "Epoch: 1681/2000 Iteration: 5045 Train loss: 1.394284 Train acc: 0.390000\n",
      "Epoch: 1683/2000 Iteration: 5050 Train loss: 1.214246 Train acc: 0.540000\n",
      "Epoch: 1683/2000 Iteration: 5050 Validation loss: 1.224055 Validation acc: 0.490000\n",
      "Epoch: 1684/2000 Iteration: 5055 Train loss: 1.233265 Train acc: 0.440000\n",
      "Epoch: 1686/2000 Iteration: 5060 Train loss: 1.351385 Train acc: 0.350000\n",
      "Epoch: 1688/2000 Iteration: 5065 Train loss: 1.162011 Train acc: 0.530000\n",
      "Epoch: 1689/2000 Iteration: 5070 Train loss: 1.285743 Train acc: 0.460000\n",
      "Epoch: 1691/2000 Iteration: 5075 Train loss: 1.345543 Train acc: 0.380000\n",
      "Epoch: 1691/2000 Iteration: 5075 Validation loss: 1.223844 Validation acc: 0.490000\n",
      "Epoch: 1693/2000 Iteration: 5080 Train loss: 1.198123 Train acc: 0.530000\n",
      "Epoch: 1694/2000 Iteration: 5085 Train loss: 1.267699 Train acc: 0.480000\n",
      "Epoch: 1696/2000 Iteration: 5090 Train loss: 1.392403 Train acc: 0.360000\n",
      "Epoch: 1698/2000 Iteration: 5095 Train loss: 1.240255 Train acc: 0.530000\n",
      "Epoch: 1699/2000 Iteration: 5100 Train loss: 1.261244 Train acc: 0.450000\n",
      "Epoch: 1699/2000 Iteration: 5100 Validation loss: 1.223661 Validation acc: 0.490000\n",
      "Epoch: 1701/2000 Iteration: 5105 Train loss: 1.370257 Train acc: 0.340000\n",
      "Epoch: 1703/2000 Iteration: 5110 Train loss: 1.196811 Train acc: 0.560000\n",
      "Epoch: 1704/2000 Iteration: 5115 Train loss: 1.270424 Train acc: 0.460000\n",
      "Epoch: 1706/2000 Iteration: 5120 Train loss: 1.354489 Train acc: 0.370000\n",
      "Epoch: 1708/2000 Iteration: 5125 Train loss: 1.171052 Train acc: 0.570000\n",
      "Epoch: 1708/2000 Iteration: 5125 Validation loss: 1.223484 Validation acc: 0.490000\n",
      "Epoch: 1709/2000 Iteration: 5130 Train loss: 1.244884 Train acc: 0.490000\n",
      "Epoch: 1711/2000 Iteration: 5135 Train loss: 1.383749 Train acc: 0.360000\n",
      "Epoch: 1713/2000 Iteration: 5140 Train loss: 1.132833 Train acc: 0.550000\n",
      "Epoch: 1714/2000 Iteration: 5145 Train loss: 1.281493 Train acc: 0.450000\n",
      "Epoch: 1716/2000 Iteration: 5150 Train loss: 1.404348 Train acc: 0.320000\n",
      "Epoch: 1716/2000 Iteration: 5150 Validation loss: 1.223328 Validation acc: 0.490000\n",
      "Epoch: 1718/2000 Iteration: 5155 Train loss: 1.168321 Train acc: 0.540000\n",
      "Epoch: 1719/2000 Iteration: 5160 Train loss: 1.297929 Train acc: 0.460000\n",
      "Epoch: 1721/2000 Iteration: 5165 Train loss: 1.364131 Train acc: 0.380000\n",
      "Epoch: 1723/2000 Iteration: 5170 Train loss: 1.189047 Train acc: 0.510000\n",
      "Epoch: 1724/2000 Iteration: 5175 Train loss: 1.284754 Train acc: 0.460000\n",
      "Epoch: 1724/2000 Iteration: 5175 Validation loss: 1.223108 Validation acc: 0.490000\n",
      "Epoch: 1726/2000 Iteration: 5180 Train loss: 1.350809 Train acc: 0.360000\n",
      "Epoch: 1728/2000 Iteration: 5185 Train loss: 1.195434 Train acc: 0.530000\n",
      "Epoch: 1729/2000 Iteration: 5190 Train loss: 1.313990 Train acc: 0.440000\n",
      "Epoch: 1731/2000 Iteration: 5195 Train loss: 1.387512 Train acc: 0.360000\n",
      "Epoch: 1733/2000 Iteration: 5200 Train loss: 1.204672 Train acc: 0.510000\n",
      "Epoch: 1733/2000 Iteration: 5200 Validation loss: 1.222964 Validation acc: 0.490000\n",
      "Epoch: 1734/2000 Iteration: 5205 Train loss: 1.248354 Train acc: 0.450000\n",
      "Epoch: 1736/2000 Iteration: 5210 Train loss: 1.368829 Train acc: 0.380000\n",
      "Epoch: 1738/2000 Iteration: 5215 Train loss: 1.219725 Train acc: 0.520000\n",
      "Epoch: 1739/2000 Iteration: 5220 Train loss: 1.318055 Train acc: 0.410000\n",
      "Epoch: 1741/2000 Iteration: 5225 Train loss: 1.371278 Train acc: 0.350000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1741/2000 Iteration: 5225 Validation loss: 1.222850 Validation acc: 0.490000\n",
      "Epoch: 1743/2000 Iteration: 5230 Train loss: 1.181624 Train acc: 0.500000\n",
      "Epoch: 1744/2000 Iteration: 5235 Train loss: 1.259139 Train acc: 0.440000\n",
      "Epoch: 1746/2000 Iteration: 5240 Train loss: 1.350317 Train acc: 0.390000\n",
      "Epoch: 1748/2000 Iteration: 5245 Train loss: 1.202867 Train acc: 0.550000\n",
      "Epoch: 1749/2000 Iteration: 5250 Train loss: 1.259274 Train acc: 0.490000\n",
      "Epoch: 1749/2000 Iteration: 5250 Validation loss: 1.222757 Validation acc: 0.490000\n",
      "Epoch: 1751/2000 Iteration: 5255 Train loss: 1.369904 Train acc: 0.390000\n",
      "Epoch: 1753/2000 Iteration: 5260 Train loss: 1.174076 Train acc: 0.550000\n",
      "Epoch: 1754/2000 Iteration: 5265 Train loss: 1.303725 Train acc: 0.450000\n",
      "Epoch: 1756/2000 Iteration: 5270 Train loss: 1.407119 Train acc: 0.350000\n",
      "Epoch: 1758/2000 Iteration: 5275 Train loss: 1.157192 Train acc: 0.560000\n",
      "Epoch: 1758/2000 Iteration: 5275 Validation loss: 1.222617 Validation acc: 0.490000\n",
      "Epoch: 1759/2000 Iteration: 5280 Train loss: 1.247154 Train acc: 0.460000\n",
      "Epoch: 1761/2000 Iteration: 5285 Train loss: 1.365185 Train acc: 0.380000\n",
      "Epoch: 1763/2000 Iteration: 5290 Train loss: 1.188593 Train acc: 0.490000\n",
      "Epoch: 1764/2000 Iteration: 5295 Train loss: 1.275648 Train acc: 0.470000\n",
      "Epoch: 1766/2000 Iteration: 5300 Train loss: 1.347043 Train acc: 0.360000\n",
      "Epoch: 1766/2000 Iteration: 5300 Validation loss: 1.222443 Validation acc: 0.490000\n",
      "Epoch: 1768/2000 Iteration: 5305 Train loss: 1.219331 Train acc: 0.510000\n",
      "Epoch: 1769/2000 Iteration: 5310 Train loss: 1.289500 Train acc: 0.470000\n",
      "Epoch: 1771/2000 Iteration: 5315 Train loss: 1.344536 Train acc: 0.370000\n",
      "Epoch: 1773/2000 Iteration: 5320 Train loss: 1.213962 Train acc: 0.530000\n",
      "Epoch: 1774/2000 Iteration: 5325 Train loss: 1.249596 Train acc: 0.480000\n",
      "Epoch: 1774/2000 Iteration: 5325 Validation loss: 1.222258 Validation acc: 0.490000\n",
      "Epoch: 1776/2000 Iteration: 5330 Train loss: 1.362859 Train acc: 0.340000\n",
      "Epoch: 1778/2000 Iteration: 5335 Train loss: 1.200883 Train acc: 0.480000\n",
      "Epoch: 1779/2000 Iteration: 5340 Train loss: 1.264351 Train acc: 0.430000\n",
      "Epoch: 1781/2000 Iteration: 5345 Train loss: 1.333362 Train acc: 0.350000\n",
      "Epoch: 1783/2000 Iteration: 5350 Train loss: 1.156634 Train acc: 0.520000\n",
      "Epoch: 1783/2000 Iteration: 5350 Validation loss: 1.222096 Validation acc: 0.490000\n",
      "Epoch: 1784/2000 Iteration: 5355 Train loss: 1.251794 Train acc: 0.460000\n",
      "Epoch: 1786/2000 Iteration: 5360 Train loss: 1.345073 Train acc: 0.390000\n",
      "Epoch: 1788/2000 Iteration: 5365 Train loss: 1.205292 Train acc: 0.530000\n",
      "Epoch: 1789/2000 Iteration: 5370 Train loss: 1.257977 Train acc: 0.450000\n",
      "Epoch: 1791/2000 Iteration: 5375 Train loss: 1.375742 Train acc: 0.400000\n",
      "Epoch: 1791/2000 Iteration: 5375 Validation loss: 1.221849 Validation acc: 0.490000\n",
      "Epoch: 1793/2000 Iteration: 5380 Train loss: 1.162555 Train acc: 0.520000\n",
      "Epoch: 1794/2000 Iteration: 5385 Train loss: 1.302379 Train acc: 0.470000\n",
      "Epoch: 1796/2000 Iteration: 5390 Train loss: 1.363731 Train acc: 0.350000\n",
      "Epoch: 1798/2000 Iteration: 5395 Train loss: 1.171859 Train acc: 0.510000\n",
      "Epoch: 1799/2000 Iteration: 5400 Train loss: 1.294156 Train acc: 0.440000\n",
      "Epoch: 1799/2000 Iteration: 5400 Validation loss: 1.221723 Validation acc: 0.490000\n",
      "Epoch: 1801/2000 Iteration: 5405 Train loss: 1.350098 Train acc: 0.390000\n",
      "Epoch: 1803/2000 Iteration: 5410 Train loss: 1.195135 Train acc: 0.520000\n",
      "Epoch: 1804/2000 Iteration: 5415 Train loss: 1.236847 Train acc: 0.480000\n",
      "Epoch: 1806/2000 Iteration: 5420 Train loss: 1.364891 Train acc: 0.370000\n",
      "Epoch: 1808/2000 Iteration: 5425 Train loss: 1.213876 Train acc: 0.550000\n",
      "Epoch: 1808/2000 Iteration: 5425 Validation loss: 1.221544 Validation acc: 0.490000\n",
      "Epoch: 1809/2000 Iteration: 5430 Train loss: 1.258431 Train acc: 0.490000\n",
      "Epoch: 1811/2000 Iteration: 5435 Train loss: 1.373150 Train acc: 0.380000\n",
      "Epoch: 1813/2000 Iteration: 5440 Train loss: 1.215768 Train acc: 0.560000\n",
      "Epoch: 1814/2000 Iteration: 5445 Train loss: 1.279774 Train acc: 0.460000\n",
      "Epoch: 1816/2000 Iteration: 5450 Train loss: 1.369319 Train acc: 0.370000\n",
      "Epoch: 1816/2000 Iteration: 5450 Validation loss: 1.221419 Validation acc: 0.490000\n",
      "Epoch: 1818/2000 Iteration: 5455 Train loss: 1.177778 Train acc: 0.530000\n",
      "Epoch: 1819/2000 Iteration: 5460 Train loss: 1.252919 Train acc: 0.470000\n",
      "Epoch: 1821/2000 Iteration: 5465 Train loss: 1.397649 Train acc: 0.370000\n",
      "Epoch: 1823/2000 Iteration: 5470 Train loss: 1.170816 Train acc: 0.550000\n",
      "Epoch: 1824/2000 Iteration: 5475 Train loss: 1.242366 Train acc: 0.470000\n",
      "Epoch: 1824/2000 Iteration: 5475 Validation loss: 1.221245 Validation acc: 0.490000\n",
      "Epoch: 1826/2000 Iteration: 5480 Train loss: 1.369756 Train acc: 0.380000\n",
      "Epoch: 1828/2000 Iteration: 5485 Train loss: 1.203336 Train acc: 0.520000\n",
      "Epoch: 1829/2000 Iteration: 5490 Train loss: 1.291687 Train acc: 0.430000\n",
      "Epoch: 1831/2000 Iteration: 5495 Train loss: 1.333713 Train acc: 0.390000\n",
      "Epoch: 1833/2000 Iteration: 5500 Train loss: 1.172948 Train acc: 0.540000\n",
      "Epoch: 1833/2000 Iteration: 5500 Validation loss: 1.221064 Validation acc: 0.490000\n",
      "Epoch: 1834/2000 Iteration: 5505 Train loss: 1.244622 Train acc: 0.470000\n",
      "Epoch: 1836/2000 Iteration: 5510 Train loss: 1.388098 Train acc: 0.350000\n",
      "Epoch: 1838/2000 Iteration: 5515 Train loss: 1.178998 Train acc: 0.560000\n",
      "Epoch: 1839/2000 Iteration: 5520 Train loss: 1.264697 Train acc: 0.460000\n",
      "Epoch: 1841/2000 Iteration: 5525 Train loss: 1.372663 Train acc: 0.350000\n",
      "Epoch: 1841/2000 Iteration: 5525 Validation loss: 1.220944 Validation acc: 0.490000\n",
      "Epoch: 1843/2000 Iteration: 5530 Train loss: 1.214782 Train acc: 0.500000\n",
      "Epoch: 1844/2000 Iteration: 5535 Train loss: 1.260043 Train acc: 0.480000\n",
      "Epoch: 1846/2000 Iteration: 5540 Train loss: 1.377890 Train acc: 0.370000\n",
      "Epoch: 1848/2000 Iteration: 5545 Train loss: 1.187431 Train acc: 0.550000\n",
      "Epoch: 1849/2000 Iteration: 5550 Train loss: 1.262632 Train acc: 0.450000\n",
      "Epoch: 1849/2000 Iteration: 5550 Validation loss: 1.220852 Validation acc: 0.490000\n",
      "Epoch: 1851/2000 Iteration: 5555 Train loss: 1.350194 Train acc: 0.390000\n",
      "Epoch: 1853/2000 Iteration: 5560 Train loss: 1.191576 Train acc: 0.510000\n",
      "Epoch: 1854/2000 Iteration: 5565 Train loss: 1.270331 Train acc: 0.460000\n",
      "Epoch: 1856/2000 Iteration: 5570 Train loss: 1.375731 Train acc: 0.350000\n",
      "Epoch: 1858/2000 Iteration: 5575 Train loss: 1.189231 Train acc: 0.560000\n",
      "Epoch: 1858/2000 Iteration: 5575 Validation loss: 1.220678 Validation acc: 0.490000\n",
      "Epoch: 1859/2000 Iteration: 5580 Train loss: 1.264894 Train acc: 0.470000\n",
      "Epoch: 1861/2000 Iteration: 5585 Train loss: 1.366202 Train acc: 0.380000\n",
      "Epoch: 1863/2000 Iteration: 5590 Train loss: 1.203742 Train acc: 0.530000\n",
      "Epoch: 1864/2000 Iteration: 5595 Train loss: 1.331366 Train acc: 0.500000\n",
      "Epoch: 1866/2000 Iteration: 5600 Train loss: 1.345626 Train acc: 0.350000\n",
      "Epoch: 1866/2000 Iteration: 5600 Validation loss: 1.220546 Validation acc: 0.490000\n",
      "Epoch: 1868/2000 Iteration: 5605 Train loss: 1.152000 Train acc: 0.510000\n",
      "Epoch: 1869/2000 Iteration: 5610 Train loss: 1.307971 Train acc: 0.460000\n",
      "Epoch: 1871/2000 Iteration: 5615 Train loss: 1.359812 Train acc: 0.370000\n",
      "Epoch: 1873/2000 Iteration: 5620 Train loss: 1.178444 Train acc: 0.520000\n",
      "Epoch: 1874/2000 Iteration: 5625 Train loss: 1.269054 Train acc: 0.470000\n",
      "Epoch: 1874/2000 Iteration: 5625 Validation loss: 1.220376 Validation acc: 0.490000\n",
      "Epoch: 1876/2000 Iteration: 5630 Train loss: 1.363130 Train acc: 0.380000\n",
      "Epoch: 1878/2000 Iteration: 5635 Train loss: 1.163721 Train acc: 0.550000\n",
      "Epoch: 1879/2000 Iteration: 5640 Train loss: 1.257909 Train acc: 0.450000\n",
      "Epoch: 1881/2000 Iteration: 5645 Train loss: 1.371637 Train acc: 0.330000\n",
      "Epoch: 1883/2000 Iteration: 5650 Train loss: 1.173189 Train acc: 0.540000\n",
      "Epoch: 1883/2000 Iteration: 5650 Validation loss: 1.220189 Validation acc: 0.490000\n",
      "Epoch: 1884/2000 Iteration: 5655 Train loss: 1.230107 Train acc: 0.490000\n",
      "Epoch: 1886/2000 Iteration: 5660 Train loss: 1.380228 Train acc: 0.370000\n",
      "Epoch: 1888/2000 Iteration: 5665 Train loss: 1.173050 Train acc: 0.520000\n",
      "Epoch: 1889/2000 Iteration: 5670 Train loss: 1.260758 Train acc: 0.450000\n",
      "Epoch: 1891/2000 Iteration: 5675 Train loss: 1.367571 Train acc: 0.410000\n",
      "Epoch: 1891/2000 Iteration: 5675 Validation loss: 1.220061 Validation acc: 0.490000\n",
      "Epoch: 1893/2000 Iteration: 5680 Train loss: 1.155232 Train acc: 0.540000\n",
      "Epoch: 1894/2000 Iteration: 5685 Train loss: 1.253297 Train acc: 0.470000\n",
      "Epoch: 1896/2000 Iteration: 5690 Train loss: 1.346671 Train acc: 0.370000\n",
      "Epoch: 1898/2000 Iteration: 5695 Train loss: 1.191899 Train acc: 0.520000\n",
      "Epoch: 1899/2000 Iteration: 5700 Train loss: 1.280410 Train acc: 0.430000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1899/2000 Iteration: 5700 Validation loss: 1.219974 Validation acc: 0.490000\n",
      "Epoch: 1901/2000 Iteration: 5705 Train loss: 1.333507 Train acc: 0.370000\n",
      "Epoch: 1903/2000 Iteration: 5710 Train loss: 1.197294 Train acc: 0.540000\n",
      "Epoch: 1904/2000 Iteration: 5715 Train loss: 1.227516 Train acc: 0.480000\n",
      "Epoch: 1906/2000 Iteration: 5720 Train loss: 1.397007 Train acc: 0.370000\n",
      "Epoch: 1908/2000 Iteration: 5725 Train loss: 1.157150 Train acc: 0.540000\n",
      "Epoch: 1908/2000 Iteration: 5725 Validation loss: 1.219867 Validation acc: 0.490000\n",
      "Epoch: 1909/2000 Iteration: 5730 Train loss: 1.272995 Train acc: 0.470000\n",
      "Epoch: 1911/2000 Iteration: 5735 Train loss: 1.382472 Train acc: 0.330000\n",
      "Epoch: 1913/2000 Iteration: 5740 Train loss: 1.209951 Train acc: 0.520000\n",
      "Epoch: 1914/2000 Iteration: 5745 Train loss: 1.241464 Train acc: 0.460000\n",
      "Epoch: 1916/2000 Iteration: 5750 Train loss: 1.361246 Train acc: 0.380000\n",
      "Epoch: 1916/2000 Iteration: 5750 Validation loss: 1.219657 Validation acc: 0.490000\n",
      "Epoch: 1918/2000 Iteration: 5755 Train loss: 1.187370 Train acc: 0.520000\n",
      "Epoch: 1919/2000 Iteration: 5760 Train loss: 1.238042 Train acc: 0.490000\n",
      "Epoch: 1921/2000 Iteration: 5765 Train loss: 1.361682 Train acc: 0.340000\n",
      "Epoch: 1923/2000 Iteration: 5770 Train loss: 1.225645 Train acc: 0.490000\n",
      "Epoch: 1924/2000 Iteration: 5775 Train loss: 1.319362 Train acc: 0.440000\n",
      "Epoch: 1924/2000 Iteration: 5775 Validation loss: 1.219500 Validation acc: 0.490000\n",
      "Epoch: 1926/2000 Iteration: 5780 Train loss: 1.358787 Train acc: 0.380000\n",
      "Epoch: 1928/2000 Iteration: 5785 Train loss: 1.207098 Train acc: 0.530000\n",
      "Epoch: 1929/2000 Iteration: 5790 Train loss: 1.266901 Train acc: 0.470000\n",
      "Epoch: 1931/2000 Iteration: 5795 Train loss: 1.346807 Train acc: 0.360000\n",
      "Epoch: 1933/2000 Iteration: 5800 Train loss: 1.174706 Train acc: 0.560000\n",
      "Epoch: 1933/2000 Iteration: 5800 Validation loss: 1.219302 Validation acc: 0.490000\n",
      "Epoch: 1934/2000 Iteration: 5805 Train loss: 1.257543 Train acc: 0.470000\n",
      "Epoch: 1936/2000 Iteration: 5810 Train loss: 1.381752 Train acc: 0.360000\n",
      "Epoch: 1938/2000 Iteration: 5815 Train loss: 1.182780 Train acc: 0.520000\n",
      "Epoch: 1939/2000 Iteration: 5820 Train loss: 1.262731 Train acc: 0.440000\n",
      "Epoch: 1941/2000 Iteration: 5825 Train loss: 1.389012 Train acc: 0.390000\n",
      "Epoch: 1941/2000 Iteration: 5825 Validation loss: 1.219178 Validation acc: 0.490000\n",
      "Epoch: 1943/2000 Iteration: 5830 Train loss: 1.175817 Train acc: 0.560000\n",
      "Epoch: 1944/2000 Iteration: 5835 Train loss: 1.241274 Train acc: 0.480000\n",
      "Epoch: 1946/2000 Iteration: 5840 Train loss: 1.361755 Train acc: 0.360000\n",
      "Epoch: 1948/2000 Iteration: 5845 Train loss: 1.169478 Train acc: 0.510000\n",
      "Epoch: 1949/2000 Iteration: 5850 Train loss: 1.268137 Train acc: 0.450000\n",
      "Epoch: 1949/2000 Iteration: 5850 Validation loss: 1.219093 Validation acc: 0.490000\n",
      "Epoch: 1951/2000 Iteration: 5855 Train loss: 1.385861 Train acc: 0.390000\n",
      "Epoch: 1953/2000 Iteration: 5860 Train loss: 1.220106 Train acc: 0.550000\n",
      "Epoch: 1954/2000 Iteration: 5865 Train loss: 1.269105 Train acc: 0.490000\n",
      "Epoch: 1956/2000 Iteration: 5870 Train loss: 1.375885 Train acc: 0.340000\n",
      "Epoch: 1958/2000 Iteration: 5875 Train loss: 1.184960 Train acc: 0.550000\n",
      "Epoch: 1958/2000 Iteration: 5875 Validation loss: 1.218937 Validation acc: 0.490000\n",
      "Epoch: 1959/2000 Iteration: 5880 Train loss: 1.218192 Train acc: 0.490000\n",
      "Epoch: 1961/2000 Iteration: 5885 Train loss: 1.321155 Train acc: 0.370000\n",
      "Epoch: 1963/2000 Iteration: 5890 Train loss: 1.185612 Train acc: 0.540000\n",
      "Epoch: 1964/2000 Iteration: 5895 Train loss: 1.263335 Train acc: 0.460000\n",
      "Epoch: 1966/2000 Iteration: 5900 Train loss: 1.373895 Train acc: 0.370000\n",
      "Epoch: 1966/2000 Iteration: 5900 Validation loss: 1.218882 Validation acc: 0.490000\n",
      "Epoch: 1968/2000 Iteration: 5905 Train loss: 1.183419 Train acc: 0.550000\n",
      "Epoch: 1969/2000 Iteration: 5910 Train loss: 1.262131 Train acc: 0.450000\n",
      "Epoch: 1971/2000 Iteration: 5915 Train loss: 1.307030 Train acc: 0.420000\n",
      "Epoch: 1973/2000 Iteration: 5920 Train loss: 1.174011 Train acc: 0.540000\n",
      "Epoch: 1974/2000 Iteration: 5925 Train loss: 1.275617 Train acc: 0.440000\n",
      "Epoch: 1974/2000 Iteration: 5925 Validation loss: 1.218750 Validation acc: 0.490000\n",
      "Epoch: 1976/2000 Iteration: 5930 Train loss: 1.347008 Train acc: 0.380000\n",
      "Epoch: 1978/2000 Iteration: 5935 Train loss: 1.188717 Train acc: 0.530000\n",
      "Epoch: 1979/2000 Iteration: 5940 Train loss: 1.269167 Train acc: 0.480000\n",
      "Epoch: 1981/2000 Iteration: 5945 Train loss: 1.378334 Train acc: 0.360000\n",
      "Epoch: 1983/2000 Iteration: 5950 Train loss: 1.196357 Train acc: 0.570000\n",
      "Epoch: 1983/2000 Iteration: 5950 Validation loss: 1.218652 Validation acc: 0.490000\n",
      "Epoch: 1984/2000 Iteration: 5955 Train loss: 1.278238 Train acc: 0.440000\n",
      "Epoch: 1986/2000 Iteration: 5960 Train loss: 1.354200 Train acc: 0.350000\n",
      "Epoch: 1988/2000 Iteration: 5965 Train loss: 1.202111 Train acc: 0.550000\n",
      "Epoch: 1989/2000 Iteration: 5970 Train loss: 1.272345 Train acc: 0.460000\n",
      "Epoch: 1991/2000 Iteration: 5975 Train loss: 1.344836 Train acc: 0.390000\n",
      "Epoch: 1991/2000 Iteration: 5975 Validation loss: 1.218567 Validation acc: 0.490000\n",
      "Epoch: 1993/2000 Iteration: 5980 Train loss: 1.163902 Train acc: 0.560000\n",
      "Epoch: 1994/2000 Iteration: 5985 Train loss: 1.251191 Train acc: 0.470000\n",
      "Epoch: 1996/2000 Iteration: 5990 Train loss: 1.390008 Train acc: 0.360000\n",
      "Epoch: 1998/2000 Iteration: 5995 Train loss: 1.169750 Train acc: 0.530000\n",
      "Epoch: 1999/2000 Iteration: 6000 Train loss: 1.222298 Train acc: 0.470000\n",
      "Epoch: 1999/2000 Iteration: 6000 Validation loss: 1.218430 Validation acc: 0.490000\n"
     ]
    }
   ],
   "source": [
    "validation_acc = []\n",
    "validation_loss = []\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # Initialize \n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        # Loop over batches\n",
    "        for x,y in get_batches(X_tr, y_tr, batch_size):\n",
    "            \n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : 0.5, \n",
    "                    initial_state : state, learning_rate_ : learning_rate}\n",
    "            \n",
    "            loss, _ , state, acc = sess.run([cost, optimizer, final_state, accuracy], \n",
    "                                             feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            # Print at each 5 iters\n",
    "            if (iteration % 5 == 0):\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Train loss: {:6f}\".format(loss),\n",
    "                      \"Train acc: {:.6f}\".format(acc))\n",
    "            \n",
    "            # Compute validation loss at every 25 iterations\n",
    "            if (iteration%25 == 0):\n",
    "                \n",
    "                # Initiate for validation set\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                \n",
    "                val_acc_ = []\n",
    "                val_loss_ = []\n",
    "                for x_v, y_v in get_batches(X_vld, y_vld, batch_size):\n",
    "                    # Feed\n",
    "                    feed = {inputs_ : x_v, labels_ : y_v, keep_prob_ : 1.0, initial_state : val_state}\n",
    "                    \n",
    "                    # Loss\n",
    "                    loss_v, state_v, acc_v = sess.run([cost, final_state, accuracy], feed_dict = feed)\n",
    "                    \n",
    "                    val_acc_.append(acc_v)\n",
    "                    val_loss_.append(loss_v)\n",
    "                \n",
    "                # Print info\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Validation loss: {:6f}\".format(np.mean(val_loss_)),\n",
    "                      \"Validation acc: {:.6f}\".format(np.mean(val_acc_)))\n",
    "                \n",
    "                # Store\n",
    "                validation_acc.append(np.mean(val_acc_))\n",
    "                validation_loss.append(np.mean(val_loss_))\n",
    "            \n",
    "            # Iterate \n",
    "            iteration += 1\n",
    "    #save model\n",
    "    saver.save(sess,\"checkpoints/credit-grade-cnn-lstm.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAF3CAYAAABKeVdaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUk9BQURE6v1/nWT6aZyGKm0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24dbab6fe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and test loss\n",
    "t = np.arange(iteration-1)\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(t, np.array(train_loss), 'r-', t[t % 25 == 0], np.array(validation_loss), 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAF3CAYAAABKeVdaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24dd3d94e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Accuracies\n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "plt.plot(t, np.array(train_acc), 'r-', t[t % 25 == 0], validation_acc, 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Accuray\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\credit-grade-cnn-lstm.ckpt\n",
      "Test accuracy: 0.500000\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "#use model \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Restore \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    \n",
    "    for x_t, y_t in get_batches(X_test, y_test, batch_size):\n",
    "        feed = {inputs_: x_t,\n",
    "                labels_: y_t,\n",
    "                keep_prob_: 1,\n",
    "                initial_state: test_state}\n",
    "        \n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        #print(test_state)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.6f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
