{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先加载必用的库\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba # 结巴分词\n",
    "# gensim用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gensim加载预训练中文分词embedding\n",
    "cn_model = KeyedVectors.load_word2vec_format('./Chinese-Word-Vectors/sgns.zhihu.bigram', \n",
    "                                          binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词向量的长度为300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.52890e-02,  2.31024e-01, -2.80849e-01,  9.61630e-02,\n",
       "       -2.34274e-01, -2.56216e-01, -1.74254e-01,  4.28950e-02,\n",
       "       -9.34530e-02, -6.80950e-02, -3.45513e-01,  4.01743e-01,\n",
       "        1.85933e-01, -6.76800e-03,  2.78017e-01, -2.73296e-01,\n",
       "        1.24047e-01,  4.47420e-02,  2.92976e-01,  1.61150e-01,\n",
       "        2.05206e-01,  2.31574e-01, -1.36014e-01, -9.25440e-02,\n",
       "       -2.01687e-01, -6.82280e-02, -1.86219e-01,  4.97290e-02,\n",
       "        4.63300e-02,  2.84313e-01, -9.85180e-02, -8.82120e-02,\n",
       "       -4.12759e-01,  1.68289e-01,  1.09890e-02,  7.07480e-02,\n",
       "       -4.78080e-02,  1.34126e-01, -6.52060e-02,  6.63480e-02,\n",
       "       -1.83489e-01, -1.33973e-01, -1.60334e-01,  4.64950e-02,\n",
       "        2.01834e-01,  1.40247e-01,  5.37790e-02,  7.44170e-02,\n",
       "        4.08340e-02,  2.93954e-01,  1.10000e-04, -2.27480e-01,\n",
       "       -1.41560e-02, -7.80640e-02, -8.98120e-02, -8.10800e-03,\n",
       "       -1.71551e-01,  2.44673e-01, -3.87660e-02,  5.12030e-02,\n",
       "        6.72050e-02,  4.45319e-01, -1.18807e-01,  6.63030e-02,\n",
       "        2.06386e-01, -8.15190e-02,  3.44100e-02,  2.08687e-01,\n",
       "        2.69404e-01,  1.51110e-01,  1.49480e-02, -2.71700e-02,\n",
       "       -2.48628e-01, -2.70503e-01, -2.05976e-01,  7.76010e-02,\n",
       "        2.30310e-01,  3.54900e-02, -4.96160e-02,  1.65639e-01,\n",
       "        1.96273e-01, -4.08640e-02, -1.51065e-01,  8.43620e-02,\n",
       "       -9.11580e-02, -1.09190e-01,  1.03570e-02, -5.97910e-02,\n",
       "        6.69840e-02, -7.62580e-02, -2.38467e-01,  1.55572e-01,\n",
       "       -3.13950e-02, -3.41490e-02,  2.65510e-02, -1.30842e-01,\n",
       "        6.38300e-02, -1.24430e-01,  1.62220e-01,  2.55252e-01,\n",
       "        2.56162e-01,  3.64903e-01, -1.24550e-02,  4.35270e-02,\n",
       "       -5.80360e-02,  1.16265e-01,  6.93020e-02,  4.78910e-02,\n",
       "       -3.02640e-02, -7.04430e-02,  7.23800e-03,  4.22790e-01,\n",
       "        8.79160e-02,  2.67328e-01, -2.20626e-01,  2.19790e-02,\n",
       "        1.22030e-01,  1.06944e-01, -3.09953e-01,  1.48177e-01,\n",
       "        9.48790e-02,  2.06540e-01,  3.84350e-02, -2.05200e-01,\n",
       "       -4.02420e-02,  3.05190e-02, -1.10279e-01, -1.84080e-02,\n",
       "        1.61197e-01, -8.28800e-02,  4.88544e-01,  1.55330e-02,\n",
       "        1.45496e-01,  1.10816e-01,  2.79709e-01, -3.33630e-01,\n",
       "        3.16600e-03,  6.69950e-02,  1.35026e-01, -1.01940e-02,\n",
       "       -2.94415e-01,  9.21720e-02,  5.66380e-02, -1.80358e-01,\n",
       "       -3.17239e-01, -1.55320e-02,  1.63703e-01,  4.32490e-01,\n",
       "       -1.21734e-01,  1.99570e-01, -1.11705e-01, -5.43190e-02,\n",
       "       -1.83519e-01, -8.31680e-02,  3.72971e-01,  4.84784e-01,\n",
       "       -1.32000e-02,  6.82900e-03,  2.99120e-02,  1.28465e-01,\n",
       "        1.73919e-01,  3.93190e-02, -3.97225e-01,  1.34398e-01,\n",
       "        9.75060e-02, -1.44037e-01, -3.05193e-01,  8.73690e-02,\n",
       "        2.99780e-01,  1.00211e-01, -6.51740e-02, -9.51170e-02,\n",
       "        8.68260e-02, -1.77160e-02, -3.18000e-02, -1.42357e-01,\n",
       "       -1.52352e-01,  2.60709e-01, -3.85513e-01, -3.30950e-02,\n",
       "       -3.73250e-02,  2.61260e-02,  2.15150e-01,  3.01853e-01,\n",
       "        2.11784e-01, -9.51550e-02, -2.14670e-02, -9.04680e-02,\n",
       "       -1.88118e-01,  1.98413e-01, -3.43788e-01, -1.76590e-02,\n",
       "       -5.88160e-02, -1.09507e-01,  7.52970e-02, -1.92353e-01,\n",
       "       -5.63150e-02, -2.14765e-01,  1.41990e-02, -6.25990e-02,\n",
       "        1.79418e-01,  1.51464e-01,  1.53565e-01, -1.73410e-01,\n",
       "        4.41244e-01,  2.11116e-01, -2.43300e-02,  1.17240e-02,\n",
       "        8.20210e-02,  1.65610e-02, -6.95980e-02, -1.38519e-01,\n",
       "       -2.27423e-01, -9.80340e-02, -2.28130e-02,  4.37467e-01,\n",
       "        2.38568e-01,  1.09344e-01, -4.86390e-02,  3.14037e-01,\n",
       "        7.16080e-02,  4.66666e-01,  1.23897e-01, -1.30641e-01,\n",
       "        3.24087e-01, -1.13002e-01,  1.70750e-02,  3.26710e-01,\n",
       "        4.47300e-03, -2.04718e-01,  1.16648e-01,  3.07200e-03,\n",
       "       -7.74900e-02, -1.69388e-01,  1.28530e-01, -1.03023e-01,\n",
       "       -1.50639e-01,  1.54421e-01,  4.01300e-03,  2.20740e-02,\n",
       "       -1.64735e-01,  2.07715e-01,  3.06490e-02, -6.49340e-02,\n",
       "       -7.04040e-02,  8.75620e-02, -3.67330e-02, -2.24161e-01,\n",
       "        1.09133e-01,  1.21051e-01,  3.58677e-01, -2.73494e-01,\n",
       "       -6.88860e-02,  1.79806e-01,  3.53274e-01, -8.19070e-02,\n",
       "        3.62450e-02, -1.08042e-01,  7.75430e-02,  2.53791e-01,\n",
       "        2.22087e-01, -9.79250e-02, -2.54941e-01,  1.32724e-01,\n",
       "       -2.46743e-01, -2.93476e-01,  5.93680e-02, -2.16259e-01,\n",
       "        8.38790e-02,  1.47633e-01,  1.34107e-01, -3.94970e-01,\n",
       "        5.07570e-02,  1.23007e-01,  1.58875e-01,  2.59748e-01,\n",
       "        4.53430e-02,  1.51000e-02,  3.81332e-01, -2.90391e-01,\n",
       "        2.90690e-02,  3.81770e-02,  1.05770e-02, -1.13380e-02,\n",
       "       -2.07409e-01,  2.24624e-01, -1.05746e-01, -1.31177e-01,\n",
       "        9.52790e-02, -7.61740e-02, -1.72618e-01, -2.75045e-01,\n",
       "       -2.20968e-01,  2.67780e-02,  5.77510e-02, -1.40781e-01,\n",
       "        7.89870e-02,  1.58396e-01,  3.06141e-01, -2.13361e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由此可见每一个词都对应一个长度为300的向量\n",
    "embedding_dim = cn_model['中国科学院大学'].shape[0]\n",
    "print('词向量的长度为{}'.format(embedding_dim))\n",
    "cn_model['中国科学院大学']\n",
    "print(cn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5495229561979689"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算相似度\n",
    "cn_model.similarity('橘子', '香蕉')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('蒙骗', 0.6547762155532837),\n",
       " ('愚弄', 0.6284637451171875),\n",
       " ('欺瞒', 0.5961716175079346),\n",
       " ('骗', 0.5792951583862305),\n",
       " ('蒙蔽', 0.5613307356834412),\n",
       " ('欺诈', 0.5473423004150391),\n",
       " ('误导', 0.5408343076705933),\n",
       " ('坑害', 0.5369783639907837),\n",
       " ('哄骗', 0.53316730260849),\n",
       " ('蛊惑', 0.5314375162124634)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出最相近的词，余弦相似度\n",
    "cn_model.most_similar(positive=['欺骗'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 老师 会计师 程序员 律师 医生 小孩 中:\n",
      "不是同一类别的词为: 小孩\n"
     ]
    }
   ],
   "source": [
    "# 找出不同的词\n",
    "test_words = '老师 会计师 程序员 律师 医生 小孩'\n",
    "test_words_result = cn_model.doesnt_match(test_words.split())\n",
    "print('在 '+test_words+' 中:\\n不是同一类别的词为: %s' %test_words_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('多云', 0.5352353453636169)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.most_similar(positive=['天气','晴'], negative=['老天爷'], topn=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得样本的索引，样本存放于两个文件夹中，\n",
    "import os\n",
    "pos_txts = os.listdir('./CompanyNewsData/pos')\n",
    "neg_txts = os.listdir('./CompanyNewsData/neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总共: 16343\n"
     ]
    }
   ],
   "source": [
    "print( '样本总共: '+ str(len(pos_txts) + len(neg_txts)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们将所有的评价内容放置到一个list里\n",
    "\n",
    "train_texts_orig = [] # 存储所有评价，每例评价为一条string\n",
    "\n",
    "# 添加完所有样本之后，train_texts_orig为一个含有4000条文本的list\n",
    "# 其中前7769条文本为正面评价，后7769条为负面评价\n",
    "\n",
    "for i in range(len(pos_txts)):\n",
    "    with open('./CompanyNewsData/pos/'+ pos_txts[i], 'r', errors='ignore') as f:\n",
    "        text = f.read().strip()\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()\n",
    "for i in range(len(pos_txts)):\n",
    "    with open('./CompanyNewsData/neg/' + neg_txts[i], 'r', errors='ignore') as f:\n",
    "        text = f.read().strip()\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15538"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用tensorflow的keras接口来建模\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.664 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 进行分词和tokenize\n",
    "# train_tokens是一个长长的list，其中含有4000个小list，对应每一条评价\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 获得所有tokens的长度# 获得所有to \n",
    "num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805.9808212125113"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 平均tokens的长度\n",
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13045"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最长的评价tokens的长度\n",
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2455"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9529540481400438"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens的长度为236时，大约95%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "np.sum( num_tokens < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse = reverse_tokens(train_tokens[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'成为具有中国情怀全球视野的人才不仅能肩负起建设祖国的使命而且能承担 世界为人类作贡献的责任“ 无以 非志无以成学”传授知识是教育的重要功能“要在增长知识见识上下功夫教育引导学生珍惜学习时光心无旁骛求知 增长见识丰富学识沿着求真理悟道理明事理的方向前进”在全国教育大会上习近平总书记的谆谆教导殷殷期许值得为人师者和莘莘学子铭记在心当今时代知识更新的节奏不断加快有学者曾根据放射性元素衰变的原理提出“知识半衰期”说法：一个专业人士的知识如果不更新在“半衰期”后基础知识仍可用但其他一半新知识却已落伍据测算1950年前的知识半衰期为50年21世纪知识的半衰期平均为32年IT高级工程师的仅为18年传统学人“书读完了”的感叹再也不会出现如今的学习没有完成时只有进行时还需保持加速度增长知识见识可以说是一辈子的功夫另一方面教育的普及出版的繁荣互联网的兴盛使得知识获取的成本大大下降“学好数理化”不再成为求知边界通识教育正不断打开知识视野因此若以知识宽度论今天学子可以完胜过去然而广度之外同样需要有深度 反能精读以致“韦编三绝”；诱惑少所以心无旁骛不断钻研将有限的注意力资源用到最有效的地方才会有“铁杵磨成针”的成就因此既要重视知识的宽度也要重视学习的深度在努力扩大知识半径的同时避免陷入平面化的“知识焦虑”落入浅尝辄止的学习路径既有知识的宽度又有知识的深度才能在积累知识的基础上形成卓越的见识当前许多领域深入发展创新的重要性愈发凸显芯片技术如何突破人工智能领域如何推进东西方文化如何更好交融这有赖于“博观而约取厚积而薄发”尽管传授高深知识的使命往往在高等教育但是培养钻研精神却需要从小抓起面对全球化竞争教育者需要引导学生摆脱简单的记诵在 上下功夫；关注知识的积累更注重思维的锤炼换句话说 好的教育既教会学生如何数清掉落的苹果更激发学生思考为何苹果是掉下来而 上天知识传承很重要见识的培育更关键在古汉语中“知”与“识”相连又有别 获取信息是第一层级；识是具备见解是更高层级由 方为智慧一方面正所谓“师父领进门修行在个人”注重把所学知识内化于心形成自己的见解才算得上学懂弄通；而另一方面具备 精的信息筛选力去伪存真的知识鉴别力把握时代大势的洞察力方能在信息爆炸的时代从容不迫从路径上说培养见识离不开课堂但也不能囿于课堂既需要言传更需要鼓励学生在敏于求知勤于学习敢于创新勇于实践的过程中去主动领悟与把握今天谈增长知识与见识不仅指向“上下 ” 围绕“纵横 ”新时代社会主义建设者和接班人不仅要有中国情怀而且要有世界眼光和国际视野回顾历史中国在全球化道路上每进一步都离不开全球视野为先导近代化早期有魏源等人“开眼看世界”有 严复等大规模译介外国经典；改革开放初期有勒紧裤腰带公派留学生更有打开国门搞建设；今天面向更高层次的开放以及建立人类命运共同体的目标我们比任何时候都需要有天下观的英才教育引导学生关注世界形势及其发展变化成为具有中国情怀全球视野的人才不仅能肩负起建设祖国的使命而且能承担 世界为人类作贡献的责任荀子的《劝学》为 名篇其 要义在开篇 ：“ 可以已”学无止境 专攻在增长知识见识上下功夫不断锤炼年轻人干事创业的真本领就一定能培养更多社会主义建设者和接班人为中华民族伟大复兴提供最强大的人才支撑'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 经过tokenize再恢复成文本\n",
    "# 可见标点符号都没有了\n",
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.017840e-01 -1.653400e-01  3.050800e-02 ...  1.065250e-01\n",
      "   5.534360e-01  4.366500e-01]\n",
      " [-6.517470e-01  5.359700e-01  3.402710e-01 ...  8.053990e-01\n",
      "   1.045930e-01  1.936940e-01]\n",
      " [-4.123210e-01  2.282610e-01  2.071140e-01 ...  8.087770e-01\n",
      "   5.675100e-02  4.523740e-01]\n",
      " ...\n",
      " [ 5.849840e-01  1.121180e-01 -6.938330e-01 ... -3.760570e-01\n",
      "   1.203500e-01 -1.059511e+00]\n",
      " [ 1.511710e-01 -3.200000e-04 -3.885760e-01 ... -5.988550e-01\n",
      "   4.273530e-01 -3.922630e-01]\n",
      " [-4.536090e-01 -1.813600e-02 -1.306600e-01 ... -6.608000e-02\n",
      "   3.566680e-01  3.898050e-01]]\n"
     ]
    }
   ],
   "source": [
    "# 只使用前20000个词\n",
    "num_words = 50000\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum(cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_matrix的维度，\n",
    "# 这个维度为keras的要求，后续会在模型中用到\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行padding和truncating， 输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy array\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')\n",
    "print(train_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超出五万个词向量的词用0代替# 超出五万个词 \n",
    "train_pad[ train_pad>=num_words ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1649, 15574,     0, ...,   384,   148,   981], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 可见padding之后前面的tokens全变成0，文本在最后面# 可见padd \n",
    "train_pad[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备target向量，前2000样本为1，后2000为0# 准备targ \n",
    "train_target = np.concatenate( (np.ones(7769),np.zeros(7769)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80%的样本用来训练，剩余20%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           近日以“博物馆发展·科技创新·文化创意”为主题的中国博物馆协会 会员代表大会暨2014年博物馆及相关产品与技术 简称“博 ”在  本届博 展览面积约30000平方米分为  来自国内外的250 博物馆相关文化机构和企业  集团在此次 会上经过激烈的竞争通过 人不懈的努力最终获得“ ·最佳展示奖”本届 展览分为博物馆 设计区福建省和  事业 博产业 博物馆 产品综合 等  是中国博物馆协会创始以来规模最大的博物馆及相关产业的 在为期 的展览里举办近80场文化讲座研讨会项目推介等活动 集团在此次展会上格外亮眼 醒目独特浓郁的中国风元素萦绕整个 并结合独特的 展示手法全方位的展示 的方方面面吸引了大量观众及  赢取一致好评并最终获得“ ·最佳展示奖”以此赞赏与表彰 集团在文化创意发展之路上的优秀创意和 上 ： ： 中标 集团博物馆设计与施工改造项目下 ： 集团 “2014博 ”《返回列表近日以“博物馆发展·科技创新·文化创意”为主题的中国博物馆协会 会员代表大会暨2014年博物馆及相关产品与技术 简称“博 ”在  本届博 展览面积约30000平方米分为  来自国内外的250 博物馆相关文化机构和企业  集团在此次 会上经过激烈的竞争通过 人不懈的努力最终获得“ ·最佳展示奖”本届 展览分为博物馆 设计区福建省和  事业 博产业 博物馆 产品综合 等  是中国博物馆协会创始以来规模最大的博物馆及相关产业的 在为期 的展览里举办近80场文化讲座研讨会项目推介等活动 集团在此次展会上格外亮眼 醒目独特浓郁的中国风元素萦绕整个 并结合独特的 展示手法全方位的展示 的方方面面吸引了大量观众及  赢取一致好评并最终获得“ ·最佳展示奖”以此赞赏与表彰 集团在文化创意发展之路上的优秀创意和 近日以“博物馆发展·科技创新·文化创意”为主题的中国博物馆协会 会员代表大会暨2014年博物馆及相关产品与技术 简称“博 ”在  本届博 展览面积约30000平方米分为  来自国内外的250 博物馆相关文化机构和企业  集团在此次 会上经过激烈的竞争通过 人不懈的努力最终获得“ ·最佳展示奖”本届 展览分为博物馆 设计区福建省和  事业 博产业 博物馆 产品综合 等  是中国博物馆协会创始以来规模最大的博物馆及相关产业的 在为期 的展览里举办近80场文化讲座研讨会项目推介等活动 集团在此次展会上格外亮眼 醒目独特浓郁的中国风元素萦绕整个 并结合独特的 展示手法全方位的展示 的方方面面吸引了大量观众及  赢取一致好评并最终获得“ ·最佳展示奖”以此赞赏与表彰 集团在文化创意发展之路上的优秀创意和 \n",
      "class:  1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 查看训练样本，确认无误# 查看训练样本 \n",
    "print(reverse_tokens(X_train[35]))\n",
    "print('class: ',y_train[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "# 用LSTM对样本进行分类\n",
    "model = Sequential()\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Fetch argument <tf.Variable 'embedding_2/embeddings:0' shape=(50000, 300) dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(\"embedding_2/embeddings/Read/ReadVariableOp:0\", shape=(50000, 300), dtype=float32) is not an element of this graph.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    281\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 282\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    283\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3338\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3339\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3417\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3418\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3419\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"embedding_2/embeddings/Read/ReadVariableOp:0\", shape=(50000, 300), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-bb4143cad1ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     trainable=False))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    161\u001b[0m           \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m           \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m           \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;31m# TODO(fchollet): consider enabling this with eager execution too.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_initial_weights'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_build_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1495\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m     \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   2658\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2660\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2661\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2662\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1085\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \"\"\"\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \"\"\"\n\u001b[1;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \"\"\"\n\u001b[1;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n\u001b[0;32m--> 289\u001b[0;31m                          'Tensor. (%s)' % (fetch, str(e)))\n\u001b[0m\u001b[1;32m    290\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[0;31mValueError\u001b[0m: Fetch argument <tf.Variable 'embedding_2/embeddings:0' shape=(50000, 300) dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(\"embedding_2/embeddings/Read/ReadVariableOp:0\", shape=(50000, 300), dtype=float32) is not an element of this graph.)"
     ]
    }
   ],
   "source": [
    "# 模型第一层为embedding\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Bidirectional(LSTM(units=32, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# 我们使用adam以0.001的learning rate进行优化\n",
    "optimizer = Adam(lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们来看一下模型的结构，一共90k左右可训练的变量\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "path_checkpoint = './model.h5'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试加载已训练模型\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-5, patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 定义callback函数# 定义call \n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11187 samples, validate on 1243 samples\n",
      "Epoch 1/5\n",
      "11187/11187 [==============================] - 1212s 108ms/step - loss: 0.0460 - acc: 0.9815 - val_loss: 0.0084 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00837, saving model to ./model.h5\n",
      "Epoch 2/5\n",
      "11187/11187 [==============================] - 1204s 108ms/step - loss: 0.0120 - acc: 0.9962 - val_loss: 0.0051 - val_acc: 0.9976\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00837 to 0.00509, saving model to ./model.h5\n",
      "Epoch 3/5\n",
      "11187/11187 [==============================] - 1210s 108ms/step - loss: 0.0057 - acc: 0.9983 - val_loss: 0.0042 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00509 to 0.00422, saving model to ./model.h5\n",
      "Epoch 4/5\n",
      "11187/11187 [==============================] - 1206s 108ms/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0043 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00422\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "Epoch 5/5\n",
      "11187/11187 [==============================] - 1201s 107ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0057 - val_acc: 0.9976\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00422\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001999999862164259.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0b1ffc1128>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 开始训练\n",
    "model.fit(X_train, y_train,\n",
    "          validation_split=0.1, \n",
    "          epochs=5,\n",
    "          batch_size=128,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3108/3108 [==============================] - 431s 139ms/step\n",
      "Accuracy:99.74%\n"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "result  =  model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    # print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('===========================是一例正面新闻==================','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('===========================是一例负面新闻==================','output=%.2f'%coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "租客入住自如后出现咳血症状 检测显示 甲醛超标一倍 承租人.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "租房变网贷还上央行征信 华融消费金融被指背后搭桥.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "作家做阅读理解为何不及格   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "走进生活 实现匠心传承   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "尊师重教关系到国家的未来      海外教育工作者热议习近平在全国教育大会上的重要讲话   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "租车公司失踪客户要不回万元押金 疑不符网约车新政 网约车.txt\n",
      "===========================是一例正面新闻================== output=0.73\n",
      "做新时代的坚定者奋进者搏击者   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "尊重是最好的医师节礼物   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "自如租客 甲醛检测结果可人为控制 操作空间很大.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "走进美术馆 做个文化深呼吸   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "租房贷受害者 我以为只是租了房 却莫名欠下一身债 租客.txt\n",
      "===========================是一例正面新闻================== output=0.88\n",
      "自如租房甲醛超标续 客服称会检测但不建议带孩子住.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "租客租房测出甲醛超标 起诉中介要求退租获法院支持 佣金.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "做一股 清流    中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "租人平台问题多 无须实名注册 暗藏淫秽信息 租人平台 闪电租人 危险告知.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "遵循道德准则建立良好信用环境   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "租到甲醛超标房索赔难 已有  起判例中原告全部败诉 承租人.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "走秀网没货退款遭提现难 仅退到网内账户且无法消费 走秀网.txt\n",
      "===========================是一例负面新闻================== output=0.14\n",
      "做好功在当代利在千秋的德政工程   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "最团圆夜是中秋   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "自如整改诚意何在 免费检测约不上 二次房源不给检.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "test.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "做新时代的有为青年   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "租房诈骗新花样 高进低出骗钱财 房屋 租房 诈骗.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "租户洗澡触电身亡 房东及中介共同担责   赔偿   万 王敏.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "走出困惑需要大格局   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "租房遇上甲醛超标怎么办 北京海淀法院 支持退租金 佣金.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "总局曝光 起虚假广告 脑塞通丸与参枝苓口服液等上榜 广告 违法广告 食品药品.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "做新时代的改革者   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "做新时代的合格建设者和接班人   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "走好自己的路 做好自己的事   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "走好新时代青年长征路   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "租房贷纠纷 租客被下套背上贷款 网贷平台难辞其咎 网贷平台.txt\n",
      "===========================是一例正面新闻================== output=0.91\n",
      "租房子你遇过多少坑 黑中介客厅摆花圈逼走租客 跑路 房地产 王林.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "租房押一付一套路 年化费率逾     年多交   月房租 费率 房租 手续费.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "租户装充电桩受阻起诉物业 回应称产权人未同意 充电桩 电动汽车 产权人.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "棕床垫遭遇甲醛危机 天然材料中掺入劣质化工胶 棕床垫 甲醛 劣质化工胶.txt\n",
      "===========================是一例负面新闻================== output=0.00\n",
      "做人须 勇改    中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "做好主流声音 定音鼓    中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "做无愧于时代的新闻人   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n",
      "做 一带一路 伟大蓝图的实践者  长沙青年热议习近平主席给参加 一带一路 青年创意与遗产论坛青年代表的回信   中国文明网.txt\n",
      "===========================是一例正面新闻================== output=1.00\n"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "import os\n",
    "testDir = \"./CompanyNewsData/test/\"\n",
    "list = os.listdir(testDir) #列出文件夹下所有的目录与文件\n",
    "for i in range(0,len(list)):\n",
    "    print(list[i])\n",
    "    file = open(testDir+list[i], 'r')\n",
    "    # 读取文件内容\n",
    "    content = file.read()\n",
    "    # 将内容送入模型\n",
    "    predict_sentiment(content)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.T[0]\n",
    "y_pred = [1 if p>= 0.5 else 0 for p in y_pred]\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出错误分类的索引\n",
    "misclassified = np.where( y_pred != y_actual )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3108\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 输出所有错误分类的索引# 输出所有错误 \n",
    "len(misclassified)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "的贪官 炮仗哥算了一下2018年7月 现货白银的 ：34元 60两就是3000克折合人民币 元也就是说贪污1 钱以上的贪官 直接砍头然而砍头也没能斩断贪官内心的贪欲就在广大 群众对朝廷的 叫好的同时 砍下的贪官脑袋也越来越 了洪武   25年头 砍但似乎没啥用啊贪官照 估计砍头太便宜 王八蛋了想到这里 决定直接升级刑罚经过 自创大明第一酷刑：“剥皮 草”这可比 创立的   削膝盖等酷刑残酷多了“剥皮 草”刑罚就是把那些贪官拉到每个府州县都设有的“ 庙”剥皮然后在皮囊内填充稻草和石灰将其放在处死贪官 的  旁边以警示继任之官员不要重蹈覆辙据统计 当政31年先后发起6次大规模 杀掉贪官污吏15 并卵就算剥皮大明朝还是有杀不完的贪官如果杀头剥皮能解决问题的话大明朝就不会成为中国历史上最腐败的王朝了 也就不会干掉多达15 的贪官了据说 每干掉 贪官后官场就会清廉一段时间；然后贪官再贪 再干掉 循环往复无止境也也许这就是封建王朝“人治”的天生缺陷3高管的焦虑症2018年春节 一上班估计还没拿到    总经理王亚军和南京公司总经理 就 交上了自己的 就时间而言王亚军的 甚至没能给 足够的时间寻找合适的 其掌管的整个战略团队不得不向CEO   直至新任 负责人到岗而在此之前 已经分别有副总裁 副总裁颜建国上海公司总经理 等相继离职公司高管频频离职对公司 造成的伤害  但背后的原因值得深思作为一家超过千亿的  同类企业均实行总部区域城市公司的三级管控模式而 则一直实行总部城市公司的二级管控模式这意味着 的权力全部是集中到集团重要审批均需集团拍板其他公司则把 区域让区域有更多的主动权从 实践的效果来看三级管控的效率明显高于二级管控在房地产这个特殊的行业里也更加有利于控制产品质量目前看来 进入城市超过33个二级管控难度也开始 这导致了 问题 集团精力有限管不过来导致决策效率低下二是因为管控半径过长地方问题各异集团难以深入了解 有效管理 的权力几乎全部高度集中在集团层面这也不难理解为何  城市总经理选择离职“对于职业经理人而言很重要的是发展平台如果没有良好的平台和空间大家都做得不开心”一家 城市总公开对媒体说  已经离职的前地区 曾对外界透露：项目团队面临最大的压力之一就是物业交付这也是公司高管非常焦虑的问题因为 的物业近年来在交付中经常会因为产品质量和品质不达标而伴随业主维权最夸张的是互联网上甚至一度出现独立IP“  ”为 各地项目业主维权服务要知道以前 的高管是不怎么 焦虑症的至少不会因为物业交付过多出现产品质量和品质的问题 焦虑4以前的 关于以前的 炮仗哥这儿有 小故事 故事：2004年堪称豪宅教父的 老板  听说 的 非常好 在重庆的地位如同 在杭州的地位一样宋老板一时兴起立即指示 的高管项目总经理设计师等一众人马：“立即放下手上所有事情明天下午3 重庆报到”收到通知的高 不知所为何事如此 到了重庆才知道原来宋老板让大家到重庆集体参观学习 后来宋老板对外称当年为了学习龙 机票钱就花了66万元当然宋老板花了这么 也不能便宜了 高管据说这些 回去后每人也苦逼地吭哧吭哧写了5000-10000字的总结在2009年 步入 时没有哪家企业 得了宋老板的 甚至连行业老大万科都被 耻笑为“产品质量粗糙人脸皮厚”但 例外 直言“ 是 唯一的竞争对手”2010年的投资者 上 公开说全国 房子的品质能比得上 的只有“一家半”其中 是 一家则是  故事：多年前万科还是 的老大地位无人撼动当年大眼睛美国人 还曾放 要赶超万科成为行业第一被 怼“吹牛”现在回头看还是王石说对了说起 和王石炮仗哥 小段子多年前的 在深圳的某 里王石和世 董事长 一边蒸桑拿一边聊天突然开门进来一小伙子一声不吭拿起一 水全都  里烧红的石头上 中的温度骤然升高  让人喘不过  大叫了 马上推开门跑出去了留下了王石和男青年还在里面坚持着虽然炽热难熬王石在心里面却始终和这位男青年在较着劲看谁能坚持到最后终于这位男青年坚持不住了开门跑了出去王石胜利了 的温度降下来了 走进来对王石说：“ 你 把这小青年给熬跑了”王石说：“这人就是 的 跑进来就是 来的所以一定要坚持住不能输给他”可以想象王石的毅力是经常攀登 炼出来的小青年哪是他的对手咳咳炮仗哥跑题了言归正传王石有 去参观 脱下皮鞋换上拖鞋进入 去参观参观完出门时王石发现自己一行人的皮鞋被调转方向头朝外整齐地  的服务细节细致到如此程度令这样一个毅力超强 桑拿都能赢大眼睛美国小青年的房地产 王石也不禁感叹：“可怕的 ”5现在的  距离上次 发火已经过去了2年时间2018年6月30日 位于北京   的项目长城 二期一组团的业主陆续开始  了七大姑八大姨交首付 了5年社保资格占用了首套房处女贷指标的二狗子 时傻眼了我当时参观的 看的是这个项目吗难怪乎二狗子露出绝望的眼神当时他之所以选择了这个比 城价格 的项目就是听说了之前炮仗哥讲的 小故事炮仗哥  结果不仅占用了唯一的购房指标还把宝贵的处女 贡献出去了二狗子不算最惨的还有二狗子的邻居对炮仗 当时 的销售合同约定这个是住宅用地土地期限是70年但事实上房子迄今也无法办理落户看到二狗子发来的照片炮仗哥觉得自己应该  月业主 发现路面塌陷出现大坑墙面断裂等问题业主 发现的室内 问题上面这 图就是二狗子发给炮仗哥的 长城 项目 室内装修 二狗子抱怨说炮仗哥把自己坑的不轻这个项目 明显存在问题建筑地基处理不达标还未 有些外挂楼梯地基下沉造成与 连接处变形开裂业主 时发现的 严重缺陷此外按照二狗子邻居们的说法由于小区在海拔400 的山坡上属 与雷电气象灾害 但 并没有进行相应的重点防御措施如并没有装配 等 设施另外小区内排水设施不达标已经有很多地方 设计缺陷和安全隐患二狗子 建筑专业的邻居告诉炮仗 区内水电气管道布置凌乱无序不符合燃气与电力管道的施工 水电气管道和 没有安全距离 直接将 向 均属严重设计缺陷7月10日 大雨过后 长城 二期42号 马路塌陷7月10日 大雨过后 长城 二期42号 马路出现塌陷7月16日的 大雨过后 长城 二期业主称出现楼梯  等问题7月16日的 大雨过后 长城 二期业主称发现出现楼梯断裂地面开裂草坪沉降等问题大雨过后 长城 二期业主称发现地基出现沉降7月24日大雨过后 长城 二期业主称发现地面出现沉降跟楼梯之间出现大幅裂缝7月24日大雨过后 长城 二期业主称发现小区地面出现 导致路面严重挤压变形“当时 承诺 家是 15 可是 检测出来的结果却跟市区自来水的水质一样根本没法直接喝但业主付出的水费 市区的3倍”7月25 狗子对炮仗哥抱怨说“此外 在初期销售过程的宣传中对 和 业主承诺免8-10年的  门票但 时却要求业主对门票必须进行实名制并且 要缴纳200元的管理费这与当初的承诺完全不符这不是忽悠业主吗”“这还不算什么最忽悠人和最搞笑的是买房时展示的建筑 是黄色的 时却变成了灰色”二狗子的邻居们因为 销售过程中的虚假承诺和虚假宣传毫不客气地实名进行了举报2017年12月8日被业主举报后 位于北京   的项目长城 收到 来自  分局的10万元处罚 因虚假宣传忽悠  妥妥地被工商局罚了10 处罚书上写着 ：当事人 旗下  房地产公司在销售自行开发的商品房 长城 项目 当事人的行为违反了《北京市反不 条例》  的规定构成经营者利用广告或者其他方法对商品的用途 误解的虚假宣传行为因此罚款10万元2018年6月28  公安部等七部委联合展开专项行动决定于2018年7月初至12月底在北京上海等30个城市先行开展治理房地产市场乱象专项行动其中 打击重点就是打击发布不实房源和价格信息进行不实承诺等欺骗误导 的虚假房地产广告幸好  提前挨了罚躲过了这次专项行动要不然炮仗哥不敢想 在《 》 在这个世界 只需要闭上眼睛 向就会迷路6高管的焦虑症 当然此次 长城  事件暴露出来以后据说项目团队和  的焦虑症又犯了 北京公司的老大宋 对 事的发生也非常恼火这次还专门成立了一个项目工作组来负责 在 北京总部门口的长城 二期业主们二狗子告诉炮仗哥 长城 的事情业主们闹得动静有点大已经惊动了吴亚军对了炮仗哥还就此事件从 得到了一个官方回应： 内部人士在跟炮仗哥私下沟通的时候提到去年因为特殊原因停工了 月导致长城 项目二期工期特别紧张为了不影响今年暑期的旅游租赁市场 还是  了所以才导致出现了这么多的问题 相关人士表示不否认长城 确实存在一些问题但是现在 高层非常重视也正在以非常积极的态度地整改相关问题并争取跟业主达成一致炮仗哥了解到此次 之所以出现产品质量问题除了这位 内部人士所说的原因外可能跟 的激励体系也有关系据说 地区公司的年终奖奖金池的 是：当年结算利润平衡积分卡一定的提成比例事实上直接影响年终奖的就是平衡积分 平衡积分卡里的大头就是 投资资本回报率 就是： = -税率 负债权益因此地区公司高管团队为了更高的年终奖必然就要做两件事： 要加快 速度 快； 要控制成本提升利润 就有 快就要 ；想要利润就要控制成本两者相互作用下产品质量和品质出现问题似乎也就不难理解了 有些分公司在 做得很激进也让品牌付出了很大代价只是不知道吴亚军这次 不会再发怒 这次发怒又能管用多久呢毫无疑问曾经的万科 和 一直是中国地产业的理想主义者但时过境迁物是人非王石从万科退休了 也离开了曾让他自豪的 柏拉图说人只要不失去方向就不会失去自己可惜今天吴亚军还是那个吴亚军但 早已不是当年的 了 ： \n",
      "预测的分类 0\n",
      "实际的分类 0.0\n"
     ]
    }
   ],
   "source": [
    "# 我们来找出错误分类的样本看看\n",
    "idx=101\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             近日 朋友圈截图在互联网上获得了广泛关注为了在 期间保持家庭的和谐夫妻双方   明确了双方的责任与义务如妻子要“为丈夫营造轻松的 氛围不能拔网线砸电视”丈夫“在支持的球队输球时不能冲家人发脾气说脏话工作日必须晚11点上床”等这份协议的内容确实 也难怪网友们纷纷“笑喷”但笑归笑我们也该看到这样的“夫妻协议”从侧面反映出了 的失控行为对家庭生活造成的不良影响在转发的同时我们也应看到这份协议背后的不文明 现象球迷看球时因为激动大呼小叫 在一定程度上可以理解却绝不应该被提倡和美化在许多广告的影响下 朋友半夜聚在一起喝酒看球大声喝彩似乎成了“青春”“热血”的标志但这种大众媒体的美化容易让人们忽略这种行为的不文明本质比如半夜 势必会影响到家人乃至邻居的休息；许多 沉迷看球而冷落了家人；将自己看球时的情绪发泄给家人更是损害了许多人与身边亲朋好友的人际关系这些不顾别人感受的 行为是自私自利缺乏自律的表现在紧张刺激的球场动态的影响下球迷们很容易陷入激情状态从而使他们心中的规则意识文明意识被削弱依据心理学的相关理论在激情状态下人的思考能力和自控能力会受到很大的折损因而很容易作出不理智的荒谬行为许多统计数据都显示 期间酒驾等交通违法事件数量会有显著增长据报道自今年  以来珠海交警至少已 酒驾醉驾52起； 仅15日夜间至16日凌晨就查处了酒后驾驶11 3起一方面司机及乘客为了看球吃宵夜随意停放车辆影响通行增大了交通事故发生的可能性另一方面许多球迷连续熬夜看球导致体力透支少数球迷看球后情绪激动很容易做出超速驾驶追逐 等危险的举动而酒后驾车更是要命的事情与此同时酒吧餐馆也常常成为 纠纷的聚集地因为许多球迷喜欢在这些公共场合看球部分不文明的球迷吃完食品后随地乱扔甚至摔酒瓶影响 ；少部分球迷骂脏话  下支持不同球队的双方球迷甚至可能发生 引发恶性事件 是为推广 文化 的充满激情的足球文化确实会让许多球迷为此兴奋狂欢但是狂欢不等于纵欲以 作为自己情绪的出口肆意发泄绝不是足球文化的一环长期以来世界各国都有 打着足球的旗号 的“ ”在健康的足球文化生态之中绝对没有这些“ ”的位置毕竟我们不可能让所有看球的球迷都和家中的配偶签署 “文明协议”要真正消灭 过程中的不文明现象还要依靠广大球迷的自律 真正的足球爱好者应该用文明的行为让足球文化得到正向传播而不应让观看 成为不文明行为的借口乃至不文明的代名词 \n",
      "预测的分类 1\n",
      "实际的分类 1.0\n"
     ]
    }
   ],
   "source": [
    "idx=1\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据库新闻数据并保存至文本文件中 python3 已将mySQLdb==》pymysql\n",
    "# import pymysql\n",
    "# import re\n",
    "# # 打开数据库连接\n",
    "# db = pymysql.connect(\"localhost\", \"root\", \"pass\", \"news\", charset='utf8' )\n",
    "\n",
    "# # 使用cursor()方法获取操作游标 \n",
    "# cursor = db.cursor()\n",
    "# selectsql = \"select news_title,content from company_news_data\"\n",
    "# cursor.execute(selectsql.encode('utf-8'))\n",
    "# data = cursor.fetchall() #所有\n",
    "# for item in data:\n",
    "#   # 写入文本文件 title+content\n",
    "#     filtrate = re.compile(u'[^\\u4E00-\\u9FA5]')  # 过滤非中文\n",
    "#     title = filtrate.sub(r' ', item[0])\n",
    "#     fo = open('/home/lihanghang/tensorflow/NLP/CompanyNewsData/pos/'+title+'.txt', \"ab+\")\n",
    "#     # 以二进制写入章节题目 需要转换为utf-8编码，否则会出现乱码\n",
    "#     fo.write(item[1].encode('UTF-8'))\n",
    "#     fo.close()\n",
    "# print(\"############写入完毕#########\")\n",
    "\n",
    "\n",
    "# # 关闭数据库连接\n",
    "# db.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于CNN进行情感分析\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4,5\"\n",
    "# 清空图\n",
    "tf.reset_default_graph()\n",
    "# 我在这里定义了5种filter，每种100个\n",
    "filters_size = [2, 3, 4, 5, 6]\n",
    "num_filters = 100\n",
    "# 超参数\n",
    "BATCH_SIZE = 128\n",
    "EPOCHES = 50\n",
    "LEARNING_RATE = 0.001\n",
    "L2_LAMBDA = 10\n",
    "KEEP_PROB = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test \n",
    "def get_batch(x, y, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    assert x.shape[0] == y.shape[0], print(\"error shape!\")\n",
    "    # shuffle\n",
    "    if shuffle:\n",
    "        shuffled_index = np.random.permutation(range(x.shape[0]))\n",
    "\n",
    "        x = x[shuffled_index]\n",
    "        y = y[shuffled_index]\n",
    "    \n",
    "    # 统计共几个完整的batch\n",
    "    n_batches = int(x.shape[0] / batch_size)\n",
    "    \n",
    "    for i in range(n_batches - 1):\n",
    "        x_batch = x[i*batch_size: (i+1)*batch_size]\n",
    "        y_batch = y[i*batch_size: (i+1)*batch_size]\n",
    "    \n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_embeddings = embedding_matrix\n",
    "print(static_embeddings)\n",
    "EMBEDDING_SIZE = embedding_dim\n",
    "# 句子最大长度\n",
    "SENTENCE_LIMIT_SIZE = max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"cnn\"):\n",
    "    with tf.name_scope(\"placeholders\"):\n",
    "        inputs = tf.placeholder(dtype=tf.int32, shape=(None, max_tokens), name=\"inputs\")\n",
    "        targets = tf.placeholder(dtype=tf.int64, shape=[None], name=\"targets\")\n",
    "        y_one_hot = tf.one_hot( targets , 1 ) # 正负分类\n",
    "        print(inputs)\n",
    "    # embeddings\n",
    "    with tf.name_scope(\"embeddings\"):\n",
    "        # embedding_matrixs = tf.Variable(initial_value=static_embeddings, trainable=False, name=\"embedding_matrixs\")\n",
    "        embed = tf.nn.embedding_lookup(embedding_matrix, inputs, name=\"embed\")\n",
    "        # 添加channel维度\n",
    "        embed_expanded = tf.expand_dims(embed, -1, name=\"embed_expand\")\n",
    "    \n",
    "    # 用来存储max-pooling的结果\n",
    "    pooled_outputs = []\n",
    "\n",
    "    # 迭代多个filter\n",
    "    for i, filter_size in enumerate(filters_size):\n",
    "        with tf.name_scope(\"conv_maxpool_%s\" % filter_size):\n",
    "            filter_shape = [filter_size, EMBEDDING_SIZE, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, mean=0.0, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.zeros(num_filters), name=\"b\")\n",
    "\n",
    "            conv = tf.nn.conv2d(input=embed_expanded, \n",
    "                                 filter=W, \n",
    "                                 strides=[1, 1, 1, 1], \n",
    "                                 padding=\"VALID\",\n",
    "                                 name=\"conv\")\n",
    "\n",
    "            # 激活\n",
    "            a = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"activations\")\n",
    "            # 池化\n",
    "            max_pooling = tf.nn.max_pool(value=a, \n",
    "                                    ksize=[1, SENTENCE_LIMIT_SIZE - filter_size + 1, 1, 1],\n",
    "                                    strides=[1, 1, 1, 1],\n",
    "                                    padding=\"VALID\",\n",
    "                                    name=\"max_pooling\")\n",
    "            pooled_outputs.append(max_pooling)\n",
    "    \n",
    "    # 统计所有的filter\n",
    "    total_filters = num_filters * len(filters_size)\n",
    "    total_pool = tf.concat(pooled_outputs, 3)\n",
    "    flattend_pool = tf.reshape(total_pool, (-1, total_filters))\n",
    "    \n",
    "    # dropout\n",
    "    with tf.name_scope(\"dropout\"):\n",
    "        dropout = tf.nn.dropout(flattend_pool, KEEP_PROB)\n",
    "    \n",
    "    # output\n",
    "    with tf.name_scope(\"output\"):\n",
    "        W = tf.get_variable(\"W\", shape=(total_filters, 1), initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.Variable(tf.zeros(1), name=\"b\")\n",
    "        \n",
    "        logits = tf.add(tf.matmul(dropout, W), b)\n",
    "        predictions = tf.nn.sigmoid(logits, name=\"predictions\")\n",
    "    \n",
    "    # loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_one_hot, logits=logits))\n",
    "        loss = loss + L2_LAMBDA * tf.nn.l2_loss(W)\n",
    "        optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "    # evaluation\n",
    "    with tf.name_scope(\"evaluation\"):\n",
    "        correct_preds = tf.equal(tf.cast(tf.greater(predictions, 0.5), tf.float32), y_one_hot)\n",
    "        accuracy = tf.reduce_sum(tf.reduce_sum(tf.cast(correct_preds, tf.float32), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储准确率\n",
    "cnn_train_accuracy = []\n",
    "cnn_test_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-416a2593f909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-39a875c92ea7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             _, l = sess.run([optimizer, loss],\n\u001b[1;32m      9\u001b[0m                             feed_dict={inputs: x_batch, \n\u001b[0;32m---> 10\u001b[0;31m                                        targets: y_batch})\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    writer = tf.summary.FileWriter(\"./graphs/cnn\", tf.get_default_graph())\n",
    "    n_batches = int(X_train.shape[0] / BATCH_SIZE)\n",
    "    for epoch in range(EPOCHES):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in get_batch(X_train, y_train):\n",
    "            _, l = sess.run([optimizer, loss],\n",
    "                            feed_dict={inputs: x_batch, \n",
    "                                       targets: y_batch})\n",
    "            total_loss += l\n",
    "        \n",
    "        train_corrects = sess.run(accuracy, feed_dict={inputs: X_train, targets: y_train})\n",
    "        train_acc = train_corrects / X_train.shape[0]\n",
    "        cnn_train_accuracy.append(train_acc)\n",
    "        \n",
    "        test_corrects = sess.run(accuracy, feed_dict={inputs: X_test, targets: y_test})\n",
    "        test_acc = test_corrects / X_test.shape[0]\n",
    "        cnn_test_accuracy.append(test_acc)\n",
    "        \n",
    "        print(\"Training epoch: {}, Training loss: {:.4f}, Train accuracy: {:.4f}, Test accuracy: {:.4f}\".format(epoch + 1, \n",
    "                                                                                                                total_loss / n_batches,\n",
    "                                                                                                                train_acc,\n",
    "                                                                                                               test_acc))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/cnn\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0c0c51ef60>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAGA9JREFUeJzt3X+UVOWd5/H3xwZsCQQQiAk0CplBIzEr0RZ14jkhK0ZAI2aTQ9R1cnQ2S0jGWZ0dHTH+SMzs7jHjxCGOPzjqsBNXo/FoXIkhI5KRMZvoSEtQQVFaJdKg0sEBASGKfvePe9tcm+qu6u7qru6Hz+ucOta9z1P3fp8u/PSt5966rYjAzMzSckCtCzAzs+pzuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhvp+Q9B1Jd9S6jpRI+oak1yXtlDS61vX0BUkbJM2ooN9ESSFpUF/UZftyuCciD5i2x3uSdheW/3Ot60uNpMHAdcDnI2JYRGwt0WdI/kt1vaRdeTAuljQxb18haY+kCYXXzJC0obC8QdIWSR8qrPuapBW9NzpLgcM9EXnADIuIYcArwBcK6+6sdX3V1E+OBg8B6oG1nfS5FzgDOAcYARwNNAEnF/rsAq4ss6864MJuV2r7JYf7/mWIpNsl7ZC0VlJjW4OkcZLuk9Qq6WVJ/62jjUg6TdJvJL0paaOk77RrP0nSryVty9vPy9cfJOn7kn4rabuk/5evmy6ppd023v/4nx/93ivpDklvAudJmibpsXwfr0q6QdKQwus/KelhSW/kUyffkvRRSW8Vp1AkHZOPeXCJcR4oaaGkzfljYb7ucOD5vNs2Sf9S4rUzgFOAORGxMiL2RsT2iLgpIv6x0PV64GxJf9TRzxu4FrhY0shO+rTtt2065Pz8Z//vkuZLOk7S0/nP64ZC/wMkXZG/J1vyfx8jCu1/mrdtlXR5u30dIGmBpBfz9nskHVyuRusbDvf9yxnA3cBIYAlwA2T/kwI/BZ4CxpMdWV4k6dQOtrML+Gq+ndOAb0g6M9/WYcDPgX8AxgJTgdX56/4OOBb4E+Bg4K+B9yqsfQ7ZkfBI4E7gXeAvgTHAiXnN38xrGA4sB/4ZGAf8MfCLiHgNWAHMLWz3T4G7I+KdEvu8HDghH8PRwDTgioh4Afhk3mdkRPzHEq+dATwRERvLjGsTcCtwdSd9mvK6Ly6zraLjgcnAV4CFZGOZQVb3XEmfzfudlz8+B3wcGMYf/l1MAW4m+xmNA0YDDYV9/AVwJvDZvP3fgRu7UKP1pojwI7EHsAGY0W7dd4DlheUpwO78+fHAK+36Xwb87wr3txD4+8Lr7i/R5wBgN3B0ibbpQEtHY8hrf7RMDRe17Rc4G/hNB/2+Avwqf14HvAZM66Dvi8DswvKpwIb8+UQggEEdvPZWsl8andW8Avga2S/B7WTBO6NtH8WfA3BU3mds/poVHWyzra7xhXVbga8Ulu8DLsqf/wL4ZqHtCOAdYBBwVXEMwIeAtwvvy3PAyYX2jxVe2+nPx4/ef/jIff/yWuH5W0B9Pn99GDAu/8i+TdI24Ftk88r7kHS8pEfy6YztwHyyI2iACWSh2N4YsjnqUm2V+MARsKTDJT0o6bV8quZ/VVADwAPAFEmTyKZNtkfEEx30HQf8trD823xdJbaShV1ZEdFKdrT83U76rAEeBBZUuP/XC893l1gelj8vNcZBZO/9OAo/94jYRTauNocB9xf+zTxH9omq5L8b61sOd4Psf+CXI2Jk4TE8ImZ30P9HZNM6EyJiBLAIUGFbpeaPfwfs6aBtFzC0bUFSHdkRalH725feDKwDJkfEh8l+GRVr+HipwiNiD3APcC7ZdMP/KdUvt5kswNocmq+rxHJgmqSGsj0z15JNjRzbSZ9vA/+VbOqsWkqNcS/ZL4NXyX5RAiBpKNnUTJuNwKx2/27qI2JTFeuzbnK4G8ATwA5Jl+YnOOskHSXpuA76DwfeiIg9kqaRXQ3S5k5ghqS5kgZJGi1pakS8BywGrstP3tZJOlHSgcALZJ8iTstPbF4BHFim5uHAm8BOSZ8AvlFoexD4mKSL8hOgwyUdX2i/nWye+Qw6D/e7gCskjZU0hmyaoqLvCkTEcuBhsiPbY/OfxfD85Oaflei/Dfg+2XmIjrbZDPwY6PBkdzfcBfylpEmShpF9AvpxROwlO8dxurIT5EPIPlkUM2MR8D/z8yzkP6c5VazNesDhbkTEu8DpZCcOXyY7yr6N7PK9Ur4JfFfSDrLAu6ewrVeA2cBfAW+QnUw9Om++GHgGWJm3fQ84ICK259u8jewE4y7gA1fPlHAx2S+VHWTz2z8u1LCDbMrlC2RTUevJjorb2n9FdiJ3VUQUpyTa+x9kJzOfzutela+r1JeBpXlt24E1QCPZUX0pPyCb1ujMd8nmvqtlMdkvuEfJ3vs9ZCdKiYi1wJ+TfVJ7leyEafF9+QHZJ7hl+b+Fx8nO31g/oPxEiNl+Jb988UcRcVutazHrDQ532+/k000Pk50z2FHresx6g6dlbL8i6Ydk0yIXOdgtZT5yNzNLkI/czcwSVLMbMI0ZMyYmTpxYq92bmQ1ITz755O8iov33QPZRNtwlLSa7TG5LRBxVol1kl0TNJvvW43kRsarcdidOnEhTU1O5bmZmViCps8t331fJtMw/ATM7aZ9FdoOiycA8sm8OmplZDZUN94h4lOwLJx2ZA9wemceBkZIquqeGmZn1jmqcUB3PB2/q1EJ1731hZmZd1KcnVCXNI5u64dBDD+3LXZtZIt555x1aWlrYs2dPrUvpVfX19TQ0NDB48D5/R6Yi1Qj3TRTuHEd2M/+Sd4WLiFuAWwAaGxt9gb2ZdVlLSwvDhw9n4sSJZNdzpCci2Lp1Ky0tLUyaNKlb26jGtMwS4KvKnEB2f+xXq7BdM7N97Nmzh9GjRycb7ACSGD16dI8+nVRyKeRdZH8pZ4yyv3P5bWAwQEQsIrvr3WygmexSyPO7XY2ZWQVSDvY2PR1j2XCPiLPLtAfZbUHNzKyf8O0HzMy6YNu2bdx0001dft3s2bPZtm1bL1RUmsPdzKwLOgr3vXv3dvq6pUuXMnLkyN4qax81u7eMmdlAtGDBAl588UWmTp3K4MGDqa+vZ9SoUaxbt44XXniBM888k40bN7Jnzx4uvPBC5s2bB/zhlis7d+5k1qxZnHTSSfz6179m/PjxPPDAAxx00EFVrdPhbmYD1tU/Xcuzm9+s6janjPsw3/7CJztsv+aaa1izZg2rV69mxYoVnHbaaaxZs+b9SxYXL17MwQcfzO7duznuuOP40pe+xOjRoz+wjfXr13PXXXdx6623MnfuXO677z7OPffcqo7D4W5m1gPTpk37wLXo119/Pffffz8AGzduZP369fuE+6RJk5g6dSoAxx57LBs2bKh6XQ53MxuwOjvC7isf+tAf/l75ihUrWL58OY899hhDhw5l+vTpJa9VP/DAA99/XldXx+7du6tel0+ompl1wfDhw9mxo/RfaNy+fTujRo1i6NChrFu3jscff7yPq/sDH7mbmXXB6NGj+cxnPsNRRx3FQQcdxCGHHPJ+28yZM1m0aBFHHnkkRxxxBCeccELN6qzZ31BtbGwM/7EOM+uq5557jiOPPLLWZfSJUmOV9GRENJZ7radlzMwS5HA3M0uQw93MLEEOdzOzBDnczcwS5HA3M0uQw93MrAu6e8tfgIULF/LWW29VuaLSHO5mZl0wUMLd31A1M+uC4i1/TznlFD7ykY9wzz338Pvf/54vfvGLXH311ezatYu5c+fS0tLCu+++y5VXXsnrr7/O5s2b+dznPseYMWN45JFHerVOh7uZDVw/XwCvPVPdbX70UzDrmg6bi7f8XbZsGffeey9PPPEEEcEZZ5zBo48+SmtrK+PGjeNnP/sZkN1zZsSIEVx33XU88sgjjBkzpro1l+BpGTOzblq2bBnLli3j05/+NMcccwzr1q1j/fr1fOpTn+Lhhx/m0ksv5Ze//CUjRozo89p85G5mA1cnR9h9ISK47LLL+PrXv75P26pVq1i6dClXXHEFJ598MldddVWf1uYjdzOzLije8vfUU09l8eLF7Ny5E4BNmzaxZcsWNm/ezNChQzn33HO55JJLWLVq1T6v7W0+cjcz64LiLX9nzZrFOeecw4knngjAsGHDuOOOO2hubuaSSy7hgAMOYPDgwdx8880AzJs3j5kzZzJu3LheP6HqW/6a2YDiW/76lr9mZvsth7uZWYIc7mY24NRqOrkv9XSMDnczG1Dq6+vZunVr0gEfEWzdupX6+vpub8NXy5jZgNLQ0EBLSwutra21LqVX1dfX09DQ0O3XO9zNbEAZPHgwkyZNqnUZ/Z6nZczMEuRwNzNLkMPdzCxBDnczswRVFO6SZkp6XlKzpAUl2kdJul/S05KekHRU9Us1M7NKlQ13SXXAjcAsYApwtqQp7bp9C1gdEf8B+Crwg2oXamZmlavkyH0a0BwRL0XE28DdwJx2faYA/wIQEeuAiZIOqWqlZmZWsUrCfTywsbDckq8regr4TwCSpgGHAftcfS9pnqQmSU2pfwHBzKyWqnVC9RpgpKTVwF8AvwHebd8pIm6JiMaIaBw7dmyVdm1mZu1V8g3VTcCEwnJDvu59EfEmcD6AJAEvAy9VqUYzM+uiSo7cVwKTJU2SNAQ4C1hS7CBpZN4G8DXg0TzwzcysBsoeuUfEXkkXAA8BdcDiiFgraX7evgg4EvihpADWAv+lF2s2M7MyKrpxWEQsBZa2W7eo8Pwx4PDqlmZmZt3lb6iamSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mlqCKwl3STEnPS2qWtKBE+whJP5X0lKS1ks6vfqlmZlapsuEuqQ64EZgFTAHOljSlXbc/B56NiKOB6cD3JQ2pcq1mZlahSo7cpwHNEfFSRLwN3A3MadcngOGSBAwD3gD2VrVSMzOrWCXhPh7YWFhuydcV3QAcCWwGngEujIj32m9I0jxJTZKaWltbu1mymZmVU60TqqcCq4FxwFTgBkkfbt8pIm6JiMaIaBw7dmyVdm1mZu1VEu6bgAmF5YZ8XdH5wE8i0wy8DHyiOiWamVlXVRLuK4HJkiblJ0nPApa06/MKcDKApEOAI4CXqlmomZlVblC5DhGxV9IFwENAHbA4ItZKmp+3LwL+BvgnSc8AAi6NiN/1Yt1mZtaJsuEOEBFLgaXt1i0qPN8MfL66pZmZWXf5G6pmZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZgmqKNwlzZT0vKRmSQtKtF8iaXX+WCPpXUkHV79cMzOrRNlwl1QH3AjMAqYAZ0uaUuwTEddGxNSImApcBvxrRLzRGwWbmVl5lRy5TwOaI+KliHgbuBuY00n/s4G7qlGcmZl1TyXhPh7YWFhuydftQ9JQYCZwX89LMzOz7qr2CdUvAL/qaEpG0jxJTZKaWltbq7xrMzNrU0m4bwImFJYb8nWlnEUnUzIRcUtENEZE49ixYyuv0szMuqSScF8JTJY0SdIQsgBf0r6TpBHAZ4EHqluimZl11aByHSJir6QLgIeAOmBxRKyVND9vX5R3/SKwLCJ29Vq1ZmZWEUVETXbc2NgYTU1NNdm3mdlAJenJiGgs18/fUDUzS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQRWFu6SZkp6X1CxpQQd9pktaLWmtpH+tbplmZtYVg8p1kFQH3AicArQAKyUtiYhnC31GAjcBMyPiFUkf6a2CzcysvEqO3KcBzRHxUkS8DdwNzGnX5xzgJxHxCkBEbKlumWZm1hWVhPt4YGNhuSVfV3Q4MErSCklPSvpqqQ1JmiepSVJTa2tr9yo2M7OyqnVCdRBwLHAacCpwpaTD23eKiFsiojEiGseOHVulXZuZWXtl59yBTcCEwnJDvq6oBdgaEbuAXZIeBY4GXqhKlWZm1iWVHLmvBCZLmiRpCHAWsKRdnweAkyQNkjQUOB54rrqlmplZpcoeuUfEXkkXAA8BdcDiiFgraX7eviginpP0z8DTwHvAbRGxpjcLNzOzjikiarLjxsbGaGpqqsm+zcwGKklPRkRjuX7+hqqZWYIc7mZmCXK4m5klyOFuZpYgh7uZWYIc7mZmCXK4m5klyOFuZpYgh7uZWYIc7mZmCXK4m5klyOFuZpYgh7uZWYIc7mZmCXK4m5klyOFuZpYgh7uZWYIc7mZmCXK4m5klyOFuZpYgh7uZWYIc7mZmCXK4m5klyOFuZpYgh7uZWYIc7mZmCXK4m5klyOFuZpYgh7uZWYIc7mZmCXK4m5klyOFuZpYgh7uZWYIc7mZmCaoo3CXNlPS8pGZJC0q0T5e0XdLq/HFV9Us1M7NKDSrXQVIdcCNwCtACrJS0JCKebdf1lxFxei/UaGZmXVTJkfs0oDkiXoqIt4G7gTm9W5aZmfVE2SN3YDywsbDcAhxfot+fSHoa2ARcHBFr23eQNA+Yly/ulPR8F+vtD8YAv6t1EX3MY07f/jZeGLhjPqySTpWEeyVWAYdGxE5Js4H/C0xu3ykibgFuqdI+a0JSU0Q01rqOvuQxp29/Gy+kP+ZKpmU2ARMKyw35uvdFxJsRsTN/vhQYLGlM1ao0M7MuqSTcVwKTJU2SNAQ4C1hS7CDpo5KUP5+Wb3drtYs1M7PKlJ2WiYi9ki4AHgLqgMURsVbS/Lx9EfBl4BuS9gK7gbMiInqx7loa0NNK3eQxp29/Gy8kPmalm8FmZvsvf0PVzCxBDnczswQ53EuQdLCkhyWtz/87qoN+5W7L8FeSor9fOdTT8Uq6VtI6SU9Lul/SyL6rvmsqeM8k6fq8/WlJx1T62v6qu2OWNEHSI5KelbRW0oV9X3339OR9ztvrJP1G0oN9V3WVRYQf7R7A3wIL8ucLgO+V6FMHvAh8HBgCPAVMKbRPIDsJ/VtgTK3H1JvjBT4PDMqff6/U6/vDo9x7lveZDfwcEHAC8G+VvrY/Pno45o8Bx+TPhwMvpD7mQvt/B34EPFjr8XT34SP30uYAP8yf/xA4s0Sfcrdl+Hvgr4GBcMa6R+ONiGURsTfv9zjZdyH6o0pupTEHuD0yjwMjJX2swtf2R90ec0S8GhGrACJiB/Ac2TfW+7uevM9IagBOA27ry6KrzeFe2iER8Wr+/DXgkBJ9St2WYTyApDnApoh4qlerrJ4ejbedPyM7IuqPKhlDR30qHX9/05Mxv0/SRODTwL9VvcLq6+mYF5IdmL3XWwX2hWrdfmDAkbQc+GiJpsuLCxERkio++pY0FPgW2VRFv9Fb4223j8uBvcCd3Xm99U+ShgH3ARdFxJu1rqc3STod2BIRT0qaXut6emK/DfeImNFRm6TX2z6W5h/VtpTo1tFtGf4ImAQ8lX9ptwFYJWlaRLxWtQF0US+Ot20b5wGnAydHPmnZD5W9lUYnfQZX8Nr+qCdjRtJgsmC/MyJ+0ot1VlNPxvwl4Iz8Hln1wIcl3RER5/Zivb2j1pP+/fEBXMsHTzD+bYk+g4CXyIK87aTNJ0v020D/P6Hao/ECM4FngbG1HkuZcZZ9z8jmWosn2p7oyvvd3x49HLOA24GFtR5HX425XZ/pDOATqjUvoD8+gNHAL4D1wHLg4Hz9OGBpod9ssisIXgQu72BbAyHcezReoJls/nJ1/lhU6zF1MtZ9xgDMB+bnz0X2x2leBJ4BGrvyfvfHR3fHDJxEdkHA04X3dnatx9Pb73NhGwM63H37ATOzBPlqGTOzBDnczcwS5HA3M0uQw93MLEEOdzOzBDnczcwS5HA3M0vQ/wcOwyl2pZC7CQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c0eaac2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnn_train_accuracy)\n",
    "plt.plot(cnn_test_accuracy)\n",
    "plt.ylim(ymin=0.5, ymax=1.01)\n",
    "plt.title(\"The accuracy of CNN model\")\n",
    "plt.legend([\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "checkpoints; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-eb30cecb728b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 在test上的准确率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoints/cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     total_correct = sess.run(accuracy,\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1713\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1715\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1716\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \"\n\u001b[1;32m   1717\u001b[0m                        + compat.as_text(save_path))\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mcheckpoint_exists\u001b[0;34m(checkpoint_prefix)\u001b[0m\n\u001b[1;32m   2054\u001b[0m   pathname = _prefix_to_checkpoint_path(checkpoint_prefix,\n\u001b[1;32m   2055\u001b[0m                                         saver_pb2.SaverDef.V2)\n\u001b[0;32m-> 2056\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0;31m# Convert the filenames to string from bytes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0msingle_filename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m           for matching_filename in pywrap_tensorflow.GetMatchingFiles(\n\u001b[1;32m    344\u001b[0m               compat.as_bytes(single_filename), status)\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: checkpoints; No such file or directory"
     ]
    }
   ],
   "source": [
    "# 在test上的准确率\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"checkpoints/cnn\")\n",
    "    \n",
    "    total_correct = sess.run(accuracy,\n",
    "                             feed_dict={inputs: x_test, targets: y_test})\n",
    "\n",
    "    print(\"The LSTM model accuracy on test set: {:.2f}%\".format(100 * total_correct / x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-aeafbcab353a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# cnn = conv.main(False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 深度学习训练的神经网络,使用TensorFlow训练的神经网络模型，保存在文件中\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnnservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoints/cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 创建服务器套接字\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'saver' is not defined"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import sys\n",
    "import threading\n",
    "import json\n",
    "import numpy as np\n",
    "# from tag import train2\n",
    "# nn=network.getNetWork()\n",
    "# cnn = conv.main(False)\n",
    "# 深度学习训练的神经网络,使用TensorFlow训练的神经网络模型，保存在文件中\n",
    "nnservice = saver.restore(sess, \"checkpoints/cnn\")\n",
    "def main():\n",
    "    # 创建服务器套接字\n",
    "    serversocket = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n",
    "    # 获取本地主机名称\n",
    "    host = socket.gethostname()\n",
    "    # 设置一个端口\n",
    "    port = 12345\n",
    "    # 将套接字与本地主机和端口绑定\n",
    "    serversocket.bind((host,port))\n",
    "    # 设置监听最大连接数\n",
    "    serversocket.listen(5)\n",
    "    # 获取本地服务器的连接信息\n",
    "    myaddr = serversocket.getsockname()\n",
    "    print(\"服务器地址:%s\"%str(myaddr))\n",
    "    # 循环等待接受客户端信息\n",
    "    while True:\n",
    "        # 获取一个客户端连接\n",
    "        clientsocket,addr = serversocket.accept()\n",
    "        print(\"连接地址:%s\" % str(addr))\n",
    "        try:\n",
    "            t = ServerThreading(clientsocket)#为每一个请求开启一个处理线程\n",
    "            t.start()\n",
    "            pass\n",
    "        except Exception as identifier:\n",
    "            print(identifier)\n",
    "            pass\n",
    "        pass\n",
    "    serversocket.close()\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "class ServerThreading(threading.Thread):\n",
    "    # words = text2vec.load_lexicon()\n",
    "    def __init__(self,clientsocket,recvsize=1024*1024,encoding=\"utf-8\"):\n",
    "        threading.Thread.__init__(self)\n",
    "        self._socket = clientsocket\n",
    "        self._recvsize = recvsize\n",
    "        self._encoding = encoding\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        print(\"开启线程.....\")\n",
    "        try:\n",
    "            #接受数据\n",
    "            msg = ''\n",
    "            while True:\n",
    "                # 读取recvsize个字节\n",
    "                rec = self._socket.recv(self._recvsize)\n",
    "                # 解码\n",
    "                msg += rec.decode(self._encoding)\n",
    "                # 文本接受是否完毕，因为python socket不能自己判断接收数据是否完毕，\n",
    "                # 所以需要自定义协议标志数据接受完毕\n",
    "                if msg.strip().endswith('over'):\n",
    "                    msg=msg[:-4]\n",
    "                    break\n",
    "            # 解析json格式的数据\n",
    "            re = json.loads(msg)\n",
    "            # 调用神经网络模型处理请求\n",
    "            res = nnservice.hand(re['content'])\n",
    "            sendmsg = json.dumps(res)\n",
    "            # 发送数据\n",
    "            self._socket.send((\"%s\"%sendmsg).encode(self._encoding))\n",
    "            pass\n",
    "        except Exception as identifier:\n",
    "            self._socket.send(\"500\".encode(self._encoding))\n",
    "            print(identifier)\n",
    "            pass\n",
    "        finally:\n",
    "            self._socket.close() \n",
    "        print(\"任务结束.....\")\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def __del__(self):\n",
    "        \n",
    "        pass\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
